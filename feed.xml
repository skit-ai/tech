<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-12-05T07:55:20+00:00</updated><id>/feed.xml</id><title type="html">Skit Tech</title><subtitle>Speech Technology from Skit</subtitle><entry><title type="html">Incorporating context to improve SLU</title><link href="/contextual-slu/" rel="alternate" type="text/html" title="Incorporating context to improve SLU" /><published>2022-08-04T00:00:00+00:00</published><updated>2022-08-04T00:00:00+00:00</updated><id>/contextual-slu</id><content type="html" xml:base="/contextual-slu/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In task-oriented dialogue systems, the spoken language understanding, or SLU, refers to the task of parsing the natural language utterances into
semantic frames. The problem of contextual SLU majorly focuses on effectively incorporating dialogue information. Current SLU systems that work in tandem with
ASR (voice bots) only incorporate the asr transcription as an input for the SLU systems to predict intent. As such, the amount of information these transcripts have is quite less.&lt;/p&gt;

&lt;h2 id=&quot;why-context&quot;&gt;Why context?&lt;/h2&gt;
&lt;p&gt;The bot prompts are a treasure-trove of contextual information. This information can be used to build better intent classification model. Few examples with the bot prompts and their intents are shown below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Bot Prompt&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;User Response&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Intent&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Hi! I am Divya, your Hathway virtual assistant. Are you looking for a new Hathway Broadband connection?&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;confirm_new_connection&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Okay!. To start with, please tell me if you are next to your Hathway device?&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;I am&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;confirm_near_device&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;For how many people do you want to book the table for?&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;seven&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;number_guests&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If we observe the first example here, the trancription just consists of &lt;em&gt;‘yes’&lt;/em&gt;, but if we include the bot prompt, we enrich the input to our SLU, thereby increasing the overall intent classification performance. The context of &lt;em&gt;hathway broadband connection&lt;/em&gt; helps in enriching the context of the utterance &lt;em&gt;yes&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;using-bot-prompts-as-context&quot;&gt;Using Bot prompts as context&lt;/h2&gt;
&lt;p&gt;We curated a private dataset of clients wherein we collect user utterances along with all the bot prompts. 
We then concatenate the bot prompts with the user utterances. After retraining our intent classification model on some clients,
we observed a performance jump of &lt;strong&gt;20-30%&lt;/strong&gt; in the intent-F1 scores. &lt;br /&gt; 
This probably happened because our user transcriptions are not rich with enough information. By
closing that information deficit via the bot prompts, the overall input helped us in achieving better performance. Another probable reason for such a huge jump could be probably because the transcription generated while using voice bots are less accurate as compared to a chat bot’s transcription wherein a user types their response. As a result, the gap increment when supplied with contextual information when working with voice bots is much larger than let’s say a bot. However, this was not the case with all our datasets. We observed that
the datasets with large number of classes didn’t perform well or at par with datasets with less number of classes. One probable reason for it could be the dataset having
a large amount of granularity with respect to intents and as such there wasn’t any significant bump in the performance. We also observed that the performance with small-talk intents such as confirm, deny etc. had a massive jump in their performance as compared to other types of intents.&lt;/p&gt;

&lt;!-- We curate a private data of our clients wherein we collect user utterances along with all the bot prompts. Earlier, our systems
were dependent on solely using user utterances to build our Intent classifier but after conctenating bot prompts to our utterances, we
observed around 30% jump in our intent-F1 scores for some of our clients. --&gt;

&lt;h2 id=&quot;some-probable-approaches-from-literature&quot;&gt;Some probable approaches from literature&lt;/h2&gt;

&lt;p&gt;After observing an improvement in our models by just using a single bot prompt, we decided to a delve a bit further and found out many 
approaches that can be utilized for our use-case. While doing literature review, we observed that encoding the contextual prompt along with the user prompt gives the best performance amongst all the methods. The current approach of concatenating bot prompts with the user prompt acts as a natural baseline for our subsequent experiments in this direction. We discuss the encoding based approaches from the literature below:&lt;/p&gt;

&lt;h3 id=&quot;encoding-dialogue-history-1-2&quot;&gt;Encoding dialogue History [1, 2]&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;We can encode [1] the complete dialogue history as shown below. Let us assume that the dialogue is
a sequence of \(D_{t} = {u_{1}, u_{2}.. u_{t}}\) bot and user utterance and at every time 
step \(t\) we are trying to output the classification for the user utterance \(u_{t}\), given \(D_{t}\).
We then divide the model into 2 components, the context encoder that acts on \(D_{t}\) to produce
a vector representation of the dialogue context denoted by \(h_{t} = H(D_{t})\) and the tagger, which takes
this context encoding \(h_{t}\), and the current utterance \(u_{t}\) as input and produces the intent output.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;context-encoder-architecture&quot;&gt;&lt;strong&gt;Context Encoder Architecture&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;baseline context encoder&lt;/strong&gt; is just encoding the previous bot prompt \(u_{t-1}\) into a single bidirectional RNN (BiRNN) layer with Gated Recurrent Unit (GRU). The final state of the context encoder GRU is used the dialogue context, \(h_{t} = BiGRU(u_{t-1})\).
For &lt;strong&gt;memory networks&lt;/strong&gt;, we encode all the dialogue context utterances, \({u_{1}, u_{2}.. u_{t}}\) into memory networks denoted by \({m_{1}, m_{2}.. m_{t}}\) using a BiGRU encoder. We add temporal context to the dialogue history utterances, for that we append special positional tokens to each utterance, \(m_{k} = BiGRU_{m}(u_{k}) \: \: 0 &amp;lt;= k &amp;lt;= t-1\).
The current utterance is also encoded using a BiGRU and is denoted by \(c\). Let \(M\) be the matrix wherein the \(i\)th row given by \(m_{i}\). A cosine similarity is obtained between each memory vector, \(m_{i}\), and the context vector \(c\). The softmax of this similarity is used as an attention distribution over the memory \(M\), and an attention distribution over the memory \(M\), and an attention weighted sum of \(M\) is used to produce the dialogue context vector \(h_{i}\).
      \(a = softmax(M_{c})\)
      \(h_{t} = a^{T}M\)&lt;/p&gt;

&lt;h4 id=&quot;tagger-architecture&quot;&gt;&lt;strong&gt;Tagger Architecture&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;A stacked BiRNN tagger is then used to model intent classification.&lt;/p&gt;

&lt;h4 id=&quot;results&quot;&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;This approach was benchmarked on a multi-turn dialogue sessions and for intent classification specifically the task of reserving tables at the 
restaurant. The intent F1 scores with memory network as the contextual encoder is &lt;strong&gt;0.890&lt;/strong&gt; and just by encoding the last prompt is &lt;strong&gt;0.865&lt;/strong&gt;. &lt;br /&gt;
&lt;img src=&quot;../assets/images/contextual/encoder_context.png&quot; alt=&quot;drawing&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Another approach [2] is to have a different encoding mechanism for bot and user utterances [2]. This approach uses a system act encoder to obtain a vector representation \(a^{t}\) of all system dialogue acts \(A^{t}\). An utterance encoder is then used
to generate the user utterance encoding \(u^{t}\) by processing the user utterance token embeddings \(x^{t}\).
We then have a dialogue encoder that summarizes the content of the dialogue using \(a^{t}\) and \(u^{t}\), and its previous
hidden state \(s^{t-1}\) to generate the dialogue context vector \(o^{t}\), and also update the hidden state.
The dialogue context vector is then used for intent classification. Both the encoders use a hierarchical RNN that processes a single utterance at a time.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;system-act-encoder&quot;&gt;&lt;strong&gt;System Act Encoder&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The system act encoder encodes the set of dialogue acts \(A^{t}\) at turn \(t\) into a vector \(a^{t}\) invariant to the order in which they appear.&lt;/p&gt;

&lt;h4 id=&quot;utterance-encoder&quot;&gt;&lt;strong&gt;Utterance Encoder&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The utterance encoder takes in the list of user utterance tokens as input. Let \(x^{t}\) denote the utterance token embeddings, which is encoded using a bi-directional GRU.
\(u^{t}, u^{t}_{o} = BRNN_{GRU}(x^{t})\)
We get the embedding representation \(u^{t}\) of the user utterance and \(u^{t}_{o}\) is the concatenation of the final states and the intermediate outputs of the forward and backward RNNs respectively.&lt;/p&gt;

&lt;h4 id=&quot;dialogue-encoder&quot;&gt;&lt;strong&gt;Dialogue Encoder&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The dialogue encoder incrementally generated the embedded representation of the dialogue context at every turn. As shown in the figure below, it takes in \(a^{t} \bigoplus u^{t}\) and its previous state \(s^{t-1}\) as inputs
and outputs the updated state \(s^{t}\) and the encoded representation of the dialogue context \(o^{t}\).&lt;/p&gt;

&lt;p&gt;The above encoded feature is then flattened to the number of intent classes using a linear layer. 
    \(p_{i}^{t} = softmax(W_{i}.o^{t} + b_{i})\)&lt;/p&gt;

&lt;h4 id=&quot;results-1&quot;&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The dialogues are obtained from simulated dialogues dataset.The dataset has dialogues from restaurant and movie domains with total of 3 intents. The baseline for this approach was getting results without any context and the overall intent accuracy was &lt;strong&gt;84.76%&lt;/strong&gt; whereas using the previous dialog encoder (\(o^{t-1}\)) and the current system encoder  (\(a^{t}\)) was &lt;strong&gt;99.54%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/contextual/encoder_context_2.png&quot; alt=&quot;drawing&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;probable-approaches&quot;&gt;Probable approaches&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Another approach that has not been discussed in literature is using a time decay function to decay the effect of older bot prompts. This would help in focusing more
towards the recent prompts and reduce the effect of older prompts.&lt;/li&gt;
  &lt;li&gt;We can also experiment by fusing different modalities (text and speech) with utterance and dialogue. The emotion from the speech modality could help in infusing much better context into the input for the intent classification.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The above approaches and experiments show that context for SLU predictions can prove to be extremely useful for improving 
intent F1 scores. These above approaches are also not computationally expensive and can be easily deployed at scale for various use-cases.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Ankur Bapna, Gokhan Tür, Dilek Hakkani-Tür, and Larry Heck. 2017. Sequential Dialogue Context Modeling for Spoken Language Understanding. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 103–114, Saarbrücken, Germany. Association for Computational Linguistics. &lt;br /&gt;
[2] Gupta, R., Rastogi, A., &amp;amp; Hakkani-Tür, D.Z. (2018). An Efficient Approach to Encoding Context for Spoken Language Understanding. ArXiv, abs/1807.00267.&lt;/p&gt;</content><author><name>Sanchit Ahuja</name></author><category term="Machine Learning" /><category term="slu" /><category term="context" /><category term="nlp" /><summary type="html">Introduction In task-oriented dialogue systems, the spoken language understanding, or SLU, refers to the task of parsing the natural language utterances into semantic frames. The problem of contextual SLU majorly focuses on effectively incorporating dialogue information. Current SLU systems that work in tandem with ASR (voice bots) only incorporate the asr transcription as an input for the SLU systems to predict intent. As such, the amount of information these transcripts have is quite less.</summary></entry><entry><title type="html">Investigating Label Noise in intent classification datasets and fixing it</title><link href="/label-noise-intro/" rel="alternate" type="text/html" title="Investigating Label Noise in intent classification datasets and fixing it" /><published>2022-06-19T00:00:00+00:00</published><updated>2022-06-19T00:00:00+00:00</updated><id>/label-noise-intro</id><content type="html" xml:base="/label-noise-intro/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Label noise has been a consistent problem even in the &lt;a href=&quot;https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/f2217062e9a397a1dca429e7d70bc6ca-Paper-round1.pdf&quot;&gt;most widely used open source datasets&lt;/a&gt;. Several papers have come up various &lt;a href=&quot;https://arxiv.org/pdf/2007.08199.pdf&quot;&gt;deep learning techniques&lt;/a&gt; to make models more robust to label noise present in their train sets. Even so, identifying label noise in your dataset and investigating it’s cause is an important process to further understand model behaviour and prevent label noise in future datasets.&lt;/p&gt;

&lt;p&gt;In this blog, we discuss why we decided to fix label noise in our datasets followed by some statistic cleaning methods we tested to narrow down regions within the dataset where label noise could be present.&lt;/p&gt;

&lt;h2 id=&quot;why-fix-label-noise&quot;&gt;Why fix label noise?&lt;/h2&gt;

&lt;p&gt;Test sets should be clean to serve as a benchmark for future decisions. To measure the impact of noisy train sets, we plot a graph of model performance versus % label noise. To conduct this experiment, we retagged an old dataset in one of our clients and thoroughly reviewed it to identify and fix mislabelled examples. The total number of mislabelled examples was 13% of the whole dataset (7591 instances). We flipped the gold labels into their noisy counterparts, trained a model on the newly formed dataset and plotted the results.&lt;/p&gt;

&lt;h3 id=&quot;impact-of-train-set-label-noise-on-our-model-performance&quot;&gt;Impact of train set label noise on our model performance&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/label-noise-blog/training_noise.png&quot; alt=&quot;image info&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the above graph. we pbserve that at 0% label noise, the model performance is around 73.8% F1 and at ~13% label noise, the model performance drops to 70.8% F1.&lt;/p&gt;

&lt;h2 id=&quot;different-cleaning-methods-to-fix-the-label-noise&quot;&gt;Different cleaning methods to fix the label noise&lt;/h2&gt;

&lt;p&gt;By measuring the reduction in cleaning effort from the baseline, we can assess the efficacy of the cleaning method. We plotted label noise recall vs % of samples retagged (or annotator effort) - Relating these metrics with previous impact graph allows us to reach interesting conclusions like - &lt;em&gt;clean y% of the dataset using a method M, and you will get some x% bump in model performance&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;random-sampling&quot;&gt;Random Sampling&lt;/h3&gt;
&lt;p&gt;Here we can sample some fixed number of instances and get them retagged. This serves as our baseline for other methods. The label noise we capture will be around 13% of each partial sample, and hence the recall will be the fraction of the partial sample (in the whole). On average, the plot will look similar to y=x, like this one for our dataset:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/label-noise-blog/random-sampling-plot.png&quot; alt=&quot;image info&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;biased-sampling&quot;&gt;Biased Sampling&lt;/h3&gt;
&lt;p&gt;This requires intermittent involvement from ops (tagging after every sampling iteration)&lt;/p&gt;

&lt;p&gt;In this method, we first randomly retag x % of samples. Then we identify the major tag confusions (as shown in the Dataset section) and pick the top 5 noisy tags and increase the weights associated to these tags in the sampling function. Then we sample again - pick top 5 tags - repeat.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/label-noise-blog/random-sampling-weights-plot.png&quot; alt=&quot;image info&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see an improvement over the baseline. We can capture around 60% of the total label noise by just tagging around 32% of the total dataset by this heuristic.&lt;/p&gt;

&lt;h3 id=&quot;datamaps&quot;&gt;Datamaps&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2009.10795.pdf&quot;&gt;This&lt;/a&gt; paper introduces datamaps - a tool to diagnose training sets during the training process itself. They introduce two metrics - confidence and variability to understand training dynamics. They further plot each instance on a confidence vs variability graph and create hard-to-learn, ambigous and easy regions. These regions correspond to how easy it is for the model to learn the particular instance. They also observe that the hard-to-learn regions also corresponded instances that had label noise.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Confidence&lt;/em&gt; - This is defined as the mean model probability of the true label across epochs.&lt;br /&gt;
&lt;em&gt;Variability&lt;/em&gt; - This measures the spread of model probability across epochs, using the standard deviation.&lt;/p&gt;

&lt;p&gt;The intuition is that instances with consistently lower confidence scores throughout the training process are hard for a model to learn. This could be because the model is not capable of learning the target label or that the target label was incorrect.&lt;/p&gt;

&lt;p&gt;We leverage the training artefacts from the paper to define a label score for each sample - as the Euclidean distance between (0,0) and (confidence, variability). Following the hypothesis of hard-to-learn regions, we expect noisy samples to have a lower label score.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Threshold on label-score&lt;/strong&gt; &lt;br /&gt;
Fixing a threshold on the label score means that all samples that score below it are considered label noise, and those that score above are considered clean. Assuming we do Human Retagging of all samples predicted as label noisy, fixing a threshold essentially fixes both the % of samples retagged and (given the clean tags-) label noise recall. Varying the threshold, we get a plot for our dataset:&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../assets/images/label-noise-blog/deja-vu-plot.png&quot; alt=&quot;image info&quot; /&gt;
Looking at our metrics, we see an improvement in the partial recleaning process.
Lets read the above plots. Say we fix the threshold at 0.43 which means we would be retagging around 28% of our dataset. This corresponds to a label noise recall of 60%, giving us a resulting dataset with 5.2% label noise from 13%. (= 0.40*13).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;n-consecutive correct instances&lt;/strong&gt; &lt;br /&gt;
Here, we will use the ordering of the label scores. Our added assumption here, is that the ordering within the regions are also useful. Based on this, we sort our samples by label score, and in ascending order. This means the noisy samples should be nearer to the top, and we base our heuristic on this. We start Human Retagging from the top of the sorted list of samples, and stop once we see N-consecutive clean samples.
Varying N, we get a plot for our dataset:&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../assets/images/label-noise-blog/deja-vu-n-consecutive.png&quot; alt=&quot;image info&quot; /&gt;&lt;br /&gt;
Again, we see an improvement in the partial recleaning process. Lets read the above plots. Say we fix N at ~38, which means we would be retagging around 35% of our dataset. This corresponds to a corresponds to a label noise recall of 60%, which means we would capture and clean 76% of the label noise. Giving us a resulting dataset with 3.1% label noise (= (1-0.76)*0.13).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cleanlab&quot;&gt;Cleanlab&lt;/h3&gt;

&lt;p&gt;This is a label noise prediction tool. We have evaluated the accuracy of this tool instead. But we won’t be able to capture all the noisy labels via this tool. This tool takes in predicted probabilities. Since cleanlab depends on output model probabilities it can’t be used to correct train sets.&lt;/p&gt;

&lt;h3 id=&quot;confident-learning&quot;&gt;Confident Learning&lt;/h3&gt;

&lt;p&gt;Confident Learning high level idea - When the predicted probability of an example is greater than a per-class-threshold, we confidently count that example as actually belonging to that threshold’s class. The thresholds for each class are the average predicted probability of examples in that class.&lt;/p&gt;

&lt;p&gt;Confident Learning estimates a joint distribution between noisy observed labels and the true latent labels. It assumes that the predicted probabilities are out-of-sample holdout probabilities (eg. K-fold cross validation). If this isn’t the case then overfitting may occur. Their algorithm also assumes that class-conditional label noise transitions are data independent.&lt;/p&gt;

&lt;p&gt;Metrics using a model trained on noisy labels.&lt;/p&gt;

&lt;p&gt;Tested on a separate test set&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/label-noise-blog/clean-set-table.png&quot; alt=&quot;image info&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Results are slightly better when the model is trained on clean data&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/label-noise-blog/noisy-set-table.png&quot; alt=&quot;image info&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We expect cleanlab to perform even better once our model test accuracies improve. Cleanlab wont be very useful if the model is performing poorly even on a clean dataset.&lt;/p&gt;

&lt;h2 id=&quot;minimizing-tagging-errors-at-source&quot;&gt;Minimizing tagging errors at source&lt;/h2&gt;

&lt;p&gt;To understand why our datasets had noisy labels, we conducted several review sessions with our annotators after they retagged datasets across multiple clients. We further classified each mislabelled example into a list of possible reasons as shown below. Here, gold tag refers to the ground truth tag. Each instance was tagged X times (X is the number of annotators) and the highest tag was chosen as the correct tag.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/label-noise-blog/tagging-errors-fix.png&quot; alt=&quot;image info&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;since our intent classifiers were not multi-label, we wanted to capture the total % of multiple intent scenarios.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We observed that the label noise patterns for each of our clients were quite different, which made the problem of generalizing label noise prediction even more difficult.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;To conclude, we quantified how using datamaps helps in reducing effort taken to clean our existing train sets. We also correlated this reduced cleaning effort with the expected improvement in model performance with the help of some plots.&lt;/p&gt;</content><author><name>Kriti Anandan</name></author><category term="Machine Learning" /><category term="label-noise" /><summary type="html">Introduction</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/label-noise-blog/label-noise.png" /><media:content medium="image" url="/assets/images/label-noise-blog/label-noise.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Theory of Mind and Implications for Conversational AI</title><link href="/theory-of-mind/" rel="alternate" type="text/html" title="Theory of Mind and Implications for Conversational AI" /><published>2022-05-19T00:00:00+00:00</published><updated>2022-05-19T00:00:00+00:00</updated><id>/theory-of-mind</id><content type="html" xml:base="/theory-of-mind/">&lt;p&gt;When a diplomat says &lt;em&gt;yes&lt;/em&gt;, he means ‘perhaps’;&lt;br /&gt;
When he says &lt;em&gt;perhaps&lt;/em&gt;, he means ‘no’;&lt;br /&gt;
When he says &lt;em&gt;no&lt;/em&gt;, he is not a diplomat.&lt;/p&gt;

&lt;p&gt;                        —&lt;em&gt;Voltaire&lt;/em&gt; (Quoted, in Spanish, in Escandell 1993.)&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Consider this example: You’re out in the street in a crowded area. A stranger walks upto you and asks for directions in your local language, &lt;em&gt;L&lt;/em&gt;. You responded, you notice the facial expressions of the stranger and that they seem to be confused, and do not understand what you said. Now, you’re confused as well, and try to clarify your instructions, but the stranger later reveals that he isn’t very fluent in the language &lt;em&gt;L&lt;/em&gt;; hence you ask for whether they understand a globally-used language &lt;em&gt;E&lt;/em&gt;, the stranger confirms, and the conversation continues.&lt;/p&gt;

&lt;p&gt;Let’s breakdown what occurred here.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The stranger asked a question in a local language &lt;em&gt;L&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;You now have a belief that the stranger is speaks the local language &lt;em&gt;L&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Due to your belief, you respond in the same language &lt;em&gt;L&lt;/em&gt;, &lt;em&gt;expecting&lt;/em&gt; the stranger to understand the information you’re trying to convey. This is a &lt;em&gt;false belief&lt;/em&gt;, as it is later revealed.&lt;/li&gt;
  &lt;li&gt;You look for verbal/non-verbal cues from the stranger that they understood.&lt;/li&gt;
  &lt;li&gt;However, the stranger &lt;em&gt;denies&lt;/em&gt; your expectation by showing absence of such cues and instead, show cues of confusion.&lt;/li&gt;
  &lt;li&gt;You attempt to further elaborate your instructions taking these cues into account, however the stranger still seems confused.&lt;/li&gt;
  &lt;li&gt;The conversation at this point feels “awkward” since your expectations of the conversation were being denied multiple times.&lt;/li&gt;
  &lt;li&gt;The stranger reveals that they aren’t very fluent in &lt;em&gt;L&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;This confirms that your &lt;em&gt;belief&lt;/em&gt; that the stranger understood &lt;em&gt;L&lt;/em&gt; was &lt;em&gt;false&lt;/em&gt;. This brings a sense of comfort since now you understand why your expectations were being denied.&lt;/li&gt;
  &lt;li&gt;You now correct for your &lt;em&gt;false belief&lt;/em&gt; and ask whether the stranger understands a globally-used language &lt;em&gt;E&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;The stranger confirms.&lt;/li&gt;
  &lt;li&gt;And the conversation continues.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This mechanism of having expectations from the other participant is a basis for successful conversation. If we lacked such an ability, the emergence of mutually accepted meanings of words, and language itself would be impossible. This also applies to non-verbal communication, such as body language and facial expressions, and to some degree, is observed in many animal species.&lt;/p&gt;

&lt;p&gt;We now dive deeper into this aspect of communication, and formalize why both, human-human and human-machine conversations breakdown and/or lead to frustration of the participants.&lt;/p&gt;

&lt;h1 id=&quot;theory-of-mind&quot;&gt;Theory of Mind&lt;/h1&gt;

&lt;p&gt;Whenever we converse, we take into account what we expect the other person to understand through our words as well as their possible responses. The ability to conceive such “theories” of other participants’ mental states is termed as the Theory of Mind (ToM).&lt;/p&gt;

&lt;p&gt;Having ToM requires the agent to acknowledge the fact that others (including the agent itself) can &lt;em&gt;believe&lt;/em&gt; in things which are not true. These beliefs are called &lt;em&gt;false beliefs&lt;/em&gt;. An agent possessing ToM can identify their own as well as other’s false beliefs and take actions to confirm and hence correct these false-beliefs.&lt;/p&gt;

&lt;h1 id=&quot;what-makes-a-conversation-human-like&quot;&gt;What makes a conversation &lt;em&gt;human-like&lt;/em&gt;?&lt;/h1&gt;

&lt;p&gt;Proposition:&lt;/p&gt;

&lt;p&gt;               &lt;em&gt;A dialogue is human-like if both agents participating have some degree of Theory of Mind.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Theory of Mind is not limited to the content of the speech (such as the words spoken), but also addresses the mannerism of speech (prosody), facial and other non-verbal cues etc. It is easy to see that if any one of the agents lack ToM or have a poor ability, the conversation becomes uncomfortable and frustrating.&lt;/p&gt;

&lt;p&gt;However, Theory of Mind is an acquired skill, expertise of humans on ToM matures over the lifespan [2], in-addition to depending on the amount of socialization the person part-takes in. This makes quantifying the degree of expertise over ToM difficult, hence quantifying the degree of &lt;em&gt;human-likeness&lt;/em&gt; is also difficult, in-addition to being be subjective.&lt;/p&gt;

&lt;h2 id=&quot;testing-the-presence-of-theory-of-mind&quot;&gt;Testing the Presence of Theory of Mind&lt;/h2&gt;

&lt;p&gt;Testing whether or not an agent is capable of modeling mental states of others is important for many reasons, one such application is diagonosing mental disorders. Such tests are called false-belief tasks. These tests check whether the agent can model other’s false-beliefs and/or confirm and correct its own false-beliefs. We will discuss two popular false-belief tasks: “Sally-Anne” and “Smarties” tasks .&lt;/p&gt;

&lt;h3 id=&quot;sally-anne-task&quot;&gt;Sally-Anne Task&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/en/a/ac/Sally-Anne_test.jpg&quot; alt=&quot;Illustration of the &amp;quot;Sally-Anne&amp;quot; Test&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The participating agent is told the following scenario:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Sally and Anne are inside a room.&lt;/li&gt;
  &lt;li&gt;Sally has a basket with one marble inside it.&lt;/li&gt;
  &lt;li&gt;Anne has an empty box with her.&lt;/li&gt;
  &lt;li&gt;Sally leaves the room without her basket.&lt;/li&gt;
  &lt;li&gt;Anne takes the marble out of Sally’s basket and puts it in her own box.&lt;/li&gt;
  &lt;li&gt;Sally comes back inside the room.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, the participant is asked, “Where will Sally look for her marble?”. If the participant replies with “the basket”, this means that the participant is able to model the mental state of a fictional character Sally, and that she doesn’t know that Anne took her marble. Children below the age of 3-4 answer with “the box”, however, older children answer with “the basket”. Some children with mental disabilities such as Down syndrome and Autism are unable to pass this test.&lt;/p&gt;

&lt;h3 id=&quot;smarties-task&quot;&gt;Smarties Task&lt;/h3&gt;
&lt;p&gt;Smarties is a popular brand of candies.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The participant is presented with a box labelled “Smarties”.&lt;/li&gt;
  &lt;li&gt;The participant is asked “what is in the box?”.&lt;/li&gt;
  &lt;li&gt;The participant replies with “candies”.&lt;/li&gt;
  &lt;li&gt;The box is opened and is revealed that the box actually contains pencils.&lt;/li&gt;
  &lt;li&gt;The participant is asked “What would someone else think is inside the box?”.&lt;/li&gt;
  &lt;li&gt;The participant passes the test if they respond with “candies”.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Theory of Mind is an acquired skill, and is not innate, i.e., we aren’t born with the ability to model other’s mental states. A study [1] shows that children first pass False-belief tasks at around 3-4 years of age, around the same time as children first learn to tell lies, suggesting that learning to lie is a pre-cursor to possessing ToM. This does make sense, as lying would &lt;em&gt;only help if the other participant is capable of having false-beliefs&lt;/em&gt;. Language and communication are also acquired skills.&lt;/p&gt;

&lt;h2 id=&quot;theory-of-mind-relevance-to-conversational-ai&quot;&gt;Theory of Mind: Relevance to Conversational AI&lt;/h2&gt;
&lt;p&gt;Having ToM allows for certain mechanisms that would not be possible otherwise. Some are listed below:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The ability of the agent to recognize its own errors in perceiving (mis-hearing), i.e., discover its own false-beliefs and ask for clarifications. This also leads to a higher order reasoning capability of the agent.&lt;/li&gt;
  &lt;li&gt;The ability of the agent to dynamically model its counterpart throughout the conversation and adjust its own behaviour inorder to maximize the success of the dialog. Dynamic response and prosody generation, turn-taking, barge-in handling, etc. are such examples.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;do-machines-have-a-theory-of-mind&quot;&gt;Do Machines have a Theory of Mind?&lt;/h2&gt;

&lt;p&gt;One of the important goals of AI is to blend in the lives of Humans and solve problems &lt;em&gt;with humans-in-the-loop&lt;/em&gt;, achieving this requires modeling humans and other machines around the agent, similar to how we humans do.&lt;/p&gt;

&lt;p&gt;Some studies [3, 4] have shown that specially designed Multi-Agent Reinforcement Learning algorithms pass the Sally-Anne False-belief task. However, False-belief tasks have not been designed for/tested against chat/voice bots. In this section, we test multiple Language Models (LM) against the Sally-Anne and Smarties tasks, and check whether they pass the tests or not.&lt;/p&gt;

&lt;h3 id=&quot;methodology&quot;&gt;Methodology&lt;/h3&gt;
&lt;p&gt;All of the experiments were done using Huggingface’s Hub has inference interface. These experiments can be easily re-ran, however, it is not guarrenteed to get the same results since the inference is non-detereministic. The tasks are widely used and are available in Wikipedia and other scientific papers on which some/all of the LMs may have been trained on, hence these tests are not conclusive.&lt;/p&gt;

&lt;h4 id=&quot;sally-anne-task-1&quot;&gt;Sally-Anne Task&lt;/h4&gt;
&lt;p&gt;Input Text: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sally and Anne are inside a room. Sally has a basket with one marble inside it. Anne has an empty box with her. Sally leaves the room without her basket. Anne takes the marble out of Sally's basket and puts it in her own box. Sally comes back inside the room. Sally will look for her marble in &lt;/code&gt;
If the LMs continue with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;her basket&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;the basket&lt;/code&gt; or anything similar, the LM passes the test, else it doesn’t.&lt;/p&gt;

&lt;h4 id=&quot;smarties-task-1&quot;&gt;Smarties Task&lt;/h4&gt;
&lt;p&gt;Input Text: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sally is presented with a box labeled &quot;Candies&quot;. Sally is asked, &quot;what is in the box?&quot;. Sally replies with &quot;candies&quot;. The box is opened and is revealed that the box actually contains pencils. Sally is asked, &quot;What would someone else think is in the box?&quot;. Sally answers &lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If the LMs continue with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;candies&lt;/code&gt; or anything similar, the LM passes the test, else it doesn’t.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Language Model&lt;/th&gt;
      &lt;th&gt;# Params&lt;/th&gt;
      &lt;th&gt;Sally-Anne&lt;/th&gt;
      &lt;th&gt;Smarties&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;DistilGPT2&lt;/td&gt;
      &lt;td&gt;82M&lt;/td&gt;
      &lt;td&gt;Fail&lt;/td&gt;
      &lt;td&gt;Fail&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-Neo-125M&lt;/td&gt;
      &lt;td&gt;125M&lt;/td&gt;
      &lt;td&gt;Fail&lt;/td&gt;
      &lt;td&gt;Fail&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-Neo-1.3B&lt;/td&gt;
      &lt;td&gt;1.3B&lt;/td&gt;
      &lt;td&gt;Fail&lt;/td&gt;
      &lt;td&gt;Fail&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-2&lt;/td&gt;
      &lt;td&gt;1.5B&lt;/td&gt;
      &lt;td&gt;Pass&lt;/td&gt;
      &lt;td&gt;Pass&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-Neo-2.7B&lt;/td&gt;
      &lt;td&gt;2.7B&lt;/td&gt;
      &lt;td&gt;Pass&lt;/td&gt;
      &lt;td&gt;Pass&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-J-6B&lt;/td&gt;
      &lt;td&gt;6B&lt;/td&gt;
      &lt;td&gt;Pass&lt;/td&gt;
      &lt;td&gt;Pass&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The largest three of the models pass both the tests. This suggests that scale might help LMs achieve some basic reasoning capabilities. This result is not surprising, since larger LMs usually do better in reasoning benchmarks.&lt;/p&gt;

&lt;p&gt;P.S. The most entertaining response award goes to DistilGPT2 for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;I don't give a fig about the box&quot;&lt;/code&gt; for the Sally-Anne task. This is not made up, I swear!&lt;/p&gt;

&lt;h2 id=&quot;implications-for-goal-oriented-conversational-ai&quot;&gt;Implications for Goal-Oriented Conversational AI&lt;/h2&gt;

&lt;p&gt;Open-domain chat has one important goal, &lt;em&gt;engagement with the user&lt;/em&gt;. The user engages with the bot &lt;em&gt;if the bot is entertaining the user&lt;/em&gt;. For this statement to hold true, the bot should appropriately make responses, which in-turn requires modeling the user, i.e., having a Theory of Mind. The &lt;em&gt;degree of engagement&lt;/em&gt; can be seen as a measure of &lt;em&gt;degree of ToM&lt;/em&gt; of the bot.&lt;/p&gt;

&lt;p&gt;Testing ToM is straight-forward for Open-domain (chit-chat) bot. However, this is tricky for goal-oriented bots, as they are designed to handle dialog under a specific domain. False-belief task defined on one domain maybe out-of-domain for another domain.&lt;/p&gt;

&lt;p&gt;Open-domain dialog is a strict generalization of goal-oriented dialog. However, goal-oriented may have goals which are defined differently from &lt;em&gt;engagement&lt;/em&gt;. In many call-center settings, &lt;em&gt;call resolution&lt;/em&gt; is the most important goal. However, when voice bots are used in-place of human agents in call centers, a new and different behaviour of users arises: &lt;em&gt;call drop&lt;/em&gt;. Users simply drop from the call if they:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;get frustrated (due to mishearing, poor reasoning capabilities etc.).&lt;/li&gt;
  &lt;li&gt;think the bot is &lt;em&gt;incapable&lt;/em&gt; of answering their queries, even if the bot is capable. This is a false-belief of the user, and the bot is unable to correct the user’s false-belief.
Call drops occur in a major chunk of the calls (40-50%).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Most bots in the industry are designed in a way that assumes &lt;em&gt;the user trusts the bot and has infinite patience&lt;/em&gt;. The bot’s behaviour is apparrently designed to optimize for resolving queries of the user, however, not to &lt;em&gt;inspire trust in the user that the bot is capable to resolve queries&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;There are two possible ways to “solve” this problem:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Explicit&lt;/code&gt;: Design the product in a way that inspires trust. Come up with the &lt;em&gt;best possible&lt;/em&gt; responses for all possible combination of dialog history and user states.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Implicit&lt;/code&gt;: Design the product in a top-down fashion rather than a bottom-up. Many believe that optimizing components with their local objective (Word-error-rate for ASR, F1 Scores for Intent classifiers etc.) would lead to a higher resolution rate. In biological systems, the higher-order function (survival) dictates lower order function (communication, language). Learning to communicate better can not ensure survival on its own. However, learning to survive &lt;em&gt;may lead to a better ability to communicate&lt;/em&gt;. In other words, optimize ML models against objective (resolution rate) in-addition to the local objective. This &lt;em&gt;will&lt;/em&gt; force the bot to behave in a way that inspires trust from the user and effectively learn to have theory of mind of the users.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first method is the industry standard, and it doesn’t seem to be working well. The second method has the clear advantage of being data-driven and scalable.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Astington, J.W., &amp;amp; Edward, M.J. (2010). The Development of Theory of Mind in Early Childhood.&lt;/p&gt;

&lt;p&gt;[2] Demetriou, A., Mouyi, A., &amp;amp; Spanoudis, G. (2010). “The development of mental processing”, Nesselroade, J. R. (2010). “Methods in the study of life-span human development: Issues and answers.” In W. F. Overton (Ed.), &lt;em&gt;Biology, cognition and methods across the life-span.&lt;/em&gt; Volume 1 of the &lt;em&gt;Handbook of life-span development&lt;/em&gt; (pp. 36–55), Editor-in-chief: R. M. Lerner. Hoboken, New Jersey: Wiley.&lt;/p&gt;

&lt;p&gt;[3] Rabinowitz, N.C., Perbet, F., Song, H.F., Zhang, C., Eslami, S.M., &amp;amp; Botvinick, M.M. (2018). Machine Theory of Mind. &lt;em&gt;ICML&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;[4] Nguyen, T.N., &amp;amp; González, C. (2020). Cognitive Machine Theory of Mind. &lt;em&gt;CogSci&lt;/em&gt;.&lt;/p&gt;</content><author><name>Surya Kant Sahu</name></author><category term="Machine Learning" /><category term="Theory of Mind" /><category term="conversational ai" /><category term="voicebot" /><category term="chatbot" /><category term="voice assistant" /><summary type="html">When a diplomat says yes, he means ‘perhaps’; When he says perhaps, he means ‘no’; When he says no, he is not a diplomat.</summary></entry><entry><title type="html">End of Utterance Detection</title><link href="/end-of-utterance-detection/" rel="alternate" type="text/html" title="End of Utterance Detection" /><published>2022-04-24T00:00:00+00:00</published><updated>2022-04-24T00:00:00+00:00</updated><id>/end-of-utterance-detection</id><content type="html" xml:base="/end-of-utterance-detection/">&lt;blockquote&gt;
  &lt;p&gt;This blog post is based on the work done by &lt;a href=&quot;https://github.com/Anirudh257&quot;&gt;Anirudh 
Thatipelli&lt;/a&gt; as an ML research fellow at Skit.ai&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;end-of-utterance-detection---when-does-a-speaker-stop-speaking&quot;&gt;End Of Utterance Detection - When does a speaker stop speaking?&lt;/h1&gt;

&lt;p&gt;End-of-utterance detection is the problem of detecting when a user has stopped speaking in a conversation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/16001446/164991645-fadf9a68-3e75-4077-8050-5aabdc30b2d1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the above image, there are four turns in total that are time-aligned.. The system initiates the conversation by speaking first (“How may I help you?”), then the user (“I want to go to Miami.”), then the system again (“Miami?”) and finally the system (“Yes.”).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The speaker who utters the first unilateral sound both initiates the conversation and gains possession of the floor. Having gained possession, a speaker maintains it until the first unilateral sounds by another speaker, at which time the latter gains possession of the floor.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;

&lt;p&gt;Despite going through many advances, the performance of spoken dialogue systems remains unsatisfactory. For example, turn-taking is a fundamental aspect of natural human conversation that helps to decide which participant has the floor in a conversation and who can speak next. Humans use many multimodal cues like prosodic features, gaze, etc to determine who has the floor in a particular conversation. The interaction is very smooth with very less gaps and overlaps between participants’ speech, making its modeling difficult. Currently, dialogue systems use a silence threshold to determine whether it should start speaking. This approach is too simplistic and can lead to many issues. The system can interrupt the user mid-utterance, known as &lt;em&gt;cut-in&lt;/em&gt;. Or it can wait too long and leads to sluggish responses and possible misrecognition, causing an increase in &lt;em&gt;latency&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;As speech-dialogue systems become more ubiquitous, it is essential to design dialogue systems that can predict end of utterance and predict turns.&lt;/p&gt;

&lt;p&gt;A dialogue system designer should also consider the trade-offs between cut-ins and latency. For Skit, an effective turn-taking system will improve customer service and decrease call-drop rate. Imbibing turn-taking capabilities into our product will make it more natural and improve the conversations with customers.&lt;/p&gt;

&lt;h1 id=&quot;previous-approaches-to-solve-the-problem&quot;&gt;Previous approaches to solve the problem&lt;/h1&gt;

&lt;p&gt;One of the earliest models to study conversations was designed by &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S088523082030111X&quot;&gt;Harvey Sacks et al&lt;/a&gt; in which he divided a conversation into two units of speech: &lt;strong&gt;Turn-constructional units (TCU)&lt;/strong&gt; and &lt;strong&gt;Transition-relevant place (TRP)&lt;/strong&gt; respectively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/16001446/164993172-cc7293f1-5267-434a-9f77-a241b44a0421.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Turn-constructional units are utterances from one speaker during which other participants assume the role as listeners. And each TCU is followed by a 
TRP, where a turn-shift can occur by the following rules:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;The current speaker may select a next speaker (other-select), using for example gaze or an address term. In the case of
dyadic conversation, this may default to the other speaker.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;If the current speaker does not select a next speaker, then any participant can self-select. The first to start gains the turn.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;If no other party self-selects, the current speaker may continue.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;To identify these TCUs and TRPs, researchers segment the speech into &lt;strong&gt;Inter-Pausal Units (IPUs)&lt;/strong&gt;, which are stretches of audio from one speaker without any silence exceeding a stipulated amount(say, 200 ms). A voice activity detection(VAD) can detect these IPUs. Hence, a turn can be considered as a sequence of IPUs from a speaker, that are not interrupted by IPUs from another speaker.&lt;/p&gt;

&lt;p&gt;To identify TRPs(turn-yielding cues) and non-TRPs(turn-hlding)cues, many cues such as syntactic completion, prosody and non-verbal cues like eye-contact have been investigated. However, it is very complicated to directly detect such cues from the data. This problem is compounded by the absence of facial cues in our data. End of utterance task can be also defined as the detection of TRPs, i.e. when the user’s turn is yielded and the system can start to speak. There are a multitude of works done in this regard, that can be divided into three types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Silence-based models. The end of the user’s utterance is detected using a VAD. A silence duration threshold is used to determine when to take the turn. 
As discussed above, this is too simplistic and can lead to misrecognitions.&lt;/li&gt;
  &lt;li&gt;IPU-based models. Potential turn-taking points (IPUs) are detecting using a VAD. Turn-taking cues in the user’s speech are processed to determine whether the turn is yielded or not (potentially also considering the length of the pause).&lt;/li&gt;
  &lt;li&gt;Continuous models. The user’s speech is processed continuously to find suitable places to take the turn, but also for identifying backchannel relevant places (BRP), or for making projections.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/16001446/165028917-d3639f4c-8fa9-44d9-88ec-5dd0928f325a.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will go through each of the approaches in the following sections:&lt;/p&gt;

&lt;h2 id=&quot;silence-based-models&quot;&gt;Silence-based models&lt;/h2&gt;

&lt;p&gt;As mentioned above, existing architectures use a fixed silence duration detection threshold to determine if the speech has ended. VAD utilizes energy and spectral features to distinguish between noise and speech in the audio. Two types of parameters are taken into consideration while designing these kinds of models.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;After the system has yielded the turn, it awaits a user response, allowing for a certain silence (a gap). If this silence exceeds the
no-input-timeout threshold (such as 5 s), the system should continue speaking, for example by repeating the last question.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once the user has started to speak, the end-silence-timeout (such as 700ms) marks the end of the turn. As the figure shows,
this allows for brief pauses (shorter than the end-silence-timeout) within the user’s speech.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/16001446/166442067-1e01892b-de3a-483a-998b-d9aa8b838345.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These simplistic models break down if the user takes too long to respond. Or when the system might interrupt the user’s speech.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/16001446/166442480-fced182c-1d42-4af0-be17-842254c4236a.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tuning the threshold for different domains is extremely difficult and user satisfaction will be affected.&lt;/p&gt;

&lt;h2 id=&quot;ipu-based-models&quot;&gt;IPU-based models&lt;/h2&gt;

&lt;p&gt;These systems are built on an assumption that the system should not start to speak while the user is speaking. Turn-taking cues at the end of pauses are used to determine whether a turn has ended. These approaches run the gamut from hand-crafted rule-based semantic parsers to machine-learning and reinforcement learning models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.cmu.edu/afs/cs/Web/People/dod/papers/sato-icslp02.pdf&quot;&gt;Sato et al’s&lt;/a&gt; work inputs over 100 different kinds of features like syntactic, semantic, final word, and prosody to decision trees to model when to take a turn. Albeit simplistic, their model achieved an accuracy of 83.9%, compared to the baseline of 76.2%. However, this approach can misclassify the IPU as a pause and uses a fixed threshold of 750 ms for pauses. To overcome this limitation, &lt;a href=&quot;https://www.sri.com/wp-content/uploads/2021/12/is_the_speaker_done_yet.pdf&quot;&gt;Ferrer et al&lt;/a&gt; condition a decision-tree classifier on the length of the pause after IPU continuously and classify on the prosodic features and n-grams of the words. &lt;a href=&quot;https://aclanthology.org/W08-0101.pdf&quot;&gt;Raux and Eskenazi&lt;/a&gt; cluster silences based on dialogue features and set a single threshold for each cluster, minimizing the overall latency by over 50% on the Let’s Go dataset.&lt;/p&gt;

&lt;p&gt;Another shortcoming with the above approaches is that they are trained on human-computer dialogue corpus. But we want to learn a model for human-human dialogues. Transferring models from human-human to human-computer based systems is not feasible. So, some authors like (&lt;a href=&quot;https://aclanthology.org/W08-0101.pdf&quot;&gt;Raux, Eskenazi&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.704.2085&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Meena et al.&lt;/a&gt; use &lt;strong&gt;bootstrapping&lt;/strong&gt;. First, a more simplistic model of turn-taking is implemented in a system and interactions are recorded. Then, the data is then manually annotated with suitable TRPs, and trained using a machine learning model like LSTM. Another approach is a &lt;strong&gt;Wizard-of-Oz&lt;/strong&gt; setup, where a hidden operator controls the system and makes the turn-taking decisions as used in &lt;a href=&quot;https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/55075/Maier%20et%20al.%202017.%20Towards%20Deep%20End-of-Turn%20Prediction.pdf?sequence=1&quot;&gt;Maier et al.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Some previous approaches utilize reinforcement learning as well. For example, &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.6018&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Jonsdottir et al&lt;/a&gt; train two agents to talk to each other, picking up prosodic cues and develop turn-taking skills. &lt;a href=&quot;https://aclanthology.org/W15-4643.pdf&quot;&gt;Khouzaimi et al.&lt;/a&gt; train a dialogue management model intending to minimize the dialogue duration and maximize the completion task ratio. But these approaches are trained in simulated environments and it is unclear if they transfer to real users.&lt;/p&gt;

&lt;h2 id=&quot;continuous-models&quot;&gt;Continuous models&lt;/h2&gt;

&lt;p&gt;Continuous models process the utterances in an incremental manner. These modules process the input frame-by-frame and pass their results to subsequent modules. It enables the system to make continuous TRP predictions, project turn completions and backchannels. Unlike previous approaches, the processing starts before the input is complete. The processing time is improved, and the output becomes more &lt;em&gt;natural&lt;/em&gt;. There is no need to train the model for end-of-turn detection. It enables a deeper understanding of utterances and project backchannels and even interrupt the user.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/16001446/165454581-fceb250f-342f-4ca8-981d-bd635b922478.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of the first works in incremental processing was &lt;a href=&quot;https://aclanthology.org/E09-1085.pdf&quot;&gt;Skantze and Schlangen&lt;/a&gt; on the task of number dictation. A benefit of incremental models is revision, as shown by  &lt;a href=&quot;https://www.researchgate.net/profile/Gabriel-Skantze/publication/257267620_Towards_incremental_speech_generation_in_conversational_systems/links/5c473188299bf12be3db10e6/Towards-incremental-speech-generation-in-conversational-systems.pdf&quot;&gt;Skantze and Hjalmarsson&lt;/a&gt;. For example, the word “four” might be amended with more speech, resulting in a revision to the word “forty”.&lt;/p&gt;

&lt;p&gt;Another work by &lt;a href=&quot;https://www.diva-portal.org/smash/get/diva2:1141130/FULLTEXT01.pdf&quot;&gt;Skantze&lt;/a&gt; doesn’t train the model for end-of-turn detection. The audio from the speakers is processed frame-by-frame (20 frames per second) and fed to an LSTM. The LSTM predicts the speech activity for the two speakers for each frame in a future 3s window. The model outperforms human judges in this task. In an extension to this work, &lt;a href=&quot;https://arxiv.org/pdf/1808.10785.pdf&quot;&gt;Roddy et al.&lt;/a&gt; propose a new LSTM architcture where the acoustic and linguistic features get processed in separate LSTM systems with different timescales.&lt;/p&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;p&gt;Most of the aforementioned works evaluate their performance on dialogue based datasets like:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://groups.inf.ed.ac.uk/cgi/maptask/estimate.cgi&quot;&gt;HCRC MapTask Corpus&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mahnob-db.eu/mimicry/&quot;&gt;Mahnob Corpus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;that have a limited purpose and may not generalize well to our problem.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While significant work has been done in end-of-utterance detection, most of these models have shortcomings. Firstly, most are trained on dialogue-based datasets only without accounting for speech-level features. Secondly, these datasets are well-curated with less noise in the background which is not the case for our datasets. To account for noise and model audio and text jointly, we will need to retrain our models with new baselines.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.lti.cs.cmu.edu/sites/default/files/research/thesis/2008/antoine_raux_flexible_turn-taking_for_spoken_dialog_systems.pdf&quot;&gt;Flexible Turn-Taking for Spoken Dialog Systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S088523082030111X&quot;&gt;Turn-taking in Conversational Systems and Human-Robot Interaction: A Review&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/afs/cs/Web/People/dod/papers/sato-icslp02.pdf&quot;&gt;Learning decision trees to determine turn-taking by spoken dialogue systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.384.968&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Rhythms of Dialogue.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pure.mpg.de/rest/items/item_2376846/component/file_2376845/content&quot;&gt; simplest systematics for the organization of turn-taking for conversation.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.diva-portal.org/smash/get/diva2:1141130/FULLTEXT01.pdf&quot;&gt;Towards a general, continuous model of turn-taking in spoken dialogue using LSTM recurrent neural networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sri.com/wp-content/uploads/2021/12/is_the_speaker_done_yet.pdf&quot;&gt;IS THE SPEAKER DONE YET? FASTER AND MORE ACCURATE END-OF-UTTERANCE DETECTION USING PROSODY&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aclanthology.org/W08-0101.pdf&quot;&gt;Optimizing Endpointing Thresholds using Dialogue 2Features in a Spoken Dialogue System&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/55075/Maier%20et%20al.%202017.%20Towards%20Deep%20End-of-Turn%20Prediction.pdf?sequence=1&quot;&gt;Towards Deep End-of-Turn Prediction for Situated Spoken Dialogue Systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.6018&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Learning smooth, human-like turntaking in realtime dialogue&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aclanthology.org/W15-4643.pdf&quot;&gt;Optimising Turn-Taking Strategies With Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aclanthology.org/E09-1085.pdf&quot;&gt;Incremental Dialogue Processing in a Micro-Domain &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Gabriel-Skantze/publication/257267620_Towards_incremental_speech_generation_in_conversational_systems/links/5c473188299bf12be3db10e6/Towards-incremental-speech-generation-in-conversational-systems.pdf&quot;&gt;Towards incremental speech generation in conversational systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.diva-portal.org/smash/get/diva2:1141130/FULLTEXT01.pdf&quot;&gt;Towards a General, Continuous Model of Turn-taking in Spoken Dialogue using LSTM Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1808.10785.pdf&quot;&gt;Multimodal Continuous Turn-Taking Prediction Using Multiscale RNNs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>anirudhthatipelli</name></author><category term="Machine Learning" /><category term="end-of-utterance" /><category term="turn-taking" /><summary type="html">This blog post is based on the work done by Anirudh Thatipelli as an ML research fellow at Skit.ai</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/demo1.jpg" /><media:content medium="image" url="/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">TTS Enhancement</title><link href="/woc/" rel="alternate" type="text/html" title="TTS Enhancement" /><published>2022-03-09T00:00:00+00:00</published><updated>2022-03-09T00:00:00+00:00</updated><id>/woc</id><content type="html" xml:base="/woc/">&lt;h1 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h1&gt;

&lt;p&gt;Text-To-Speech (TTS) systems of Skit, as well as TTS systems in general, have a tendency to mix some ambient noise along with the speech it outputs. This aim of this research project was to remove that noise and quantify how well the noise has been removed using standard metrics.&lt;/p&gt;

&lt;p&gt;Listen to the clean speech sample here for reference-&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws4 = WaveSurfer.create({
           container: '#waveform-4',
           backend: 'MediaElement'
         });
         ws4.load('/assets/audios/posts/woc/sp01.wav');

         ws4.on('audioprocess', function () {
           let progressText = ws4.getCurrentTime().toFixed(2) + ' / ' + ws4.getDuration().toFixed(2)
           document.getElementById('player-progress-4').innerHTML = progressText
         });

         ws4.on('ready', function () {
           let progressText = ws4.getCurrentTime().toFixed(2) + ' / ' + ws4.getDuration().toFixed(2)
           document.getElementById('player-progress-4').innerHTML = progressText
         });

         ws4.on('finish', function () {
           let button = $('#controls-4 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-4').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws4.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws4.skipBackward()
                 break
               case 'forward':
                 ws4.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-4&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-4&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-4&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;p&gt;and the distorted sample-&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws5 = WaveSurfer.create({
           container: '#waveform-5',
           backend: 'MediaElement'
         });
         ws5.load('/assets/audios/posts/woc/sp01_car_sn0.wav');

         ws5.on('audioprocess', function () {
           let progressText = ws5.getCurrentTime().toFixed(2) + ' / ' + ws5.getDuration().toFixed(2)
           document.getElementById('player-progress-5').innerHTML = progressText
         });

         ws5.on('ready', function () {
           let progressText = ws5.getCurrentTime().toFixed(2) + ' / ' + ws5.getDuration().toFixed(2)
           document.getElementById('player-progress-5').innerHTML = progressText
         });

         ws5.on('finish', function () {
           let button = $('#controls-5 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-5').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws5.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws5.skipBackward()
                 break
               case 'forward':
                 ws5.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-5&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-5&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-5&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Speech enhancement can be done using the traditional signal processing techniques or using deep learning techniques. In this project, we mainly focused on the signal processing aspects of noise reduction. Signal processing techniques can be further divided into 3 more categories-&lt;/p&gt;

&lt;h2 id=&quot;spectral-subtractive-algorithms&quot;&gt;Spectral Subtractive algorithms&lt;/h2&gt;

&lt;p&gt;The main principle is as follows- assuming additive noise, one can obtain an estimate of the clean signal spectrum by subtracting an estimate of the noise spectrum from the noisy speech spectrum. The noise spectrum can be estimated and updated, during periods when the signal is absent. The assumption made is that noise is stationary or a slowly varying process and that the noise spectrum does not change significantly between the updating periods. The enhanced signal is obtained by computing the IDFT of the estimated signal spectrum using the phase of the noisy signal.&lt;/p&gt;

&lt;h2 id=&quot;statistical-model-based-algorithms&quot;&gt;Statistical Model based algorithms&lt;/h2&gt;

&lt;p&gt;Given a set of measurements that depend on an unknown parameter, we wish to find a nonlinear estimator of the parameter of interest. These measurements correspond to the set of DFT coefficients of the noisy signal and the parameters of interest are the set of DFT coefficients of the clean signal. Various techniques from estimation theory which include maximum-likelihood (ML) estimators and the Bayesian estimators like MMSE and MAP estimators are used for this purpose.&lt;/p&gt;

&lt;h2 id=&quot;subspace-algorithms&quot;&gt;Subspace algorithms&lt;/h2&gt;

&lt;p&gt;These algorithms are based on the principle that the clean signal might be confined to a subspace of the noisy Euclidean space. Given a method for decomposing the vector space of the noisy signal into a direct sum of the subspace that is occupied by the clean signal and a subspace occupied by the noise signal, for example SVD, we could estimate the clean signal simply by nulling the component of the noisy vector residing in the noisy subspace.&lt;/p&gt;

&lt;h1 id=&quot;contributions&quot;&gt;Contributions&lt;/h1&gt;
&lt;h2 id=&quot;filters&quot;&gt;Filters&lt;/h2&gt;

&lt;p&gt;Speech enhancement can be done using the traditional signal processing techniques or using deep learning techniques. We hypothesised that signal processing techniques would be suitable for task and tested them out. We implemented some of the popular speech enhancement methods which were suitably modified to tackle the problem at hand.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wiener Filter&lt;/strong&gt;&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Block Diagram of Wiener Filter&quot; src=&quot;/assets/images/posts/woc/wiener.png&quot; /&gt;
  &lt;figcaption&gt;Block Diagram of Wiener Filter&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The input signal w[n] goes through a linear and time-invariant system to produce an output signal x[n]. We are to design the system in such a way that the output signal, x[n], is as close to the desired signal, s[n], as possible. This can be done by computing the estimation error, e[n], and making it as small as possible. The optimal filter that minimizes the estimation error is called the &lt;em&gt;Wiener filter.&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws6 = WaveSurfer.create({
           container: '#waveform-6',
           backend: 'MediaElement'
         });
         ws6.load('/assets/audios/posts/woc/wiener_filtered_sp01_car_sn0.wav');

         ws6.on('audioprocess', function () {
           let progressText = ws6.getCurrentTime().toFixed(2) + ' / ' + ws6.getDuration().toFixed(2)
           document.getElementById('player-progress-6').innerHTML = progressText
         });

         ws6.on('ready', function () {
           let progressText = ws6.getCurrentTime().toFixed(2) + ' / ' + ws6.getDuration().toFixed(2)
           document.getElementById('player-progress-6').innerHTML = progressText
         });

         ws6.on('finish', function () {
           let button = $('#controls-6 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-6').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws6.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws6.skipBackward()
                 break
               case 'forward':
                 ws6.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-6&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-6&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-6&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;MMSE and MMSE Log Filter&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These fall under the umbrella of Bayesian estimation techniques. We saw above that the Wiener estimator can be derived by minimizing the error between a linear model of the clean spectrum and the true spectrum. The Wiener estimator is considered to be the optimal (in the mean-square-error sense) complex spectral estimator, but is not the optimal spectral magnitude estimator. Acknowledging the importance of the short-time spectral amplitude (STSA) on speech intelligibility and quality, several authors have proposed optimal methods for obtaining the spectral amplitudes from noisy observations. In particular, we are looking for sought that minimized the mean-square error between the estimated and true magnitudes:&lt;/p&gt;

\[e = E{ (\hat{X_k} - X_k)^2 }\]

&lt;p&gt;where \(\hat{X_k}\) is the estimate spectral magnitude at frequency \(\omega_k\) and \(X_k\) is the true magnitude of the clean signal.&lt;/p&gt;

&lt;p&gt;The MMSE Log is an improvement upon the MMSE estimator. Although a metric
based on the squared error of the magnitude spectra is mathematically tractable, it may not be subjectively meaningful. It has been suggested that a metric based on the squared error of the log-magnitude spectra may be more suitable for speech processing. So we minimize :&lt;/p&gt;

\[e = E \{ (log \hat X_k - log X_k)^2 \}\]

&lt;p&gt;and we notice a significant improvement in the results compared to the original MMSE estimator.&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws7 = WaveSurfer.create({
           container: '#waveform-7',
           backend: 'MediaElement'
         });
         ws7.load('/assets/audios/posts/woc/mmse_filtered_sp01_car_sn0.wav');

         ws7.on('audioprocess', function () {
           let progressText = ws7.getCurrentTime().toFixed(2) + ' / ' + ws7.getDuration().toFixed(2)
           document.getElementById('player-progress-7').innerHTML = progressText
         });

         ws7.on('ready', function () {
           let progressText = ws7.getCurrentTime().toFixed(2) + ' / ' + ws7.getDuration().toFixed(2)
           document.getElementById('player-progress-7').innerHTML = progressText
         });

         ws7.on('finish', function () {
           let button = $('#controls-7 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-7').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws7.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws7.skipBackward()
                 break
               case 'forward':
                 ws7.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-7&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-7&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-7&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws8 = WaveSurfer.create({
           container: '#waveform-8',
           backend: 'MediaElement'
         });
         ws8.load('/assets/audios/posts/woc/mmse_log_filtered_sp01_car_sn0.wav');

         ws8.on('audioprocess', function () {
           let progressText = ws8.getCurrentTime().toFixed(2) + ' / ' + ws8.getDuration().toFixed(2)
           document.getElementById('player-progress-8').innerHTML = progressText
         });

         ws8.on('ready', function () {
           let progressText = ws8.getCurrentTime().toFixed(2) + ' / ' + ws8.getDuration().toFixed(2)
           document.getElementById('player-progress-8').innerHTML = progressText
         });

         ws8.on('finish', function () {
           let button = $('#controls-8 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-8').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws8.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws8.skipBackward()
                 break
               case 'forward':
                 ws8.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-8&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-8&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-8&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Berouti’s Oversubstraction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This method consists of subtracting an overestimate of the noise power spectrum, while preventing the resultant spectral components from going below a preset minimum value (spectral floor).&lt;/p&gt;

\[\hat X(\omega)=\begin{cases} |Y(\omega)|^2 - \alpha |\hat D(\omega)|^2&amp;amp; \text{if } |Y(\omega)|^2 \geq (\alpha + \beta) |D(\omega)|^2 \\ \beta |\hat D(\omega)|^2 &amp;amp; \text{else} \end{cases}\]

&lt;p&gt;where \(\alpha (\geq 1)\) is the oversubtraction factor and \(0 \leq \beta \leq 1\) is the spectral floor parameter.&lt;/p&gt;

&lt;p&gt;When we subtract the estimate of the noise spectrum from the noisy speech
spectrum, there remain peaks in the spectrum. Some of those peaks are broadband (encompassing a wide range of frequencies) whereas others are narrow band, appearing as spikes in the spectrum. By oversubtracting the noise spectrum, that is, by using \(\alpha\), we can reduce the amplitude of the broadband peaks and, in some cases, eliminate them altogether. This by itself, however, is not sufficient because the deep valleys surrounding the peaks still remain in the spectrum. For that reason, spectral flooring is used to “fill in” the spectral valleys and possibly mask the remaining peaks by the neighbouring spectral components of comparable value. The valleys between peaks are no longer deep when \(\beta &amp;gt; 0\) compared to when \(\beta = 0\).&lt;/p&gt;

&lt;p&gt;The parameter \(\beta\) controls the amount of remaining residual noise
and the amount of perceived musical noise. If the spectral floor parameter \(\beta\) is too
large, then the residual noise will be audible but the musical noise will not be perceptible. Conversely, if \(\beta\) is too small, the musical noise will become annoying but the residual noise will be markedly reduced.&lt;/p&gt;

&lt;p&gt;The parameter \(\alpha\) affects the amount of speech spectral distortion caused by
the subtraction. If \(\alpha\) is too large, then the resulting signal will be severely distorted to the point that intelligibility may suffer.&lt;/p&gt;

\[\alpha = \alpha_0 - \frac{3}{20}  \textit{SNR} : \text{ for} -5 \leq \textit{SNR} \leq -20\]

&lt;p&gt;where \(\alpha_0\) is the desired value of  \(\alpha\) at 0 dB SNR and the \(\textit{SNR}\) is the short term SNR estimated at each frame.&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws9 = WaveSurfer.create({
           container: '#waveform-9',
           backend: 'MediaElement'
         });
         ws9.load('/assets/audios/posts/woc/ss_filtered_sp01_car_sn0.wav');

         ws9.on('audioprocess', function () {
           let progressText = ws9.getCurrentTime().toFixed(2) + ' / ' + ws9.getDuration().toFixed(2)
           document.getElementById('player-progress-9').innerHTML = progressText
         });

         ws9.on('ready', function () {
           let progressText = ws9.getCurrentTime().toFixed(2) + ' / ' + ws9.getDuration().toFixed(2)
           document.getElementById('player-progress-9').innerHTML = progressText
         });

         ws9.on('finish', function () {
           let button = $('#controls-9 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-9').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws9.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws9.skipBackward()
                 break
               case 'forward':
                 ws9.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-9&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-9&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-9&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;p&gt;The Kalman filter is a general recursive state estimation technique which is modified to work on the speech denoising problem.&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws10 = WaveSurfer.create({
           container: '#waveform-10',
           backend: 'MediaElement'
         });
         ws10.load('/assets/audios/posts/woc/kalman_filtered_sp01_car_sn0.wav');

         ws10.on('audioprocess', function () {
           let progressText = ws10.getCurrentTime().toFixed(2) + ' / ' + ws10.getDuration().toFixed(2)
           document.getElementById('player-progress-10').innerHTML = progressText
         });

         ws10.on('ready', function () {
           let progressText = ws10.getCurrentTime().toFixed(2) + ' / ' + ws10.getDuration().toFixed(2)
           document.getElementById('player-progress-10').innerHTML = progressText
         });

         ws10.on('finish', function () {
           let button = $('#controls-10 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-10').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws10.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws10.skipBackward()
                 break
               case 'forward':
                 ws10.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-10&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-10&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-10&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;h2 id=&quot;intelligibility-metrics&quot;&gt;Intelligibility Metrics&lt;/h2&gt;

&lt;p&gt;Along with techniques for speech enhancement, it is important to quantify the degree of enhancement which our methods provide. For this, we tested several metrics as discussed below-&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Perceptual Evaluation of Speech Quality (PESQ)&lt;/strong&gt; is a full-reference algorithm and analyzes the speech signal sample-by-sample after a temporal alignment of corresponding excerpts of reference and test signal. PESQ results essentially model mean opinion score (MOS) that cover a scale from 1 (bad) to 5 (excellent).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Short-Time Objective Intelligibility (STOI)&lt;/strong&gt; is an objective metric showing high correlation (\(\rho=0.95\)) with the intelligibility of both noisy, and TF-weighted noisy speech&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gross Pitch Error (GPE)&lt;/strong&gt; is the proportion of frames, considered voiced by both pitch tracker and ground truth, for which the relative pitch error is higher than a certain threshold, which is usually set to 20%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Voicing Error Decision (VED)&lt;/strong&gt; is the proportion of frames for which an incorrect voiced/unvoiced decision is made.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;F0 Frame Error (FFE)&lt;/strong&gt; is the proportion of frames for which an error (either according to the GPE or the VDE criterion) is made. FFE can be seen as a single measure for assessing the overall performance of a pitch tracker.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mel Cepstral Distortion (MCD)&lt;/strong&gt; is a measure of how different two sequences of mel cepstra are. It is used in assessing the quality of parametric speech synthesis systems, including statistical parametric speech synthesis systems, the idea being that the smaller the MCD between synthesized and natural mel cepstral sequences, the closer the synthetic speech is to reproducing natural speech.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;We apply our methods on 2 different datasets: First on the public &lt;a href=&quot;https://ecs.utdallas.edu/loizou/speech/noizeus/&quot;&gt;NOIZEUS&lt;/a&gt; dataset and next on a dataset created by the in-house TTS systems of Skit. The results are quite satisfactory when we apply our methods on the NOIZEUS dataset and we found that the Wiener Filter and the Kalman Filters perform the best outperforming one another for different signal-to-noise ratios (SNR).&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;MCD metric&quot; src=&quot;/assets/images/posts/woc/res1.png&quot; /&gt;
  &lt;figcaption&gt;Effect of filters wrt MCD metric&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;PESQ metric&quot; src=&quot;/assets/images/posts/woc/res2.png&quot; /&gt;
  &lt;figcaption&gt;Effect of filters wrt PESQ metric&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;However they do not perform as well as we want on the TTS dataset. In fact, we observe that our models adversely affecting the input speech. There can be various reasons attributed to this, the primary one being that speech denoising of real life data and TTS Systems are quite different, since both have different noise types. Real life noise is either additive and can be subtracted by noise estimation or can be decomposed as a direct sum of a clean subspace and a pure noise subspace. But the noise in TTS systems are much more subtle and the noise cannot be modelled to be simply additive. Here the noise is generated along with the speech. Hence most of the traditional filters which although work well for real life noise separation, do not work well for this use case. This is where we planned to resort to deep learning models like the Facebook denoiser and SeGAN.&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;You can find more information on the &lt;a href=&quot;https://github.com/skit-ai/woc-tts-enhancement&quot;&gt;Github Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/publication/224738211_Enhancement_of_speech_corrupted_by_acoustic_noise&quot;&gt;Berouti’s Spectral Subtraction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ccrma.stanford.edu/~orchi/Documents/thesis_KF.pdf&quot;&gt;Kalman Filter for Speech Enhancement&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.perlego.com/book/2193587/speech-enhancement-theory-and-practice-second-edition-pdf&quot;&gt;Speech Enhancement by Philipos C. Loizou&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hal.archives-ouvertes.fr/hal-00923967/document&quot;&gt;A comparative study of pitch extraction algorithms on a large variety of singing sounds&lt;/a&gt;, &lt;a href=&quot;https://ieeexplore.ieee.org/document/941023&quot;&gt;PESQ&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Author: Ananyapam De, a final year student at IISER Kolkata, majoring in Statistics, while minoring in the Computational Sciences.&lt;/p&gt;</content><author><name></name></author><category term="Machine Learning" /><category term="TTS" /><category term="speech-enhancement" /><summary type="html">Problem Statement</summary></entry><entry><title type="html">Turn Taking Dynamics in Voice Bots</title><link href="/Turn_Taking_Dynamics_in_Voice_Bots/" rel="alternate" type="text/html" title="Turn Taking Dynamics in Voice Bots" /><published>2022-03-07T00:00:00+00:00</published><updated>2022-03-07T00:00:00+00:00</updated><id>/Turn_Taking_Dynamics_in_Voice_Bots</id><content type="html" xml:base="/Turn_Taking_Dynamics_in_Voice_Bots/">&lt;p&gt;One of the challenges in building an interactive voice bots is accounting for turn taking behaviour. Turn-taking is a difficult problem to get right, even for humans. In all our circles, we’d know of at least one person who likes to interrupt a lot and doesn’t have good turn taking etiquette.  Having a conversation with such a person can be quite irritating as one feels one is not getting heard or even getting a chance to finish one’s sentence.&lt;/p&gt;

&lt;p&gt;Turn-taking is even more difficult in a multi-party setting. You might remember the last group call you had and just when you were about to take the turn, someone else jumped right in (because you waited for a tad bit too long) and you never got to speak. Turn-taking behaviour also differs culturally. In some cultures, interruptions and barge-ins are a lot more natural. There is also a difference in the inter-turn pause duration. These factors often lead to an unnatural conversation flow when speaking to a person from a different culture.&lt;/p&gt;

&lt;p&gt;Note : Bots with explicit turn-taking signalling like wake-words are out of scope for this blog.&lt;/p&gt;

&lt;h2 id=&quot;natural-turn-taking-dynamics&quot;&gt;Natural Turn Taking Dynamics&lt;/h2&gt;

&lt;p&gt;Irrespective of nuances, there are aspects of turn taking behaviour which are globally present in natural human-human conversation and one’s that we would want to imbibe in a human-bot interaction as well.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Barge-ins: These are situations when one agent interrupts the other. They occur very commonly. Examples of situations are : when one feels the other person is making a mistake or when ones feels the need to add some essential information, one naturally barges in.&lt;/li&gt;
  &lt;li&gt;Full Duplex Conversations : A half duplex conversation is one where turns are alternatively taken, like playing a tennis match, however in natural conversations, there are often instances when both people are saying something at the same time.
    &lt;ul&gt;
      &lt;li&gt;backchannels : words and fillers like “okay”, “alright” or “hmm” provide a lot of context about the state of the other person(for example attentiveness), especially when one is talking over the phone and visual cues are absent.&lt;/li&gt;
      &lt;li&gt;corrections : at times, when a person is saying something, one might want to make a small correction. For example, if there is an announcement being made “for the next meeting, you are supposed to finish submissions by 12th December, at so and so time….”. When the person is saying 12th December someone might correct by saying 13th December. This information is assimilated by the person and they often correct themselves. So, humans have the ability to hear and understand even while speaking and are active listeners.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/turn-taking-dynamics/duplex-conversations.png&quot; /&gt;
    &lt;figcaption&gt;Fig 1: Full duplex vs half duplex conversations.&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Minimal inter-turn pauses : if you’ve ever spoken with a voice assistant, one of the first observations is that it takes too long to start speaking after you are done and the other way around. Human conversations have a much lower turn taking latency. If this latency is near optimal, it also lends to a feeling that the other person is understanding you and the left over impression is that of a conversation gone well. Human’s have an average pause duration of 200ms as shown below, while bots have a much higher latency.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/turn-taking-dynamics/pause-duration.png&quot; /&gt;
    &lt;figcaption&gt;Fig 2: Turn Taking Pause duration as measured from the Switchboard corpus. Image is taken from [1].&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Turn taking cues : often in natural conversations, people produce small vocal cues like filler words “umm” or “uhhh” to convey that they want to say something and take the turn.&lt;/li&gt;
  &lt;li&gt;Turn yielding cues : there are markers is conversations when one knows that the person is done speaking. This is how we are able to separate pauses, which happen when a person is thinking in between his utterance vs one when he is done speaking.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;turn-taking-dynamics-in-voice-bots&quot;&gt;Turn-Taking Dynamics in Voice Bots&lt;/h2&gt;

&lt;p&gt;Below, we discuss different versions of turn-taking dynamics implemented in voice-bots each with more features and increasing levels of difficulty.&lt;/p&gt;

&lt;h3 id=&quot;version---10&quot;&gt;Version - 1.0&lt;/h3&gt;

&lt;p&gt;These are some characteristics of a bare bone turn-taking behaviour that one would need in a voice bot deployment.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Initial patience : the time that the bot waits for the person to starts speaking&lt;/li&gt;
  &lt;li&gt;Silence detection : if the bot detects silence for a certain duration after the person has started speaking, it assumes the person’s turn is over.&lt;/li&gt;
  &lt;li&gt;Max turn duration : it doesn’t make sense to just be listening (because of error compounding, loss of context, maybe one is hearing just noise), so usually voice bots have a maximum duration to which they listen to the user.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;version---20&quot;&gt;Version - 2.0&lt;/h3&gt;

&lt;p&gt;This version add robustness for real life situations, make the bot more human-like and tries to reduce the latency between turns.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;VAD instead of silence detection : Often existence of background noise, speech and other signals causes the bot to keep listening. Instead one could train a Voice-Activity Detection system rather than use silence detection, to have robustness to background events and to listen to the user only when they are speaking.&lt;/li&gt;
  &lt;li&gt;Variable thresholds for silence detection and max duration : In some states for example, when the bot is expecting a yes/no answer, it makes sense to use smaller thresholds. In general dynamic thresholding should be used.&lt;/li&gt;
  &lt;li&gt;For turn-switching, instead of a simple VAD, use an IPU based model discussed &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S088523082030111X&quot;&gt;here&lt;/a&gt;. This uses a smaller VAD threshold + cues to predict the turn is over. One could start with some verbal cues for example phrase completion.&lt;/li&gt;
  &lt;li&gt;Adding backchannels as bot responses : So far we’ve only discussed aspects of perception, but backchannels are a very useful response feature. It makes the user feel that the bot is more attentive and is actively listening.
    &lt;ul&gt;
      &lt;li&gt;One could also add filler words in the main channel, when the bot is taking too long to produce a response in cases of high latency. This would prevent the user from asking a question to verify if the bot is there or not. Without this, the user’s speech would lead to further increase in latency as it would be perceived as a case when the user wants to take the turn and say something useful.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;version---30&quot;&gt;Version - 3.0&lt;/h3&gt;

&lt;p&gt;There are no good baselines for these and working on improvements would constitute state of the art performance.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-party situations : These are a lot more complex and require modelling multiple parties. An application could be when the bot is overseeing a human-machine interaction say, between a call centre agent and a human. Another common use is when during a typical 2 party interaction, someone interrupts the user. This requires the bot being aware that the user is speaking to someone else and then waiting.&lt;/li&gt;
  &lt;li&gt;Full - Duplex Conversations : Unlike human-human conversations a bots can attentively listen at the same time, while saying something. This offers a possibility of redesigning interactions which can leverage this feature.&lt;/li&gt;
  &lt;li&gt;Personalisation of Turn taking behaviour : This involves changing the parameters based on user characteristics. One could entrain one’s system to be more in line with the user’s behaviour. At times when the user is angry it might involve changing the durations to feel that they are being heard.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references-&quot;&gt;References :&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S088523082030111X&quot;&gt;Turn-taking in Conversational Systems and Human-Robot Interaction: A Review&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Swaraj Dalmia</name></author><category term="Machine Learning" /><category term="Turn-taking" /><category term="barge-in" /><category term="duplex conversations" /><summary type="html">One of the challenges in building an interactive voice bots is accounting for turn taking behaviour. Turn-taking is a difficult problem to get right, even for humans. In all our circles, we’d know of at least one person who likes to interrupt a lot and doesn’t have good turn taking etiquette. Having a conversation with such a person can be quite irritating as one feels one is not getting heard or even getting a chance to finish one’s sentence.</summary></entry><entry><title type="html">Feature Disentanglement - I</title><link href="/feature-disentanglement1/" rel="alternate" type="text/html" title="Feature Disentanglement - I" /><published>2022-02-22T00:00:00+00:00</published><updated>2022-02-22T00:00:00+00:00</updated><id>/feature-disentanglement1</id><content type="html" xml:base="/feature-disentanglement1/">&lt;p&gt;The main advantage of deep learning is the ability to learn from the data in an end-to-end manner. The core of deep learning is representation, the deep learning models transform the representation of the data at each layer into a condensed representation with reduced dimension. Deep Learning models are often also termed as black-box models as these representations are difficult to interpret, understanding these representations can give us an insight about which feature of the data is more important and will allow us to control the learning process. Recently there has been a lot of interest in representation learning and controlling the learned representations which give an edge over multiple tasks like controlled synthesis, better representations for specific downstream tasks.&lt;/p&gt;

&lt;h1 id=&quot;data-representation-and-latent-code&quot;&gt;Data Representation and Latent Code&lt;/h1&gt;
&lt;p&gt;An image \((x)\) from the MNIST dataset has 28x28 = 784 dimensions which is a sparse representation of the image that can be visualized. But all these dimensions are not required to represent the image. The content of the images can be represented in a condensed form using lesser dimensions called latent code. Although the actual image has 784 dimensions \(x \in R^{784}\), one way of representing MNIST image can be with just an integer ie: \(z \in \{0, 1, 2, …, 9\}\). This representation \(z\) reduces the dimension of representing the image \(x\) to 1 which captures the content of which number is present in the image and the variability in the dataset. This is one example of discrete latent code for the MNIST dataset, a continuous latent code will contain more information about the image such as the style of the image, position of the number, size of the number in the image, etc.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;600&quot; height=&quot;300&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://www.mdpi.com/applsci/applsci-09-03169/article_deploy/html/images/applsci-09-03169-g001.png&quot; /&gt;
  &lt;figcaption&gt;Fig 2:  Sample Images of MNIST from [1]&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;autoencoder&quot;&gt;AutoEncoder&lt;/h2&gt;

&lt;p&gt;Autoencoder[2] models are popularly used to learn such latent code in an unsupervised manner by compressing the image to a fixed dimension code \(z\) and generating the image back using this latent code with an encoder-decoder model.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://d3i71xaburhd42.cloudfront.net/08b0b21725c236fb1860285677a00248f77c7587/2-Figure1-1.png&quot; /&gt;
  &lt;figcaption&gt;Fig 2: Autoencoder architecture from Autoencoders[2]&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The encoder \(q_{\phi}(z \mid x)\) of the autoencoder compresses the image to a fixed dimension\((d)\) latent code\((z)\), and the decoder \(p_{\theta}(x \mid z)\) is a conditional image generator. The dimension of z has to be such that, the image can be completely reconstructed by the decoder with the latent code. Choosing the dimension of the latent code is a problem on its own[3].&lt;/p&gt;

&lt;p&gt;The autoencoder models trained will successfully encode the images into a latent code \(z\), but there is no guarantee that the latent code can be easily inferred, ie: we do not know where in the d-dimensional space the model encoded the image into, and thus difficult to choose a latent code to generate image during inference. So the conclusion is we have no idea how and where the encoder encodes the images, so we do not have control over synthesis during inference. The following figure shows the latent code learned by the AutoEncoder model with different training, as we can observe the latent space keep changing the range and quadrant and thus difficult to infer.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/feature-disentanglement/fig1.png&quot; /&gt;
  &lt;figcaption&gt;Fig 3: Latent code of MNIST images learned by an Auto Encoder [4]&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;variational-autoencodervae&quot;&gt;Variational AutoEncoder(VAE)&lt;/h2&gt;

&lt;p&gt;Variational autoencoders(VAE) [6] solve this problem by forcing the latent code (z) to be close to a known prior distribution(Gaussian), this gives us control over the latent space. During inference, the latent space can be sampled from this known distribution for image generation. The following figure shows the latent code learned by VAE with different training, and the latent space across training is centered to the mean 0 across dimensions.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/feature-disentanglement/fig2.png&quot; /&gt;
  &lt;figcaption&gt;Fig 4: Latent code of MNIST images learned by a VAE [4]&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;VAE allows us to have control over the latent space and sample from the known prior distribution. But this again does not give us control over the generation of the image. Say if you want to generate an image of the number ‘3’ or ‘7’, you cannot do that(at least not directly). This is where the term “disentanglement” comes into play.&lt;/p&gt;

&lt;h1 id=&quot;disentanglement&quot;&gt;Disentanglement&lt;/h1&gt;
&lt;p&gt;Feature disentanglement is isolating the source of variation in observation data. There is a lot more factors/feature of an MNIST image other than the number itself, such as the location of the number in the image, size of the image, angle of the number, etc. These factors are independent of each other.&lt;/p&gt;

&lt;p&gt;Feature disentanglement involves separating underlying concepts of “Big one in the left”: ie: size(big), number(one), location(left).
Our interest here is to see if we can isolate these factors in the latent code so that we can have control over the generation of the images. So we want the encoder to disentangle the representation into different factors and then we generate the image with desired factors say “small seven in the top rotated 30 degrees”.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;800&quot; height=&quot;400&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://d3i71xaburhd42.cloudfront.net/35da0a2001eea88486a5de677ab97868c93d0824/6-Figure2-1.png&quot; /&gt;
  &lt;figcaption&gt;Fig 5: Generated MNIST images by InfoGAN [5] varied digit, thickness and roatation.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;beta-vae&quot;&gt;Beta-VAE&lt;/h2&gt;
&lt;p&gt;Beta-VAE is a variant of VAE which allows disentanglement of the learned latent code. Beta-VAE adds hyperparameter to the loss function which modulates the learning constraint of VAE.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;800&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://miro.medium.com/max/1400/1*Z6tj5bVoArekVgv65gfkfg.png&quot; /&gt;
  &lt;figcaption&gt;Fig 6: Loss function of beta-VAE [7].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;The first part of the loss function takes care of the reconstruction of the image, it is the second term that learns the latent code of VAE. Different dimensions that span across Gaussians are independent, so by making the prior distribution gaussian, we force the dimensions of the latent code to be independent of each other. So increasing the weight of the second part of the loss, makes the latent code to be disentangled and independent. But this also brings a tradeoff between disentanglement and the reconstruction capability of the VAE. Although Beta-VAE models are good in disentangling the features, the reconstruction ability of this model is not the best.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;800&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_4.00.13_PM.png&quot; /&gt;
  &lt;figcaption&gt;Fig 7: Samples generated by beta-VAE [7].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;beta-tcvae&quot;&gt;Beta-TCVAE&lt;/h2&gt;
&lt;p&gt;beta-TCVAE decomposes the KL divergence[10] term of the loss function of VAE into reconstruction loss, Index-code mutual information[8] between data and latent variable, Total Correlation[9] of z, and Dimension wise KL divergence[10] of \(z\)(respectively in the following formula). This helps to break the overall KL Divergence of \(z\) into dimension-wise quantities, which will focus on each dimension of the latent code \(z\). In this formulation, the beta hyperparameter is only on the Total Correlation term which is more important for disentanglement without affecting the reconstruction. So, Beta-TCVAE has better reconstruction ability than Beta-VAE with similar disentanglement property.&lt;/p&gt;

\[\mathcal{L}_{\beta-\mathrm{TC}}:=\mathbb{E}_{q(z \mid n) p(n)}[\log p(n \mid z)]-\alpha I_{q}(z ; n)-\beta \operatorname{KL}\left(q(z) \| \prod_{j} q\left(z_{j}\right)\right)-\gamma \sum_{j} \operatorname{KL}\left(q\left(z_{j}\right) \| p\left(z_{j}\right)\right)\]

&lt;p&gt;where \(\alpha = \gamma = 1\) and only \(\beta\) is varies as the hyperparameter.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img width=&quot;800&quot; alt=&quot;Can't See? Something went wrong!&quot; src=&quot;https://vitalab.github.io/article/images/IsolatingSourcesOfDisentanglementInVAEs/figure1.jpg&quot; /&gt;
  &lt;figcaption&gt;Fig 8: Samples generated by beta-TCVAE [8].&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;In future posts, we will examine many new methods for feature disentanglement and how these methods can be applied to speech signals.&lt;/p&gt;

&lt;h2 id=&quot;references-&quot;&gt;References :&lt;/h2&gt;

&lt;p&gt;[1] : &lt;a href=&quot;https://www.mdpi.com/2076-3417/9/15/3169/htm&quot;&gt;A Survey of Handwritten Character Recognition with MNIST and EMNIST&lt;/a&gt; (2019)&lt;/p&gt;

&lt;p&gt;[2] : &lt;a href=&quot;https://arxiv.org/abs/2003.05991&quot;&gt;Autoencoders&lt;/a&gt; (2021)&lt;/p&gt;

&lt;p&gt;[3] : &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0925231215015994&quot;&gt;Squeezing bottlenecks: Exploring the limits of autoencoder semantic representation capabilities&lt;/a&gt; (2016)&lt;/p&gt;

&lt;p&gt;[4] : &lt;a href=&quot;https://www.youtube.com/watch?v=itOlzH9FHkI&quot;&gt;Disentangled Representations - How to do Interpretable Compression with Neural Models&lt;/a&gt; (2020)&lt;/p&gt;

&lt;p&gt;[5] : &lt;a href=&quot;https://arxiv.org/abs/1606.03657&quot;&gt;InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets&lt;/a&gt; (2016)&lt;/p&gt;

&lt;p&gt;[6] : &lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot;&gt;Auto-Encoding Variational Bayes&lt;/a&gt; (2013)&lt;/p&gt;

&lt;p&gt;[7] : &lt;a href=&quot;https://openreview.net/forum?id=Sy2fzU9gl&quot;&gt;beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework &lt;/a&gt; (2017)&lt;/p&gt;

&lt;p&gt;[8] : &lt;a href=&quot;https://arxiv.org/abs/1802.04942&quot;&gt;Isolating Sources of Disentanglement in Variational Autoencoders&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[9] : &lt;a href=&quot;https://en.wikipedia.org/wiki/Total_correlation&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[10] : &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;</content><author><name>Shangeth Rajaa</name></author><category term="Machine Learning" /><summary type="html">The main advantage of deep learning is the ability to learn from the data in an end-to-end manner. The core of deep learning is representation, the deep learning models transform the representation of the data at each layer into a condensed representation with reduced dimension. Deep Learning models are often also termed as black-box models as these representations are difficult to interpret, understanding these representations can give us an insight about which feature of the data is more important and will allow us to control the learning process. Recently there has been a lot of interest in representation learning and controlling the learned representations which give an edge over multiple tasks like controlled synthesis, better representations for specific downstream tasks.</summary></entry><entry><title type="html">Google Summer of Code, 2022</title><link href="/gsoc-2022/" rel="alternate" type="text/html" title="Google Summer of Code, 2022" /><published>2022-02-18T00:00:00+00:00</published><updated>2022-02-18T00:00:00+00:00</updated><id>/gsoc-2022</id><content type="html" xml:base="/gsoc-2022/">&lt;h1 id=&quot;google-summer-of-code---2022&quot;&gt;Google Summer of Code - 2022&lt;/h1&gt;

&lt;p&gt;This page contains ideas which we’d like to get help from GSoC Contributors. But before all that, if you haven’t heard about Skit.&lt;/p&gt;

&lt;h2 id=&quot;what-is-skit&quot;&gt;What is Skit?&lt;/h2&gt;

&lt;p&gt;We are a series B funded, AI-first SaaS voice automation company specializing in delivering multilingual voice bots for contact center automation. We have our Speech Bots deployed in major banks and large enterprises in several verticals. We have a foothold in India and are expanding in the US and South-East Asia.&lt;/p&gt;

&lt;p&gt;We have been listed in &lt;a href=&quot;https://www.forbes.com/sites/johnkang/2021/04/19/the-forbes-30-under-30-asia-startups-unshackling-businesses-using-ai/?sh=9268fa85f9aa&quot;&gt;Forbes 30 Under 30 Asia 2021&lt;/a&gt; and have been named by Gartner as a &lt;a href=&quot;https://www.businesswire.com/news/home/20211221005315/en/Skit-Named-as-a-Cool-Vendor-in-Gartner-Cool-Vendors-in-Conversational-and-NLT-Widen-Use-Cases-Domain-Knowledge-and-Dialect-Support?utm_medium=email&amp;amp;_hsmi=202791220&amp;amp;_hsenc=p2ANqtz--yQEAO9Q810nr71J9I8MPppXkOWpWg51LqIrOdv_Wc2X_Hj-ydmia5ruRLbQEEat7EPQ6fn_GHMMVWu4tUV8beoU2BQA&amp;amp;utm_content=202791220&amp;amp;utm_source=hs_email&quot;&gt;Cool vendor in Conversational and NLT Widen Use Cases&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our goal is to build the most natural and robust multi lingual voice bot with state of the art human-machine interaction capabilities.&lt;/p&gt;

&lt;p&gt;We build voicebots, so that agents don’t have to sit and answer user queries 24/7 for 365 days.&lt;/p&gt;

&lt;p&gt;You can get to know more about us over &lt;a href=&quot;https://skit.ai/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Our tech blog is present &lt;a href=&quot;https://tech.skit.ai/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;communication&quot;&gt;Communication&lt;/h2&gt;

&lt;p&gt;You can reach out to us at our skit-gsoc community discord, link to join here : &lt;a href=&quot;https://discord.gg/Y9sJwz5Sw8&quot;&gt;https://discord.gg/Y9sJwz5Sw8&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;gsoc-2022-ideas&quot;&gt;GSoC 2022 Ideas&lt;/h2&gt;

&lt;h1 id=&quot;idea-1-enhancements-to-dialogy-via-core-code-or-plugins&quot;&gt;Idea 1: Enhancements to dialogy via core code or plugins.&lt;/h1&gt;

&lt;h3 id=&quot;project-description&quot;&gt;Project Description&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://skit-ai.github.io/dialogy/&quot;&gt;Dialogy&lt;/a&gt; is a framework to build machine-learning solutions for speech applications speech dialogue systems.&lt;/p&gt;

&lt;p&gt;The main principles which form the backbone of dialogy are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Plugin-based: Makes it easy to import/export components to projects.&lt;/li&gt;
  &lt;li&gt;Stack-agnostic: No assumptions made on ML stack; your choice of machine learning library will not be affected by using Dialogy.&lt;/li&gt;
  &lt;li&gt;Progressive: Minimal boilerplate writing to let you focus on your machine learning problems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At current shape, dialogy allows one to train models, test performance on metrics and deploy applications. We want to add more features which are desirable in an complete SLU framework. These features could include&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hyperparameter Tuning- There are multiple hyperparameters involved in using a transformers. A hyperparameter tuning integration would allow developer with a seamless way to experiment with hyperparams and train optimal models.&lt;/li&gt;
  &lt;li&gt;Model interpretability via Captum, LIT integration- Interpretability is necessary from both business and development perspectives. Such tools would allow developers to explain why a certain prediction was made, as well as discover biases and faults in the model.&lt;/li&gt;
  &lt;li&gt;Integration with experiment tracking platforms- It gets difficult to keep track of models being trained by developers- some are meant for production releases, some are meant for experimentation. An experiment tracking integration would enable developers to manage their models and results easily.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;github-links&quot;&gt;GitHub Link(s):&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/skit-ai/dialogy&quot;&gt;dialogy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/skit-ai/dialogy-template-simple-transformers&quot;&gt;dialogy-template-simple-transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expected-outcomes&quot;&gt;Expected Outcomes&lt;/h3&gt;

&lt;p&gt;Dialogy as a platform would have following features after the project&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Integrated with experiment tracking for better project management&lt;/li&gt;
  &lt;li&gt;Integrated with model explainability and interpretability frameworks allowing users to use these tools&lt;/li&gt;
  &lt;li&gt;Integrated with an hyperparameter tuning service/library&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;skills-required&quot;&gt;Skills Required&lt;/h3&gt;
&lt;p&gt;Understanding of Machine Learning frameworks like PyTorch, Huggingface, etc.
And of course Python.&lt;/p&gt;

&lt;h3 id=&quot;possible-mentors&quot;&gt;Possible Mentors&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Himansu - &lt;a href=&quot;https://www.linkedin.com/in/himansu-didwania/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/hdidwania&quot;&gt;github&lt;/a&gt;, email - himansu@skit.ai&lt;/li&gt;
  &lt;li&gt;Jaivarsan - &lt;a href=&quot;https://www.linkedin.com/in/jaivarsan-b-50264b148/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/greed2411&quot;&gt;github&lt;/a&gt;, email - jaivarsan@skit.ai&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;expected-size&quot;&gt;Expected Size&lt;/h3&gt;
&lt;p&gt;175 hours&lt;/p&gt;

&lt;h3 id=&quot;difficulty&quot;&gt;Difficulty&lt;/h3&gt;
&lt;p&gt;Medium&lt;/p&gt;

&lt;h1 id=&quot;idea-2-speaker-anonymization&quot;&gt;Idea 2: Speaker Anonymization&lt;/h1&gt;

&lt;h3 id=&quot;project-description-1&quot;&gt;Project Description&lt;/h3&gt;
&lt;p&gt;The goal of this project is to explore and implement methods to anonymize speech to remove the speaker information from the signal by distorting the speaker prosodic features or with techniques like voice conversion. The speaker information in the signal can be used to attack the speaker verification systems which leads to privacy and security concerns. The idea is to explore simple signal/speech processing techniques which are faster in both implementation and deployment, as well as neural methods which use deep learning models and architectures to resynthesis the speech signal with the original speaker’s information removed. Removing the speaker’s information can help us to use speech datasets without worrying about privacy attacks on the speakers in the dataset. This project involves the implementation of various research papers in the field and improving upon them and creating a python package for speaker anonymization.&lt;/p&gt;

&lt;h3 id=&quot;github&quot;&gt;GitHub&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/skit-ai/aunom&quot;&gt;aunom&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expected-outcomes-1&quot;&gt;Expected Outcomes&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Literature Review of research papers in the field&lt;/li&gt;
  &lt;li&gt;Implementation of various research papers which are interesting to the project in python.&lt;/li&gt;
  &lt;li&gt;Github repository for experiments and final python package for speaker anonymization with proper documentation.&lt;/li&gt;
  &lt;li&gt;Demonstration of the methods implemented and final package.&lt;/li&gt;
  &lt;li&gt;Talk/presentation on the work done during the GSoC period.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;skills-required-1&quot;&gt;Skills Required&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Experience in Signal/Speech processing (Preferred)&lt;/li&gt;
  &lt;li&gt;Experience in Deep learning for speech tasks like Automatic Speech Recognition, Speech synthesis, speaker-related tasks  etc.&lt;/li&gt;
  &lt;li&gt;Python and deep learning frameworks like PyTorch/TensorFlow.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;possible-mentors-1&quot;&gt;Possible Mentors&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Shangeth Rajaa - &lt;a href=&quot;https://www.linkedin.com/in/shangeth/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/shangeth&quot;&gt;github&lt;/a&gt;, email - shangeth.rajaa@skit.ai&lt;/li&gt;
  &lt;li&gt;Swaraj - &lt;a href=&quot;https://www.linkedin.com/in/swaraj-dalmia/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/swarajdalmia&quot;&gt;github&lt;/a&gt;, email - swaraj@skit.ai&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;expected-size-1&quot;&gt;Expected Size&lt;/h3&gt;
&lt;p&gt;175 Hours&lt;/p&gt;

&lt;h3 id=&quot;difficulty-1&quot;&gt;Difficulty&lt;/h3&gt;
&lt;p&gt;Medium&lt;/p&gt;

&lt;h1 id=&quot;idea-3-improving-kaldi-serve-performance&quot;&gt;Idea 3: Improving kaldi-serve performance&lt;/h1&gt;

&lt;h3 id=&quot;project-description-2&quot;&gt;Project Description&lt;/h3&gt;
&lt;p&gt;​​Kaldi Serve is a plug-and-play abstraction over the &lt;a href=&quot;https://kaldi-asr.org/&quot;&gt;Kaldi ASR&lt;/a&gt; toolkit, designed for ease of deployment and optimal runtime performance. It currently has the following key features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Real-time streaming (uni &amp;amp; bi-directional) audio recognition.&lt;/li&gt;
  &lt;li&gt;Thread-safe concurrent Decoder queue for server environments.&lt;/li&gt;
  &lt;li&gt;RNNLM lattice rescoring.&lt;/li&gt;
  &lt;li&gt;N-best alternatives with AM/LM costs, word-level timings, and confidence scores.&lt;/li&gt;
  &lt;li&gt;Easy extensibility for custom applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are mainly looking at improving the runtime performance of the ASR pipeline by offloading Decoder computation to the GPU to be performed in mini-batches, and implementing a request queueing mechanism within the gRPC server to be able to utilize the parallel computing capability in order to boost latencies under high concurrent loads.&lt;/p&gt;

&lt;h3 id=&quot;github-1&quot;&gt;GitHub&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/skit-ai/kaldi-serve&quot;&gt;kaldi-serve&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expected-outcomes-2&quot;&gt;Expected Outcomes&lt;/h3&gt;
&lt;p&gt;Integration with Kaldi Batched Threaded NNet3 CUDA pipeline for enabling batched computation in kaldi-serve gRPC application.&lt;/p&gt;

&lt;h3 id=&quot;skills-required-2&quot;&gt;Skills Required&lt;/h3&gt;
&lt;p&gt;Having some combination of these:
Languages: C++, CUDA, Python
Frameworks: Kaldi ASR, Pytorch, gRPC, Pybind11
Basics: Speech Recognition, Language Modeling, Deep Learning&lt;/p&gt;

&lt;h3 id=&quot;possible-mentors-2&quot;&gt;Possible Mentors&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Prabhsimran - &lt;a href=&quot;https://www.linkedin.com/in/pskrunner14/&quot;&gt;linkedin&lt;/a&gt;, &lt;a href=&quot;https://github.com/pskrunner14&quot;&gt;github&lt;/a&gt;, email - prabhsimran@skit.ai&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expected-size-2&quot;&gt;Expected Size&lt;/h3&gt;
&lt;p&gt;175 hours&lt;/p&gt;

&lt;h3 id=&quot;difficulty-2&quot;&gt;Difficulty&lt;/h3&gt;
&lt;p&gt;Medium&lt;/p&gt;</content><author><name>Jaivarsan B</name></author><category term="Machine Learning" /><category term="work" /><summary type="html">Google Summer of Code - 2022</summary></entry><entry><title type="html">Speaker Entrainment</title><link href="/speaker-entrainment/" rel="alternate" type="text/html" title="Speaker Entrainment" /><published>2022-02-04T00:00:00+00:00</published><updated>2022-02-04T00:00:00+00:00</updated><id>/speaker-entrainment</id><content type="html" xml:base="/speaker-entrainment/">&lt;p&gt;In this post, we will discuss the phenomenon of speaker entrainment and the insights we gained when designing a voice-bot that entrains on the user’s speech. This work was done by me as a ML Research Intern at Skit, supervised by Swaraj Dalmia.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Speaker Entrainment (also known as accomodation or alignment) is a psycho-social phenomenon that has been observed in human-human conversations in which interlocutors tend to match each other’s speech features. Believed to be crucial to the success and naturalness of human-human conversations, this can look like matching style related aspects such as pitch, rate of articulation or intensity, or content related factors, such as lexical patterns.&lt;/p&gt;

&lt;p&gt;This phenomenon essentially helps one manage the social “distance” between the two speakers, and hence serves to build trust. This trust has the potential to increase call resolution rates and improve customer satisfaction in task-oriented dialog systems.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/speaker-entrainment/user-study.jpg&quot; /&gt;
  &lt;figcaption&gt;User study for speaker entrainment.&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Much research has gone into what features are relevant for speaker entrainment, such as phonetic features [1], linguistic features such as word choice [2], structure/syntax [3], style [4] and acoustic-prosodic features (pitch, intensity, rate of articulation, NHR, jitter, shimmer) [5, 6, 7].&lt;/p&gt;

&lt;p&gt;Based on this, our research work in this project aims at building a baseline bot using low-level understanding of the above features to establish the statistical significance of speaker entrainment as well as understand its potential to improve customer experience. We also publish a &lt;a href=&quot;https://tech.skit.ai/explore/speaker-entrainment&quot;&gt;demo&lt;/a&gt; to show what this looks like in real-world conversations.&lt;/p&gt;

&lt;h1 id=&quot;methodology&quot;&gt;Methodology&lt;/h1&gt;

&lt;p&gt;There have been a few implementations of speaker entrainment modules in research, such as Lubold &lt;em&gt;et al.&lt;/em&gt; (2016) [8], which discusses a system of entrainment based only on modelling pitch and Levitan &lt;em&gt;et al.&lt;/em&gt; (2016) [9] which details a system based on \(f_0\), intensity and rate of articulation. Subsequently Hoegen &lt;em&gt;et al.&lt;/em&gt; (2019) [10] discusses a system of modelling acoustic and content (lexical) variables separately, and Entrainment2Vec (2020) [11] details a graph based model of entrainment with vector representations in multi-party dialog systems.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
  &lt;img alt=&quot;Can't See? Something went wrong!&quot; src=&quot;/assets/images/posts/speaker-entrainment/entrainer.png&quot; /&gt;
  &lt;figcaption&gt;Basic structure of a Speaker Entrainment dialog system (reproduced from [14]).&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;For our baseline model, we choose mean \(f_0\), intensity and rate of articulation (calculated by utterances/sec). We average over these over the past three utterances (two speaker utterances and one bot utterance) to calculate the value for next bot utterance. This is inspired from [10] and prevents drastic jumps in the bot’s voice profile, which might lead to unnaturalness. For the value of any feature \(F\) at turn \(i\) we have,&lt;/p&gt;

\[F_{bot, i} = \frac{F_{user, i-1} + F_{bot, i-1} + F_{user, i-2}}{3}\]

&lt;p&gt;as the value of the feature at \(i\)-th turn in the bot’s speech.&lt;/p&gt;
&lt;h1 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h1&gt;

&lt;p&gt;We record 11 scripts with varying measures in each of the three features (e.g. pitch rising, intensity low/high, rate of articulation low) and one with an angry user using the entraining bot and a control bot which does not entrain. We involve 30 participants in this experiment who are asked to rate the bots on factors such as likeability and naturalness. We then conduct a paired right taled t-test to determine the statistical significance of speaker entrainment over the three features and combinations thereof.&lt;/p&gt;

&lt;p&gt;The questions have been inspired from Shamekhi &lt;em&gt;et al.&lt;/em&gt; [12], which are posed comparatively, e.g. “Does Bot A sound more natural than Bot B?”. These are answered on a comparative scale as well (from Strongly Disagree to Strongly Agree) to encourage decisiveness in differentiation among participants. Note that if one assumes the comparative scale comes from a difference of scores that the participant evaluates internally, the paired t-test can still be conducted. This is because if we assume \(X_C\) to be the comparative score arising from the difference of \(X_A\) and \(X_B\) (score for Bot A and Bot B respectively), we have,&lt;/p&gt;

\[X_C = X_B - X_A\]

&lt;p&gt;Note that for performing a t-test we only need the difference in mean and the sum of squares of standard deviation.&lt;/p&gt;

\[t=\frac{E[X_B]-E[X_A]}{\sqrt{\frac{Var(X_B)}{n}+\frac{Var(X_B)}{n}}}\]

&lt;p&gt;This can be easily rearranged to&lt;/p&gt;

\[t=\frac{E[X_c]}{\sqrt{\frac{Var(X_c)}{n}}}\]

&lt;p&gt;With this t-value, knowing what side of the tail our data lies on, we use the right tailed cumulative distribution and calculate the p-values accordingly.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;We find that the entrained bot performs better than the non-entrained bot in most cases. Keeping our \(\alpha=0.01\), we reject the null hypothesis for entrainment in multiple feature-sets, namely: pitch, intensity, pitch and rate of articulation combined and loudness and rate of articulation combined. In most other cases, including that of the angry user, we find that the entrained bot performs better as well, however the p-values aren’t as signficant. There are cases when the non-entrained bot performs better than the the entrained bot, but they have a large overlap with the cases in which our perception module performs inaccurately, e.g. rate of articulation is higher in shorter utterances due to lesser number of silent periods.&lt;/p&gt;

&lt;h2 id=&quot;issues&quot;&gt;Issues&lt;/h2&gt;

&lt;p&gt;There are a few issues with our investigations in terms of the insights we can draw. Firstly, in cases of the entrained bot performing better, it is difficult to disentangle whether this is a result of the changed voice sounding better in absolute (e.g. perhaps the participants have a preference to higher pitched/faster speaking voices as a result of shared socio-cultural factors).&lt;/p&gt;

&lt;p&gt;Secondly, in the cases of entrained bot performing worse, it is again difficult to disentangle if this poor performance arises from entraining poorly or whether speaker entrainment over this feature leads to bad performance in general.&lt;/p&gt;

&lt;h1 id=&quot;future-work&quot;&gt;Future Work&lt;/h1&gt;

&lt;p&gt;Levitan (2020) [13] is an inspiration of future directions in speaker entrainment research. So far, there is a significant lacunae in speaker entrainment research as far as incorporation of deep learning is concerned, be it in the perception module (i.e. rich representation spaces for audio) or control module (TTS with a natural control over features and emotions).&lt;/p&gt;

&lt;p&gt;Classifying user speech should also be incorporated to understand the degree of entrainment that is necessary, rather than just entraining for every user, which can look like discriminating based on the style of conversation such as High Involvement and High Consideration (described in Hoegen &lt;em&gt;et al.&lt;/em&gt; [10]). This layer of classification serves to decide the quality/degree of entrainment, which has the potential to improve customer experience. Furthermore, this layer can help detect angry/dissatisfied users as well to plan appropriate course of action by detecting high intensity speech, hyper-articulation, etc.&lt;/p&gt;

&lt;p&gt;One can stand to improve the quality of experimentation as well. Apart from having more granular options for providing opinions (like a 7-point scale), we can have more speakers as users and more voice profiles for the bot to disentangle the experiment’s results from inherent voice qualities. Moreover, any improvement in the entrainment module will improve the quality of results as well.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Many psycho-sociologists deem speaker entrainment to be crucial to naturalness and trust building in human-human conversations. Recent advancements discussed in Levitan (2020) [13] imply that speaker entrainment is more nuanced than previously thought, but this means if implemented and modelled well, speaker entrainment has the potential of significantly changing the way voicebots interact with users.&lt;/p&gt;

&lt;h1 id=&quot;references-&quot;&gt;References :&lt;/h1&gt;
&lt;p&gt;[1] J. S. Pardo, “On phonetic convergence during conversational interaction,” J. Acoust. Soc. Am., vol. 119, no. 4, pp. 2382–2393, 2006.&lt;/p&gt;

&lt;p&gt;[2]	K. G. Niederhoffer and J. W. Pennebaker, “Linguistic style matching in social interaction,” J. Lang. Soc. Psychol., vol. 21, no. 4, pp. 337–360, 2002.&lt;/p&gt;

&lt;p&gt;[3]	D. Reitter, J. D. Moore, and F. Keller, “Priming of syntactic rules in task-oriented dialogue and spontaneous conversation,” 2010.&lt;/p&gt;

&lt;p&gt;[4]	C. Danescu-Niculescu-Mizil and L. Lee, “Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs,” ArXiv Prepr. ArXiv11063077, 2011.&lt;/p&gt;

&lt;p&gt;[5]	R. Levitan and J. Hirschberg, “Measuring Acoustic-Prosodic Entrainment with Respect to Multiple Levels and Dimensions,” 2011.&lt;/p&gt;

&lt;p&gt;[6]	R. Levitan, A. Gravano, L. Willson, Š. Beňuš, J. Hirschberg, and A. Nenkova, “Acoustic-prosodic entrainment and social behavior,” in Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human language technologies, 2012, pp. 11–19.&lt;/p&gt;

&lt;p&gt;[7]	N. Lubold and H. Pon-Barry, “Acoustic-prosodic entrainment and rapport in collaborative learning dialogues,” in Proceedings of the 2014 ACM workshop on Multimodal Learning Analytics Workshop and Grand Challenge, 2014, pp. 5–12.&lt;/p&gt;

&lt;p&gt;[8] N. Lubold, H. Pon-Barry, and E. Walker, “Naturalness and rapport in a pitch adaptive learning companion,” Dec. 2015, pp. 103–110. doi: 10.1109/ASRU.2015.7404781.&lt;/p&gt;

&lt;p&gt;[9]	R. Levitan et al., “Implementing Acoustic-Prosodic Entrainment in a Conversational Avatar,” in Proc. Interspeech 2016, 2016, pp. 1166–1170. doi: 10.21437/Interspeech.2016-985.&lt;/p&gt;

&lt;p&gt;[10]	R. Hoegen, D. Aneja, D. McDuff, and M. Czerwinski, “An End-to-End Conversational Style Matching Agent,” Proc. 19th ACM Int. Conf. Intell. Virtual Agents, pp. 111–118, Jul. 2019, doi: 10.1145/3308532.3329473.&lt;/p&gt;

&lt;p&gt;[11]	Z. Rahimi and D. Litman, “Entrainment2vec: Embedding entrainment for multi-party dialogues,” in Proceedings of the AAAI Conference on Artificial Intelligence, 2020, vol. 34, no. 05, pp. 8681–8688.&lt;/p&gt;

&lt;p&gt;[12] A. Shamekhi, M. Czerwinski, G. Mark, M. Novotny, and G. Bennett, “An Exploratory Study Toward the Preferred Conversational Style for Compatible Virtual Agents,” Oct. 2017, Accessed: May 28, 2021. &lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/exploratory-study-toward-preferred-conversational-style-compatible-virtual-agents-2/&quot;&gt;Online Available&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[13] R. Levitan, “Developing an Integrated Model of Speech Entrainment,” in Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, Yokohama, Japan, Jul. 2020, pp. 5159–5163. doi: 10.24963/ijcai.2020/727.&lt;/p&gt;

&lt;p&gt;[14] Levitan, Rivka, et al. “Implementing Acoustic-Prosodic Entrainment in a Conversational Avatar.” Interspeech. Vol. 16. 2016.&lt;/p&gt;</content><author><name>Shikhar Mohan</name></author><category term="Machine Learning" /><category term="ASR" /><summary type="html">In this post, we will discuss the phenomenon of speaker entrainment and the insights we gained when designing a voice-bot that entrains on the user’s speech. This work was done by me as a ML Research Intern at Skit, supervised by Swaraj Dalmia.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/entrain.png" /><media:content medium="image" url="/assets/images/entrain.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Speech-First Conversational AI</title><link href="/speech-first-conversational-ai/" rel="alternate" type="text/html" title="Speech-First Conversational AI" /><published>2022-02-02T00:00:00+00:00</published><updated>2022-02-02T00:00:00+00:00</updated><id>/speech-first-conversational-ai</id><content type="html" xml:base="/speech-first-conversational-ai/">&lt;p&gt;We often get asked about the differences between voice and chat bots. The most
common perception is that the voice bot problem can be reduced to chat bot after
plugging in an Automatic Speech Recognition (ASR) and Text to Speech (TTS)
system. We believe that’s an overly naive assumption about spoken conversations,
even in restricted goal-oriented dialog systems. This post is an attempt to
describe the differences involved and define what &lt;em&gt;Speech-First&lt;/em&gt; Conversational
AI means.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Speech is the most sophisticated behavior of the most complex organism in the
known universe. - &lt;a href=&quot;https://youtu.be/Zy3Ny-WjyGE?t=251&quot;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Conversational AI systems solve problems of conversations, either using text or
voice. Since &lt;em&gt;conversations&lt;/em&gt; are specific to humans, there are many
anthropomorphic expectations from these systems. These expectations, while still
strong, are less restraining in text conversations as compared to speech. Speech
is deeply ingrained in human communication and minor misses could lead to
violation of user expectations. Contrast this with text messaging which is a,
relatively new, human-constructed channel&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; where expectations are different
and more lenient.&lt;/p&gt;

&lt;p&gt;There are multiple academic sources on differences between speech and text, here
we will describing a few key differences that we have noticed while building
speech-first conversation systems in more practical settings.&lt;/p&gt;

&lt;h2 id=&quot;signal&quot;&gt;Signal&lt;/h2&gt;

&lt;p&gt;In addition to the textual content, speech signals contain information about the
user’s state, trait, and the environment. Speech isn’t merely a redundant
modality, but adds valuable extra information. Different style of uttering the
same utterance can drastically change the meaning, something that’s used a lot
in human-human conversations.&lt;/p&gt;

&lt;p&gt;Environmental factors like recording quality, background ambience, and audio
events impact signals’ reception and semantics. Even beyond the immediate
environment, a lot of socio-cultural factors are embedded in speech beyond the
level they are in text chats. Because the signals are rich, the difficulty of a
few common problems across text and speech, like low-resource languages, is
higher.&lt;/p&gt;

&lt;h2 id=&quot;noise&quot;&gt;Noise&lt;/h2&gt;

&lt;p&gt;Once you go on transcribing audios utterances using ASRs, transcription errors
will add on to your burden. While ASR systems are improving day-on-day, there
still is error potential in handling acoustically similar utterances. Overall,
an entirely new set of problems like far-field ASR, signal enhancement, etc.
exist in spoken conversations.&lt;/p&gt;

&lt;p&gt;Additionally many &lt;em&gt;noisy&lt;/em&gt; deviations from fluent speech are not mere errors but
develop their own pragmatic sense and convey strong meaning. Speech
&lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_disfluency&quot;&gt;disfluencies&lt;/a&gt; are commonly
assumed behaviors of natural conversations and lack of them could even cause
discomfort.&lt;/p&gt;

&lt;h2 id=&quot;interaction-behavior&quot;&gt;Interaction Behavior&lt;/h2&gt;

&lt;p&gt;We don’t take turns in a half-duplex manner while talking. Even then, most
dialog management systems are designed like sequential turn-taking state
machines where party A says something, then hands over control to party B, then
takes back after B is done. The way we take turns in true spoken conversations
is more &lt;em&gt;full-duplex&lt;/em&gt; and that’s where a lot of interesting conversational
phenomenon happen.&lt;/p&gt;

&lt;p&gt;While conversing, we freely barge-in, attempt corrections, and show other
backchannel behaviors. When the other party also start doing the same and
utilizing these both parties can have much more effective and grounded
conversations.&lt;/p&gt;

&lt;p&gt;Additionally, because of lack of a visual interface to keep the context, user
recall around dialog history is different and that leads to different flow
designs.&lt;/p&gt;

&lt;h2 id=&quot;personalization-and-adaptations&quot;&gt;Personalization and adaptations&lt;/h2&gt;

&lt;p&gt;With all the extra added richness in the signals, the potential of
personalization and adaptations goes up. A human talking to another human does
many micro-adaptations including the choice-of-words (common with text
conversations) and the acoustics of their voices based on the ongoing
conversation.&lt;/p&gt;

&lt;p&gt;Sometimes these adaptations get ossified and form &lt;em&gt;sub-languages&lt;/em&gt; that need
different approaches of designing conversations. In our experience, people
talking to voice bots talks in a different sub-language, a relatively
understudied phenomenon.&lt;/p&gt;

&lt;h2 id=&quot;response-generation&quot;&gt;Response Generation&lt;/h2&gt;

&lt;p&gt;Similar to the section on input &lt;em&gt;signals&lt;/em&gt;, the output &lt;em&gt;signal&lt;/em&gt; from the voice
bot is also (should also be) extremely rich. This puts a lot of stake in
response production for natural conversation. The timing and content of sounds,
along with their tones impart strong semantic and pragmatic sense to the
utterance. Clever use of these also drive the conversations in a more fruitful
direction for both parties.&lt;/p&gt;

&lt;p&gt;Possibilities concerning this area of work is &lt;em&gt;extremely&lt;/em&gt; limited in text
messaging.&lt;/p&gt;

&lt;h2 id=&quot;development&quot;&gt;Development&lt;/h2&gt;

&lt;p&gt;Finally, working with audios is more difficult than text because of additional
and storage processing capabilities needed. Here is an audio utterance for the
text “1 2 3”:&lt;/p&gt;

&lt;script&gt;
       $(document).ready(function () {
         var ws3 = WaveSurfer.create({
           container: '#waveform-3',
           backend: 'MediaElement'
         });
         ws3.load('/assets/audios/posts/speech-first-conversational-ai/counts.wav');

         ws3.on('audioprocess', function () {
           let progressText = ws3.getCurrentTime().toFixed(2) + ' / ' + ws3.getDuration().toFixed(2)
           document.getElementById('player-progress-3').innerHTML = progressText
         });

         ws3.on('ready', function () {
           let progressText = ws3.getCurrentTime().toFixed(2) + ' / ' + ws3.getDuration().toFixed(2)
           document.getElementById('player-progress-3').innerHTML = progressText
         });

         ws3.on('finish', function () {
           let button = $('#controls-3 &gt; [data-action=&quot;play-pause&quot;]')
           button.find('i:first').toggleClass('fa-play')
           button.find('i:first').toggleClass('fa-pause')
           button.toggleClass('btn-dark')
         });

         for (let button of document.getElementById('controls-3').children) {
           button.onclick = function (e) {
             let action = button.getAttribute('data-action')
             switch (action) {
               case 'play-pause':
                 ws3.playPause()
                 $(button).find('i:first').toggleClass('fa-play')
                 $(button).find('i:first').toggleClass('fa-pause')
                 $(button).toggleClass('btn-dark')
                 break
               case 'backward':
                 ws3.skipBackward()
                 break
               case 'forward':
                 ws3.skipForward()
                 break
             }
           }
         }
       });
     &lt;/script&gt;

&lt;style&gt;
  .player-controls {
    margin: 20px 0;
  }
&lt;/style&gt;

&lt;div id=&quot;waveform-3&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;player-controls&quot; id=&quot;controls-3&quot;&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;backward&quot;&gt;&lt;i class=&quot;fa fa-backward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;play-pause&quot;&gt;&lt;i class=&quot;fa fa-play&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;button class=&quot;btn btn-sml&quot; data-action=&quot;forward&quot;&gt;&lt;i class=&quot;fa fa-forward&quot;&gt;&lt;/i&gt;&lt;/button&gt;
  &lt;code class=&quot;btn btn-sml disabled&quot; id=&quot;player-progress-3&quot;&gt;&lt;/code&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;❯ file counts.wav  # 48.0 kB (48,044 bytes)
counts.wav: RIFF (little-endian) data, WAVE audio, Microsoft PCM, 16 bit, mono 8000 Hz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Compare this with just 6 bytes needed for the string itself (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;echo &quot;1 2 3&quot; | wc
--bytes&lt;/code&gt;).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;These differences lead to gaps that are difficult to bridge and that’s what
keeps us busy at Skit. If these problems interest you, you should reach out to
us on &lt;a href=&quot;mailto:join@skit.ai&quot;&gt;join@skit.ai&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Epistolary communication aside. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Abhinav Tushar</name></author><category term="Machine Learning" /><category term="speech" /><summary type="html">We often get asked about the differences between voice and chat bots. The most common perception is that the voice bot problem can be reduced to chat bot after plugging in an Automatic Speech Recognition (ASR) and Text to Speech (TTS) system. We believe that’s an overly naive assumption about spoken conversations, even in restricted goal-oriented dialog systems. This post is an attempt to describe the differences involved and define what Speech-First Conversational AI means.</summary></entry></feed>