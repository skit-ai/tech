<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title> | Skit Tech</title>

    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Skit Tech | Speech Technology from Skit</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Skit Tech" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Speech Technology from Skit" />
<meta property="og:description" content="Speech Technology from Skit" />
<meta property="og:site_name" content="Skit Tech" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Skit Tech" />
<script type="application/ld+json">
{"url":"/authors/Shashank/","headline":"Skit Tech","description":"Speech Technology from Skit","@type":"WebPage","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/images/logo.png"}},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap & Theme Stylesheets -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
      src="https://code.jquery.com/jquery-3.3.1.min.js"
      integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
      crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WRCLS27Y94"></script>
<script>
 window.dataLayer = window.dataLayer || [];
 function gtag(){dataLayer.push(arguments);}
 gtag('js', new Date());

 gtag('config', 'G-WRCLS27Y94');
</script>



</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/">
            <img src="/assets/images/logo.png" height="20" alt="logo">
        </a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
  <a class="nav-link" href="/authors-list/">Authors</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="/explore/">Explore</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="/resources/">Resources</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="/careers/">Careers</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="/about/">About</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="/feed.xml">
    <i class="fa fa-rss text-danger"></i>
  </a>
</li>

            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "/about/",
    "title": "About",
    "body": "      Skit is an AI-First SaaS business enhancing customer experience through intelligent voice conversations.        This is the homepage and blog of the technology teams powering our speech   conversational systems. We work on conversations from all angles, signals, symbols, and   systems.          Follow us:        Github           Gitlab           @SkitTech           @Skit_ai           Linkedin      "
    }, {
    "id": 2,
    "url": "/authors-list/",
    "title": "Authors",
    "body": " {% for author in site. data. authors %}                                   {{ author[1]. name }} :       View Posts      {{ author[1]. bio }}                       {% if author[1]. website %}        &nbsp;       {% endif %}       {% if author[1]. github %}        &nbsp;       {% endif %}       {% if author[1]. twitter %}        &nbsp;       {% endif %}       {% if author[1]. linkedin %}        &nbsp;       {% endif %}                      {% endfor %}"
    }, {
    "id": 3,
    "url": "/buy-me-a-coffee/",
    "title": "Buy me a coffee",
    "body": "Hi! I am Sal, web designer &amp; developer at WowThemes. net. The free items I create are my side projects and Mundana for Jekyll is one of them. You can find all the work I release for free here. You have my permission to use the free items I develop in your personal, commercial or client projects. If you’d like to reward my work, I would be honored and I could dedicate more time maintaining the free projects. Thank you so much! Buy me a coffee "
    }, {
    "id": 4,
    "url": "/careers/",
    "title": "Careers",
    "body": "person-1 need to rethink the careers page person-2 &gt; need to rethink the careers page Yea, not the first time we have said this, I have given up on it at least forthe main website or other places. tech website we can do it ourselves whateverwe find good (which means throwing away the recruiterbox thing also) person-1 doing ourselves only. This time will happen. I will do a PR. Please click here for jobopenings in our ML and Engineering teams. "
    }, {
    "id": 5,
    "url": "/categories/",
    "title": "Categories",
    "body": "          Categories          {% for category in site. categories %}     {{ category[0] }}:           {% assign pages_list = category[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 6,
    "url": "/contact/",
    "title": "Contact",
    "body": "  Please send your message to {{site. name}}. We will reply as soon as possible!   "
    }, {
    "id": 7,
    "url": "/explore/emotional-tts/",
    "title": "Emotional TTS",
    "body": "Using our rich Emotional TTS system, we deliver the right tonality and superiorcustomer experience in our dialog systems. This page showcases a sample ofemotional presets and variations from our synthesizer.  Reference Audios: These are audio files from the training data.         Neutral    Happy    Sad    Angry                           Your browser does not support the audio element.                            Your browser does not support the audio element.                               Your browser does not support the audio element.                            Your browser does not support the audio element.                               Your browser does not support the audio element.                            Your browser does not support the audio element.                               Your browser does not support the audio element.                            Your browser does not support the audio element.                    Excited    Apologetic    Fear    Surprise                           Your browser does not support the audio element.                            Your browser does not support the audio element.                               Your browser does not support the audio element.                            Your browser does not support the audio element.                               Your browser does not support the audio element.                            Your browser does not support the audio element.                               Your browser does not support the audio element.                            Your browser does not support the audio element.                    Calm                           Your browser does not support the audio element.                            Your browser does not support the audio element.                 Synthesized Audios: These are audios synthesized from Skit’s Emotional TTS system.  Audio 1 : The swan dive was far short of perfect.         Neutral    Happy    Sad    Angry                           Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                    Excited    Apologetic    Fear    Surprise                           Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                    Calm                           Your browser does not support the audio element.               Audio 2 : The beauty of the view stunned the young boy.         Neutral    Happy    Sad    Angry                           Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                    Excited    Apologetic    Fear    Surprise                           Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                    Calm                           Your browser does not support the audio element.               Audio 3 : Two blue fish swam in the tank.         Neutral    Happy    Sad    Angry                           Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                    Excited    Apologetic    Fear    Surprise                           Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                    Calm                           Your browser does not support the audio element.               Audio 4 : Her purse was full of useless trash.         Neutral    Happy    Sad    Angry                           Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                    Excited    Apologetic    Fear    Surprise                           Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                    Calm                           Your browser does not support the audio element.               Audio 5 : The colt reared and threw the tall rider.         Neutral    Happy    Sad    Angry                           Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                    Excited    Apologetic    Fear    Surprise                           Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                               Your browser does not support the audio element.                    Calm                           Your browser does not support the audio element.               "
    }, {
    "id": 8,
    "url": "/engineering/",
    "title": "Engineering",
    "body": "{% assign category = “Engineering” %}{% assign pages_list = site. categories[category] | slice: 0, 5 %}         This is the home page of the Engineering team at Skit.            Team:    Current members:       {% for member in site. data. teams. engineering %}    {% if member. active %}    {{ member. name }} ({{ member. tenure }})    {% endif %}    {% endfor %}      Past members:       {% for member in site. data. teams. engineering %}    {% unless member. active %}    {{ member. name }} ({{ member. tenure }})    {% endunless %}    {% endfor %}            Recent posts:        {% for post in pages_list %}                      {{ post. title }}            		     {{ post. date | date: '%b %d, %Y' }}	                  {% endfor %}          All posts under Engineering      "
    }, {
    "id": 9,
    "url": "/explore/",
    "title": "Explore",
    "body": "Explore Speech Technologies from Skit.  Response Production:                Emotional TTS:          Use emotion presets and variations to deliver the right tonality in your     dialog system.                                  Voice Cloning:            Voice cloning enables one to generate synthesised speech in their voice using minimal data.                                     Natural TTS:            We have state of the art performance in Naturalness for a TTS system.                Behaviors and Personalities:                Speaker Entrainment:          Speaker entrainment attunes the bot to the features in the user's speech.             "
    }, {
    "id": 10,
    "url": "/",
    "title": "Home",
    "body": "{% if page. url ==  /  %}    {% assign latest_post = site. posts[0] %}           {% if latest_post. image %}     &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); &gt;     &lt;/div&gt;    {% else %}                    {% endif %}        {{ latest_post. title }}  :      {{ latest_post. excerpt | newline_to_br | strip_newlines | replace: '', ' ' | strip_html | strip | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                      {{ latest_post. date | date: '%b %d, %Y' }}                      {%- assign second_post = site. posts[1] -%}                      {% if second_post. image %}       &lt;img class= w-100 post-image  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{site. baseurl}}/{{ second_post. image }}{% endif %}  alt= {{ second_post. title }} &gt;       {% else %}              {% endif %}                                       {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                            {{ second_post. date | date: '%b %d, %Y' }}                                {%- assign third_post = site. posts[2] -%}                              {% if third_post. image %}          &lt;img class= w-100 post-image  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;         {% else %}                   {% endif %}                                       {{ third_post. title }}        :                  In          {% for category in third_post. categories %}         {{ category }},          {% endfor %}                                  {{ third_post. date | date: '%b %d, %Y' }}                             {%- assign fourth_post = site. posts[3] -%}                      {% if fourth_post. image %}        &lt;img class= w-100 post-image  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;       {% else %}               {% endif %}                             {{ fourth_post. title }}      :              In        {% for category in fourth_post. categories %}       {{ category }},        {% endfor %}                          {{ fourth_post. date | date: '%b %d, %Y' }}                 {% for post in site. posts %}{% if post. tags contains  sticky  %}                    {{post. title}}                 {{ post. excerpt | newline_to_br | strip_newlines | replace: '', ' ' | strip_html | strip | truncate: 136 }}                 Read More            {% if post. image %}                  {% else %}                  {% endif %}      {% endif %}{% endfor %}{% endif %}       All Stories:     {% for post in paginator. posts %}      {% include main-loop-card. html %}    {% endfor %}               {% if paginator. total_pages &gt; 1 %}             {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}            {% include sidebar-featured. html %}  "
    }, {
    "id": 11,
    "url": "/ml/",
    "title": "Machine Learning",
    "body": "{% assign category = “Machine Learning” %}{% assign pages_list = site. categories[category] | slice: 0, 5 %}         This is the home page of Machine Learning team at Skit. We work on Speech   Conversational Systems. On this website, we keep notes and artifacts from   our work.        In case of queries, reach out to machine-learning@skit. ai.            Team:    Current members:       {% for member in site. data. teams. ml %}    {% if member. active %}    {{ member. name }} ({{ member. tenure }})    {% endif %}    {% endfor %}      Past members:       {% for member in site. data. teams. ml %}    {% unless member. active %}    {{ member. name }} ({{ member. tenure }})    {% endunless %}    {% endfor %}            Recent posts:         {% for post in pages_list %}                      {{ post. title }}            		     {{ post. date | date: '%b %d, %Y' }}	                  {% endfor %}          All posts under ML      "
    }, {
    "id": 12,
    "url": "/explore/natural-tts/",
    "title": "Natural TTS",
    "body": "Our TTS has the state of the art results in Naturalness. Below, we present some example audio’s generated by our TTS as contrasted against Google-TTS.  Conversational Tonality: Our TTS has a very distinct conversational tonality that differentiates it from other TTS vendors in the industry. Example Audio’s: Text 1 : The whole family gathered around the computer waiting for my sister to come home.         Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Text 2 : Hi, what can i do for you today ?        Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Text 3 : You will be surprised to know what he did yesterday !        Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Text 4 : Can you please pass me the spoon ?.         Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Filled Pauses: Filled pauses are articulations like “umm”, “uh” etc which occur very commonly in human conversations. Out TTS supports filled pauses and generates very natural sounding audios. Example Audio’s: Text 1 : Uh, i agree with you.         Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Text 2 : Um, can you just give me a second.         Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Text 3 : I can, uh, yes, i think i can finish this.         Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Filler Words: Filler words are amongst the most commonly used words in any language. Example words in english are “okay”, “alright”, “you know” etc. These don’t necessarily add any content to the utterance, but have different functional, paralinguistic importance. These often also a specific intonation that are reflective of filler words which is supported by our TTS. Example Audio’s: Text 1 : Okay. I can do that for you.         Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Text 2 : Alright. that sounds good. Please give me a second.         Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Text 3 : Okay, so, like have you heard of the new company that he started.         Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Text 4 : You know, I never thought this would happen.         Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Ellipses: Ellipses are signified by 3 dots(“…”) and are used often as markers of hesitation, thinking or breaks in conversational flows.  Example Audio’s: Text 1 : Can you just . . . hold for a minute        Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Text 2 : I am unsure what to choose . . . i think i will go for this one.         Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Text 3 : I am going to . . . play tennis today.         Skit    Google                           Your browser does not support the audio element.                               Your browser does not support the audio element.               "
    }, {
    "id": 13,
    "url": "/privacy-policy/",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 14,
    "url": "/resources/",
    "title": "Resources",
    "body": "Contains links to publications, dataset releases, reading resources, and other artifacts. For technology demos go to theExplore tab.  Publications:  Rajaa, S. , 2023. Improving End-to-End SLU performance with Prosodic Attentionand Distillation. In Interspeech 2023.  Rajaa, S. , Anandan, K. , Dalmia, S. , Gupta, T. and Chng, E. S. , 2023, June. Improving Spoken Language Identification withMap-Mix. In ICASSP2023-2023 IEEE International Conference on Acoustics, Speech and SignalProcessing (ICASSP) (pp. 1-5). IEEE.  Sahu, S. K. and Dalmia, S. , 2022. On The Diversity of ASR Hypotheses In SpokenLanguageUnderstanding. In I Can’t Believe It’s Not Better Workshop: Understanding Deep LearningThrough Empirical Falsification.  Surya Kant Sahu. 2022. TaskMix: Data Augmentation for Meta-Learning of SpokenIntent Understanding. InFindings of the Association for Computational Linguistics: AACL-IJCNLP 2022,pages 67–72, Online only. Association for Computational Linguistics.  Karthik Ganesan, Pakhi Bamdev, Jaivarsan B, Amresh Venugopal, and AbhinavTushar. 2021. N-Best ASR Transformer: Enhancing SLU Performance usingMultiple ASR Hypotheses. InProceedings of the 59th Annual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Conference on Natural LanguageProcessing (Volume 2: Short Papers), pages 93–98, Online. Association forComputational Linguistics. Datasets:  Emotional TTS datasetrelease by Swaraj Dalmia,Kaustav Tamuly, Pulkit Mishra, Shangeth Raaja, Kumarmanas Nethil &amp; AbhinavTushar Phone Number EntityDataset by KumarmanasNethil, Anirudh Thatipelli &amp; Sachin Kumar Speech to IntentDataset by KumarmanasNethil, Kriti Anandan, &amp; Unnati SenaniLibrary:       Collection of our readings and notes on ML, Speech, NLP, and Technology in   general. We organize various reading sessions and seminars internally.    This page keeps publicly accessible notes and lists from those.        Zotero public library   Seminars   Old Paper Reading Archive     Projects: Here is a selection of projects from our GitHubportfolio. Subscribe to our blog tostay updated on upcoming items.       "
    }, {
    "id": 15,
    "url": "/explore/speaker-entrainment/",
    "title": "Speaker Entrainment",
    "body": "Speaker entrainment is a phenomenon observed in human-human conversations where one interlocutor attunes their speech’s acoustic, lexical and semantic features to the other interlocutor. This project aims to create a bot which can entrain on the acoustic features of user’s speech. Incorporating such behavior into bots is known to increase trust, naturalness and likeability, which is likely to increase customer satisfaction and call resolution rate.  Baseline Module: The following audio samples are generated from the Baseline entrainment module, which entrains over pitch (fundamental frequency), intensity (loudness) and rate of articulation. Demo Audio Samples: Script-1: Entraining over pitch (fundamental frequency) in this audio sample, entrained performs better. In this script, the pitch of the user is rising and the bot attunes itself to that.         Not Entrained    Entrained                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Script-2: Entraining over rate of articulation in this audio sample, entrained performs better. An excerpt from a user-bot interaction is provided here, where in the entrained version, our bot increases its rate of articulation according to the user.         Not Entrained    Entrained                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Script-3: Entraining over intensity in this audio sample, the non-entrained performs better. In this script excerpt, the pitch rises but that is a result of the user being angry since the bot does not understand him, among other factors. The bot becoming louder in response is very detrimental to call quality, which is why the entrained bot performs worse in this case.        Not Entrained    Entrained                           Your browser does not support the audio element.                               Your browser does not support the audio element.               "
    }, {
    "id": 16,
    "url": "/tags/",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 17,
    "url": "/explore/voice-cloning/",
    "title": "Voice Cloning",
    "body": "Presenting some results of the Beta Feature of Voice Cloning that we currently support in our TTS. We trained our model on 30 mins of recordings from a new speaker. Some examples of both the target and generated audio are added below.  Example Audios: Text 1 : It is of the first importance that the letter used should be fine in form.         Target    Generated                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Text 2 : The general solidity of a page is much to be sought for.         Target    Generated                           Your browser does not support the audio element.                               Your browser does not support the audio element.               Text 3 : The two yards were adjoining, that for the common side much the largest.         Target    Generated                           Your browser does not support the audio element.                               Your browser does not support the audio element.               "
    }, {
    "id": 18,
    "url": "/authors/anirudhdagar/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 19,
    "url": "/authors/kritianandan/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 20,
    "url": "/authors/mithun/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 21,
    "url": "/authors/swarajdalmia/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 22,
    "url": "/authors/janaab11/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 23,
    "url": "/authors/shantanu28sharma/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 24,
    "url": "/authors/lepisma/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 25,
    "url": "/authors/prabhsimran/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 26,
    "url": "/authors/greed2411/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 27,
    "url": "/authors/Shangeth/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 28,
    "url": "/authors/Shashank/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 29,
    "url": "/authors/Shahid/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 30,
    "url": "/authors/deepankar/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 31,
    "url": "/authors/ojus1/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 32,
    "url": "/authors/shikharmn/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 33,
    "url": "/authors/sanchit-ahuja/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 34,
    "url": "/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 35,
    "url": "/page2/",
    "title": "Home",
    "body": "{% if page. url ==  /  %}    {% assign latest_post = site. posts[0] %}           {% if latest_post. image %}     &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); &gt;     &lt;/div&gt;    {% else %}                    {% endif %}        {{ latest_post. title }}  :      {{ latest_post. excerpt | newline_to_br | strip_newlines | replace: '', ' ' | strip_html | strip | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                      {{ latest_post. date | date: '%b %d, %Y' }}                      {%- assign second_post = site. posts[1] -%}                      {% if second_post. image %}       &lt;img class= w-100 post-image  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{site. baseurl}}/{{ second_post. image }}{% endif %}  alt= {{ second_post. title }} &gt;       {% else %}              {% endif %}                                       {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                            {{ second_post. date | date: '%b %d, %Y' }}                                {%- assign third_post = site. posts[2] -%}                              {% if third_post. image %}          &lt;img class= w-100 post-image  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;         {% else %}                   {% endif %}                                       {{ third_post. title }}        :                  In          {% for category in third_post. categories %}         {{ category }},          {% endfor %}                                  {{ third_post. date | date: '%b %d, %Y' }}                             {%- assign fourth_post = site. posts[3] -%}                      {% if fourth_post. image %}        &lt;img class= w-100 post-image  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;       {% else %}               {% endif %}                             {{ fourth_post. title }}      :              In        {% for category in fourth_post. categories %}       {{ category }},        {% endfor %}                          {{ fourth_post. date | date: '%b %d, %Y' }}                 {% for post in site. posts %}{% if post. tags contains  sticky  %}                    {{post. title}}                 {{ post. excerpt | newline_to_br | strip_newlines | replace: '', ' ' | strip_html | strip | truncate: 136 }}                 Read More            {% if post. image %}                  {% else %}                  {% endif %}      {% endif %}{% endfor %}{% endif %}       All Stories:     {% for post in paginator. posts %}      {% include main-loop-card. html %}    {% endfor %}               {% if paginator. total_pages &gt; 1 %}             {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}            {% include sidebar-featured. html %}  "
    }, {
    "id": 36,
    "url": "/page3/",
    "title": "Home",
    "body": "{% if page. url ==  /  %}    {% assign latest_post = site. posts[0] %}           {% if latest_post. image %}     &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); &gt;     &lt;/div&gt;    {% else %}                    {% endif %}        {{ latest_post. title }}  :      {{ latest_post. excerpt | newline_to_br | strip_newlines | replace: '', ' ' | strip_html | strip | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                      {{ latest_post. date | date: '%b %d, %Y' }}                      {%- assign second_post = site. posts[1] -%}                      {% if second_post. image %}       &lt;img class= w-100 post-image  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{site. baseurl}}/{{ second_post. image }}{% endif %}  alt= {{ second_post. title }} &gt;       {% else %}              {% endif %}                                       {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                            {{ second_post. date | date: '%b %d, %Y' }}                                {%- assign third_post = site. posts[2] -%}                              {% if third_post. image %}          &lt;img class= w-100 post-image  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;         {% else %}                   {% endif %}                                       {{ third_post. title }}        :                  In          {% for category in third_post. categories %}         {{ category }},          {% endfor %}                                  {{ third_post. date | date: '%b %d, %Y' }}                             {%- assign fourth_post = site. posts[3] -%}                      {% if fourth_post. image %}        &lt;img class= w-100 post-image  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;       {% else %}               {% endif %}                             {{ fourth_post. title }}      :              In        {% for category in fourth_post. categories %}       {{ category }},        {% endfor %}                          {{ fourth_post. date | date: '%b %d, %Y' }}                 {% for post in site. posts %}{% if post. tags contains  sticky  %}                    {{post. title}}                 {{ post. excerpt | newline_to_br | strip_newlines | replace: '', ' ' | strip_html | strip | truncate: 136 }}                 Read More            {% if post. image %}                  {% else %}                  {% endif %}      {% endif %}{% endfor %}{% endif %}       All Stories:     {% for post in paginator. posts %}      {% include main-loop-card. html %}    {% endfor %}               {% if paginator. total_pages &gt; 1 %}             {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}            {% include sidebar-featured. html %}  "
    }, {
    "id": 37,
    "url": "/speech-conversational-llms/",
    "title": "Speech LLMs for Conversations",
    "body": "2024/05/09 - With LLMs making conversational systems has become easier. You no longer need tofocus on the low-level details of categorizing semantics and designingresponses. Instead, you can concentrate on controlling high-level behaviors viaan LLM. This is the trend that we see most of the world moving towards asproducts are using vendor combinations of ASR, LLM, and TTS with some dialogmanagement stitched in between. While this is going to be the norm soon, we wantto keep exploring areas from where the next set of quality improvements willcome. Earlier we discussed how spokenconversations are richer than pure text and how the gap would be not bridged byLLMs purely working on transcriptions. In one of our recent experiments we builtan efficient multi-modal LLM that takes speech directly to provide betterconversational experience. For production usage, the constraint here is thatthis should happen without losing the flexibility that you get in a text-onlyLLM around writing prompts, making changes, evaluating, and debugging. Below is a conversation with our recent in-house Speech LLM based conversationalsystem. Notice that because of the extra information in speech some micropersonalizations can happen like usage of gendered pronouns1. You also getlower impact of transcription errors and in general better responses innon-speech signals. With access to both speech and text domains, the modelallows for more fluent turn-taking, though not demonstrated in the currentconversation. In addition, our approach also reduces the combined model size(&lt;2B) for taking speech to response, leading to lower compute latency ascompared to larger systems. The model above doesn’t yet control speech synthesis beyond the textual markersit can generate, but that’s something to be added soon (you might have noticederratic pitch shifts in the call above since TTS vendors don’t contextualizebased on past conversations). Stay tuned for more details on how we take thisand similar research areas forward.       Of course concerns around paralinguistic prediction accuracies areextremely important to take something like this in production.  &#8617;    "
    }, {
    "id": 38,
    "url": "/confidence-calibration/",
    "title": "Improving consumer verification using confidence calibration and thresholding",
    "body": "2024/01/09 - In the past year, our team’s current focus has shifted to building robust and scalable voice-bots for US companies. In particular, we are honing in on the use case of facilitating the collection of borrowed funds. Given the stringent compliance standards for user verification in the US, our voicebot must excel in this aspect, leaving no room for error. Our top priority is to avoid any inadvertent verification of false users, which could potentially lead to the exposure of sensitive debt-related information. On a biased dataset, curated to tackle this problem, false consumer verifications are estimated to occur in ~0. 67% of samples. Technical AnalysisWe wanted to do a deep dive of the problem from an ML standpoint. We noticed that we needed to be more confident in our predictions. But what do we mean by confidence in our predictions? It is defined as the ability of the model to provide an accurate probability of correctness for any of its predictions. For example, if our SLU model predicts that the intent is _confirm_ with a probability of 0. 3 then the prediction has a 30% of being correct provided the model has been calibrated properly.      This plot shows how under-confident we are on lower probabilities. Naturally, classes with low probabilities should be   classified as wrong class predictions but our uncalibrated model is unable to do so.  Metrics and Diagrams: We realized that model miscalibration and rejecting low confidence prediction is the major problem that we need to solve. But how do we quantify model calibration? Expected and Max Calibration Error: Expected Calibration Error (ECE) measures the disparity between a model’s confidence and its accuracy. It can be computed directly using a closed-form formula or approximated by dividing predictions into bins based on their confidence scores. Within each bin, the average confidence and accuracy differences are calculated, and then ECE is obtained by taking a weighted average of these bin-wise differences proportional to the bin sizes. Maximum Calibration Error (MCE) is similar to ECE but is equal to the maximum difference between average confidence and accuracy across bins.       [ECE = \sum_{i=1}^M \frac{   Bin_{i}   }{N}.    a_{i} - c_{i}   ]         [MCE = \max_{i \in {1,\ldots,M}}   a_{i} - c_{i}   ]   The lower the ECE and MCE values, the better calibrated the model is. Reliability Diagrams: Reliability diagrams depict accuracy on the y-axis and average confidence scores on the x-axis. A line plot is formed using the accuracy and the confidence points. A diagonal line through the origin indicates a perfectly calibrated model where confidence is equal to accuracy for each bin.     Reliability diagram of our deployed model. The dashed line represents perfect calibration. The blue bins are   the actual bins with these many predictions and their corresponding accuracy falling under one bin Adopted SolutionUpon careful analysis of our data and model, we have found evidence suggesting that calibrating our SLU (Spoken Language Understanding) models could effectively mitigate false positives and enhance the overall confidence of our model predictions. Subsequently, we delved into the existing literature to explore potential solutions to address this issue. We came across multiple solutions that tackle model miscalibration such as Ensemble based Calibration [5], mixup [3], using Bayesian Neural Networks [4] etc. We narrowed it down to one solution that was easy to integrate and did not require us to re-train our models i. e. Temperature Scaling. Temperature Scaling [1]: Temperature scaling involves tuning a softmax temperature value to minimize Negative Log-likelihood loss on a held-out validation set. This value is then used to soften the output of the softmax layer. [\text{Softmax}(y) = \frac{\exp(y_i)}{\sum_j \exp(y_j)}] [\text{Temp-Softmax}(y) = \frac{\exp(y_i/T)}{\sum_j \exp(y_j/T)}] The intuition behind temperature scaling is that the T value penalizes high probability scores thereby resulting in better confidence in these values. After tuning a temperature scaling value on our validation dataset, we noticed that our model was better calibrated(Lower ECE and MCE values as well) and was able to better classify low-confidence score predictions.     Post calibration, we notice that we are able to better classify low-confidence scores under wrong prediction bins.          Old Reliability Graph (on Validation Set)           New Reliability Graph   Thresholding: Keeping our current objective in mind where we reject low-confidence predictions, we realized that thresholding individual intents on a temperature-scaled model could work quite well for us. We wanted an increment in our precision numbers without hitting recall for our intents. This is because we want to maximize our confidence in the current prediction without inadvertently increasing our False Negatives i. e. we still want to be accurate while predicting our positive class (could be any intent here). We plotted precision-recall curves with thresholds on the x-axis and precision/recall on the y-axis. We have default recipes to generate these threshold values that ensure that we maximize precision without affecting the recall. The data scientist or an ML engineer then can have a look at these plots and accordingly decide which threshold values to go with if they feel that     A precision-recall curve for the `_confirm_` intent ResultsAfter performing the above experiments on our datasets, we noticed a bump of 10 points in macro-precision without affecting the macro-recall (\(\pm\) 1 point in recall). Our model became more robust to mis-classifications and was able to reject low-confidence predictions thereby increasing the confidence in the model to handle sensitive compliance-related turns. Our product metrics also improved by a margin - after implementing the above solution we were able to bring down false consumer verification from~0. 67% to ~0. 18% on the dataset. CaveatsTemperature Scaling does not work well when the dataset distributions differ i. e. whenever there’s data drift between the production data and the validation data, the calibration of the model is not accurate.  If there’s a data drift, the above methodology needs to be performed again to maintain data sanity. Thresholding affects recall (read reduce) when we go forward with maximizing precision gains. Citations Guo, Chuan, et al. “On calibration of modern neural networks. ” International Conference on Machine Learning. PMLR, 2017 https://towardsdatascience. com/confidence-calibration-for-deep-networks-why-and-how-e2cd4fe4a086 Zhang, Hongyi, et al. “mixup: Beyond empirical risk minimization. ” arXiv preprint arXiv:1710. 09412 (2017) Neal, Radford M.  Bayesian learning for neural networks. Vol. 118. Springer Science &amp; Business Media, 2012.  Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. “Simple and scalable predictive uncertainty estimation using deep ensembles. ” Advances in neural information processing systems 30 (2017). "
    }, {
    "id": 39,
    "url": "/speech-first-conversational-ai-revisited/",
    "title": "Speech-First Conversational AI Revisited",
    "body": "2023/05/11 - Around last year, we shared our views on how nuances of spoken conversations make voicebots different than chatbots. With the recent advancements in conversational technology, thanks to Large Language Models (LLMs), we wanted to revisit the implications on what we call Speech-First Conversational AI. This post is one of many such reexaminations. We will try quoting the older blog post wherever possible, but if you haven’t read the older post, you are encouraged to do so here before going any further. What’s Changed?: In one line, the problem of having believable and reasonable conversations is solved with the current generation of LLMs. You would still get factual problems and minor niggles, but I could ask my grandmother to sit and chat with an LLM based text bot without breaking her mental model of how human conversations could happen, at all. Internally, we use the phrase “text conversations are solved” to describe the impact of LLMs on our technology. But how does this reflect in spoken conversations? Will they also be solved? Sooner than later, sure. But there are some details to look in that go beyond raw textual models to do this well. Beyond the statement of “text conversations are solved”, there are more upgrades that make us excited about their implications for spoken conversations. The most important being the hugely improved capability to model any behavior that can be meaningfully-translated in text. For example, if you want to do speech backchannel modeling right now, you might get very far by connecting a perception system with an LLM rather than building something else altogether. This pattern is part of the promises of AGI, and knowing that we are getting there gradually is very stimulating. Spoken Dialog: Let’s revisit the points that make spoken conversations different than textual ones, as described in the post last year. In the main, all the points are still relevant, but the complexities involved in solutions are different now. As a Speech AI company, this is helping us get better answers to the question of how should we go about more natural interactions between humans and machines. 1. Signal:  Speech isn’t merely a redundant modality, but adds valuable extra information. Different styles of uttering the same utterance can drastically change the meaning, something that’s used a lot in human-human conversations. This is still relevant and important. While speech recognition systems have started to become better in transcribing content, robust consumption of non-lexical content is still a problem to solve for doing spoken conversations. One of our fresh research works (releasing soon) involves utilizing prosodic information along with lexical to increase language understanding and the gain we got is still significant. 2. Noise: Speech recognition systems have come a long way from 2022. WER performance, even in noisy audios, is extremely good and one could trust ASRs a lot more for downstream consumption than one could do last year. More non-speech markers, timing information, etc. are accessible easily and accurately which could be clubbed with LLMs directly to simplify modeling behaviors like disfluencies. 3. Interaction Behavior:  We don’t take turns in a half-duplex manner while talking. Even then, most dialog management systems are designed like sequential turn-taking state machines where party A says something, then hands over control to party B, then takes back after B is done. The way we take turns in true spoken conversations is more full-duplex and that’s where a lot of interesting conversational phenomena happen.  While conversing, we freely barge-in, attempt corrections, and show other backchannel behaviors. When the other party also starts doing the same and utilizing these both parties can have much more effective and grounded conversations. Simulacrums of full duplex systems Google Duplex already have hinted at why this is important. While the product impact of full-duplex conversations has been elusive because of technology’s brittleness, with LLMs and better speech models, the practical viability of this is pretty high now. A natural thread of work is modeling conversations speech to speech which is already happening in the research community. But even before perfecting that, we can significantly get better in spoken interactions with currently available technologies and some clever engineering. 4. Runtime Performance:  In chat conversations, model latencies and the variance over a sample don’t impact user experience a lot. Humans look at chat differently and latency, even in seconds doesn’t change the user experience as much as in voice.  This makes it important for the voice stack to run much faster to avoid any violation of implicit contracts of spoken conversations. This is something where heavy LLMs don’t do well natively. Large, high-quality models often require a GPU and optimization to meet speech latency requirements efficiently. 5. Personalization and Adaptation:  With all the extra added richness in the signals, the potential of personalization and adaptation goes up. A human talking to another human does many micro-adaptations including the choice of words (common with text conversations) and the acoustics of their voices based on the ongoing conversation.  Sometimes these adaptations get ossified and form sub-languages that need different approaches for designing conversations. In our experience, people talking to voice bots talks in a different sub-language, a relatively understudied phenomenon. As LLMs reduce the complexity and effort needed to model and design behaviors, we should get more product-level work on this in both textual and speech conversations. You might already see a bunch of AI talking heads, personas, etc. with the promise of adapting to you. Something like this was possible earlier but with much more effort than now. 6. Response Generation: With LLMs, the quality of responses content is extremely high and natural. And if they are not, you can always make them so by providing a few examples. Specifically for speech, LLMs are good substrate for modelling inputs to speech synthesis. Instead of hand-tuning SSMLs, we can now let an LLM model high-level markers to guide the right generation of spoken responses at the right time. Additionally, similar to speech recognition, speech synthesis has got huge upgrades from last year. Systems like bark provide a glimpse of the high quality of utterance along with the higher order control that could be driven by an LLM. 7. Development: This stays the same as before. Audio datasets still have more information than text and the maintenance burden is higher. Though there is a general reduction of complexity in the language understanding side because of one model handling many problems together. Thus reducing annotation, system maintenance, and other related efforts. What’s Next?: With higher order emergent behaviors coming in LLMs, there is a general lifting up of problems that we solve in ML. All this has led to an unlocking of a sort where everyone is rethinking the limits of automation. For a product like ours—goal-oriented voicebots—we expect the reduction in modeling complexity to increase the extent of automation, even for dialogs that were considered forte of human-agents. Technologically, the time is ripe to achieve great strides towards truly natural spoken conversations with machines. Something that was always undercut because of the, rightfully present, friction between user experience and technological limitations. Note that the definition of natural here still hangs on the evolving dynamics of human machine interactions, but we will see a phase transition for sure. "
    }, {
    "id": 40,
    "url": "/contextual-slu/",
    "title": "Incorporating context to improve SLU",
    "body": "2022/08/04 - Introduction: In task-oriented dialogue systems, the spoken language understanding, or SLU, refers to the task of parsing the natural language utterances intosemantic frames. The problem of contextual SLU majorly focuses on effectively incorporating dialogue information. Current SLU systems that work in tandem withASR (voice bots) only incorporate the asr transcription as an input for the SLU systems to predict intent. As such, the amount of information these transcripts have is quite less. Why context?: The bot prompts are a treasure-trove of contextual information. This information can be used to build better intent classification model. Few examples with the bot prompts and their intents are shown below:       Bot Prompt   User Response   Intent         Hi! I am Divya, your Hathway virtual assistant. Are you looking for a new Hathway Broadband connection?   yes   confirm_new_connection       Okay!. To start with, please tell me if you are next to your Hathway device?   I am   confirm_near_device       For how many people do you want to book the table for?   seven   number_guests   If we observe the first example here, the trancription just consists of ‘yes’, but if we include the bot prompt, we enrich the input to our SLU, thereby increasing the overall intent classification performance. The context of hathway broadband connection helps in enriching the context of the utterance yes. Using Bot prompts as context: We curated a private dataset of clients wherein we collect user utterances along with all the bot prompts. We then concatenate the bot prompts with the user utterances. After retraining our intent classification model on some clients,we observed a performance jump of 20-30% in the intent-F1 scores.  This probably happened because our user transcriptions are not rich with enough information. Byclosing that information deficit via the bot prompts, the overall input helped us in achieving better performance. Another probable reason for such a huge jump could be probably because the transcription generated while using voice bots are less accurate as compared to a chat bot’s transcription wherein a user types their response. As a result, the gap increment when supplied with contextual information when working with voice bots is much larger than let’s say a bot. However, this was not the case with all our datasets. We observed thatthe datasets with large number of classes didn’t perform well or at par with datasets with less number of classes. One probable reason for it could be the dataset havinga large amount of granularity with respect to intents and as such there wasn’t any significant bump in the performance. We also observed that the performance with small-talk intents such as confirm, deny etc. had a massive jump in their performance as compared to other types of intents. Some probable approaches from literature: After observing an improvement in our models by just using a single bot prompt, we decided to a delve a bit further and found out many approaches that can be utilized for our use-case. While doing literature review, we observed that encoding the contextual prompt along with the user prompt gives the best performance amongst all the methods. The current approach of concatenating bot prompts with the user prompt acts as a natural baseline for our subsequent experiments in this direction. We discuss the encoding based approaches from the literature below: Encoding dialogue History [1, 2]:  We can encode [1] the complete dialogue history as shown below. Let us assume that the dialogue isa sequence of \(D_{t} = {u_{1}, u_{2}. . u_{t}}\) bot and user utterance and at every time step \(t\) we are trying to output the classification for the user utterance \(u_{t}\), given \(D_{t}\). We then divide the model into 2 components, the context encoder that acts on \(D_{t}\) to producea vector representation of the dialogue context denoted by \(h_{t} = H(D_{t})\) and the tagger, which takesthis context encoding \(h_{t}\), and the current utterance \(u_{t}\) as input and produces the intent output. Context Encoder Architecture: The baseline context encoder is just encoding the previous bot prompt \(u_{t-1}\) into a single bidirectional RNN (BiRNN) layer with Gated Recurrent Unit (GRU). The final state of the context encoder GRU is used the dialogue context, \(h_{t} = BiGRU(u_{t-1})\). For memory networks, we encode all the dialogue context utterances, \({u_{1}, u_{2}. . u_{t}}\) into memory networks denoted by \({m_{1}, m_{2}. . m_{t}}\) using a BiGRU encoder. We add temporal context to the dialogue history utterances, for that we append special positional tokens to each utterance, \(m_{k} = BiGRU_{m}(u_{k}) \: \: 0 &lt;= k &lt;= t-1\). The current utterance is also encoded using a BiGRU and is denoted by \(c\). Let \(M\) be the matrix wherein the \(i\)th row given by \(m_{i}\). A cosine similarity is obtained between each memory vector, \(m_{i}\), and the context vector \(c\). The softmax of this similarity is used as an attention distribution over the memory \(M\), and an attention distribution over the memory \(M\), and an attention weighted sum of \(M\) is used to produce the dialogue context vector \(h_{i}\).    \(a = softmax(M_{c})\)   \(h_{t} = a^{T}M\) Tagger Architecture: A stacked BiRNN tagger is then used to model intent classification. Results: This approach was benchmarked on a multi-turn dialogue sessions and for intent classification specifically the task of reserving tables at the restaurant. The intent F1 scores with memory network as the contextual encoder is 0. 890 and just by encoding the last prompt is 0. 865.   Another approach [2] is to have a different encoding mechanism for bot and user utterances [2]. This approach uses a system act encoder to obtain a vector representation \(a^{t}\) of all system dialogue acts \(A^{t}\). An utterance encoder is then usedto generate the user utterance encoding \(u^{t}\) by processing the user utterance token embeddings \(x^{t}\). We then have a dialogue encoder that summarizes the content of the dialogue using \(a^{t}\) and \(u^{t}\), and its previoushidden state \(s^{t-1}\) to generate the dialogue context vector \(o^{t}\), and also update the hidden state. The dialogue context vector is then used for intent classification. Both the encoders use a hierarchical RNN that processes a single utterance at a time. System Act Encoder: The system act encoder encodes the set of dialogue acts \(A^{t}\) at turn \(t\) into a vector \(a^{t}\) invariant to the order in which they appear. Utterance Encoder: The utterance encoder takes in the list of user utterance tokens as input. Let \(x^{t}\) denote the utterance token embeddings, which is encoded using a bi-directional GRU. \(u^{t}, u^{t}_{o} = BRNN_{GRU}(x^{t})\)We get the embedding representation \(u^{t}\) of the user utterance and \(u^{t}_{o}\) is the concatenation of the final states and the intermediate outputs of the forward and backward RNNs respectively. Dialogue Encoder: The dialogue encoder incrementally generated the embedded representation of the dialogue context at every turn. As shown in the figure below, it takes in \(a^{t} \bigoplus u^{t}\) and its previous state \(s^{t-1}\) as inputsand outputs the updated state \(s^{t}\) and the encoded representation of the dialogue context \(o^{t}\). The above encoded feature is then flattened to the number of intent classes using a linear layer.   \(p_{i}^{t} = softmax(W_{i}. o^{t} + b_{i})\) Results: The dialogues are obtained from simulated dialogues dataset. The dataset has dialogues from restaurant and movie domains with total of 3 intents. The baseline for this approach was getting results without any context and the overall intent accuracy was 84. 76% whereas using the previous dialog encoder (\(o^{t-1}\)) and the current system encoder (\(a^{t}\)) was 99. 54%.  Probable approaches:  Another approach that has not been discussed in literature is using a time decay function to decay the effect of older bot prompts. This would help in focusing moretowards the recent prompts and reduce the effect of older prompts.  We can also experiment by fusing different modalities (text and speech) with utterance and dialogue. The emotion from the speech modality could help in infusing much better context into the input for the intent classification. Conclusion: The above approaches and experiments show that context for SLU predictions can prove to be extremely useful for improving intent F1 scores. These above approaches are also not computationally expensive and can be easily deployed at scale for various use-cases. References: [1] Ankur Bapna, Gokhan Tür, Dilek Hakkani-Tür, and Larry Heck. 2017. Sequential Dialogue Context Modeling for Spoken Language Understanding. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 103–114, Saarbrücken, Germany. Association for Computational Linguistics. [2] Gupta, R. , Rastogi, A. , &amp; Hakkani-Tür, D. Z. (2018). An Efficient Approach to Encoding Context for Spoken Language Understanding. ArXiv, abs/1807. 00267. "
    }, {
    "id": 41,
    "url": "/label-noise-intro/",
    "title": "Investigating Label Noise in intent classification datasets and fixing it",
    "body": "2022/06/19 - Introduction: Label noise has been a consistent problem even in the most widely used open source datasets. Several papers have come up various deep learning techniques to make models more robust to label noise present in their train sets. Even so, identifying label noise in your dataset and investigating it’s cause is an important process to further understand model behaviour and prevent label noise in future datasets. In this blog, we discuss why we decided to fix label noise in our datasets followed by some statistic cleaning methods we tested to narrow down regions within the dataset where label noise could be present. Why fix label noise?: Test sets should be clean to serve as a benchmark for future decisions. To measure the impact of noisy train sets, we plot a graph of model performance versus % label noise. To conduct this experiment, we retagged an old dataset in one of our clients and thoroughly reviewed it to identify and fix mislabelled examples. The total number of mislabelled examples was 13% of the whole dataset (7591 instances). We flipped the gold labels into their noisy counterparts, trained a model on the newly formed dataset and plotted the results. Impact of train set label noise on our model performance: In the above graph. we pbserve that at 0% label noise, the model performance is around 73. 8% F1 and at ~13% label noise, the model performance drops to 70. 8% F1. Different cleaning methods to fix the label noise: By measuring the reduction in cleaning effort from the baseline, we can assess the efficacy of the cleaning method. We plotted label noise recall vs % of samples retagged (or annotator effort) - Relating these metrics with previous impact graph allows us to reach interesting conclusions like - clean y% of the dataset using a method M, and you will get some x% bump in model performance. Random Sampling: Here we can sample some fixed number of instances and get them retagged. This serves as our baseline for other methods. The label noise we capture will be around 13% of each partial sample, and hence the recall will be the fraction of the partial sample (in the whole). On average, the plot will look similar to y=x, like this one for our dataset: Biased Sampling: This requires intermittent involvement from ops (tagging after every sampling iteration) In this method, we first randomly retag x % of samples. Then we identify the major tag confusions (as shown in the Dataset section) and pick the top 5 noisy tags and increase the weights associated to these tags in the sampling function. Then we sample again - pick top 5 tags - repeat.  We see an improvement over the baseline. We can capture around 60% of the total label noise by just tagging around 32% of the total dataset by this heuristic. Datamaps: This paper introduces datamaps - a tool to diagnose training sets during the training process itself. They introduce two metrics - confidence and variability to understand training dynamics. They further plot each instance on a confidence vs variability graph and create hard-to-learn, ambigous and easy regions. These regions correspond to how easy it is for the model to learn the particular instance. They also observe that the hard-to-learn regions also corresponded instances that had label noise. Confidence - This is defined as the mean model probability of the true label across epochs. Variability - This measures the spread of model probability across epochs, using the standard deviation. The intuition is that instances with consistently lower confidence scores throughout the training process are hard for a model to learn. This could be because the model is not capable of learning the target label or that the target label was incorrect. We leverage the training artefacts from the paper to define a label score for each sample - as the Euclidean distance between (0,0) and (confidence, variability). Following the hypothesis of hard-to-learn regions, we expect noisy samples to have a lower label score.    Threshold on label-score Fixing a threshold on the label score means that all samples that score below it are considered label noise, and those that score above are considered clean. Assuming we do Human Retagging of all samples predicted as label noisy, fixing a threshold essentially fixes both the % of samples retagged and (given the clean tags-) label noise recall. Varying the threshold, we get a plot for our dataset:   Looking at our metrics, we see an improvement in the partial recleaning process. Lets read the above plots. Say we fix the threshold at 0. 43 which means we would be retagging around 28% of our dataset. This corresponds to a label noise recall of 60%, giving us a resulting dataset with 5. 2% label noise from 13%. (= 0. 40*13).     n-consecutive correct instances Here, we will use the ordering of the label scores. Our added assumption here, is that the ordering within the regions are also useful. Based on this, we sort our samples by label score, and in ascending order. This means the noisy samples should be nearer to the top, and we base our heuristic on this. We start Human Retagging from the top of the sorted list of samples, and stop once we see N-consecutive clean samples. Varying N, we get a plot for our dataset:   Again, we see an improvement in the partial recleaning process. Lets read the above plots. Say we fix N at ~38, which means we would be retagging around 35% of our dataset. This corresponds to a corresponds to a label noise recall of 60%, which means we would capture and clean 76% of the label noise. Giving us a resulting dataset with 3. 1% label noise (= (1-0. 76)*0. 13).  Cleanlab: This is a label noise prediction tool. We have evaluated the accuracy of this tool instead. But we won’t be able to capture all the noisy labels via this tool. This tool takes in predicted probabilities. Since cleanlab depends on output model probabilities it can’t be used to correct train sets. Confident Learning: Confident Learning high level idea - When the predicted probability of an example is greater than a per-class-threshold, we confidently count that example as actually belonging to that threshold’s class. The thresholds for each class are the average predicted probability of examples in that class. Confident Learning estimates a joint distribution between noisy observed labels and the true latent labels. It assumes that the predicted probabilities are out-of-sample holdout probabilities (eg. K-fold cross validation). If this isn’t the case then overfitting may occur. Their algorithm also assumes that class-conditional label noise transitions are data independent. Metrics using a model trained on noisy labels. Tested on a separate test set Results are slightly better when the model is trained on clean data We expect cleanlab to perform even better once our model test accuracies improve. Cleanlab wont be very useful if the model is performing poorly even on a clean dataset. Minimizing tagging errors at source: To understand why our datasets had noisy labels, we conducted several review sessions with our annotators after they retagged datasets across multiple clients. We further classified each mislabelled example into a list of possible reasons as shown below. Here, gold tag refers to the ground truth tag. Each instance was tagged X times (X is the number of annotators) and the highest tag was chosen as the correct tag.   since our intent classifiers were not multi-label, we wanted to capture the total % of multiple intent scenarios. We observed that the label noise patterns for each of our clients were quite different, which made the problem of generalizing label noise prediction even more difficult. Conclusion: To conclude, we quantified how using datamaps helps in reducing effort taken to clean our existing train sets. We also correlated this reduced cleaning effort with the expected improvement in model performance with the help of some plots. "
    }, {
    "id": 42,
    "url": "/theory-of-mind/",
    "title": "Theory of Mind and Implications for Conversational AI",
    "body": "2022/05/19 - When a diplomat says yes, he means ‘perhaps’;When he says perhaps, he means ‘no’;When he says no, he is not a diplomat.                         —Voltaire (Quoted, in Spanish, in Escandell 1993. ) IntroductionConsider this example: You’re out in the street in a crowded area. A stranger walks upto you and asks for directions in your local language, L. You responded, you notice the facial expressions of the stranger and that they seem to be confused, and do not understand what you said. Now, you’re confused as well, and try to clarify your instructions, but the stranger later reveals that he isn’t very fluent in the language L; hence you ask for whether they understand a globally-used language E, the stranger confirms, and the conversation continues. Let’s breakdown what occurred here.  The stranger asked a question in a local language L.  You now have a belief that the stranger is speaks the local language L.  Due to your belief, you respond in the same language L, expecting the stranger to understand the information you’re trying to convey. This is a false belief, as it is later revealed.  You look for verbal/non-verbal cues from the stranger that they understood.  However, the stranger denies your expectation by showing absence of such cues and instead, show cues of confusion.  You attempt to further elaborate your instructions taking these cues into account, however the stranger still seems confused.  The conversation at this point feels “awkward” since your expectations of the conversation were being denied multiple times.  The stranger reveals that they aren’t very fluent in L.  This confirms that your belief that the stranger understood L was false. This brings a sense of comfort since now you understand why your expectations were being denied.  You now correct for your false belief and ask whether the stranger understands a globally-used language E.  The stranger confirms.  And the conversation continues. This mechanism of having expectations from the other participant is a basis for successful conversation. If we lacked such an ability, the emergence of mutually accepted meanings of words, and language itself would be impossible. This also applies to non-verbal communication, such as body language and facial expressions, and to some degree, is observed in many animal species. We now dive deeper into this aspect of communication, and formalize why both, human-human and human-machine conversations breakdown and/or lead to frustration of the participants. Theory of MindWhenever we converse, we take into account what we expect the other person to understand through our words as well as their possible responses. The ability to conceive such “theories” of other participants’ mental states is termed as the Theory of Mind (ToM). Having ToM requires the agent to acknowledge the fact that others (including the agent itself) can believe in things which are not true. These beliefs are called false beliefs. An agent possessing ToM can identify their own as well as other’s false beliefs and take actions to confirm and hence correct these false-beliefs. What makes a conversation human-like?Proposition:                A dialogue is human-like if both agents participating have some degree of Theory of Mind. Theory of Mind is not limited to the content of the speech (such as the words spoken), but also addresses the mannerism of speech (prosody), facial and other non-verbal cues etc. It is easy to see that if any one of the agents lack ToM or have a poor ability, the conversation becomes uncomfortable and frustrating. However, Theory of Mind is an acquired skill, expertise of humans on ToM matures over the lifespan [2], in-addition to depending on the amount of socialization the person part-takes in. This makes quantifying the degree of expertise over ToM difficult, hence quantifying the degree of human-likeness is also difficult, in-addition to being be subjective. Testing the Presence of Theory of Mind: Testing whether or not an agent is capable of modeling mental states of others is important for many reasons, one such application is diagonosing mental disorders. Such tests are called false-belief tasks. These tests check whether the agent can model other’s false-beliefs and/or confirm and correct its own false-beliefs. We will discuss two popular false-belief tasks: “Sally-Anne” and “Smarties” tasks . Sally-Anne Task: The participating agent is told the following scenario:  Sally and Anne are inside a room.  Sally has a basket with one marble inside it.  Anne has an empty box with her.  Sally leaves the room without her basket.  Anne takes the marble out of Sally’s basket and puts it in her own box.  Sally comes back inside the room. Now, the participant is asked, “Where will Sally look for her marble?”. If the participant replies with “the basket”, this means that the participant is able to model the mental state of a fictional character Sally, and that she doesn’t know that Anne took her marble. Children below the age of 3-4 answer with “the box”, however, older children answer with “the basket”. Some children with mental disabilities such as Down syndrome and Autism are unable to pass this test. Smarties Task: Smarties is a popular brand of candies.  The participant is presented with a box labelled “Smarties”.  The participant is asked “what is in the box?”.  The participant replies with “candies”.  The box is opened and is revealed that the box actually contains pencils.  The participant is asked “What would someone else think is inside the box?”.  The participant passes the test if they respond with “candies”. Theory of Mind is an acquired skill, and is not innate, i. e. , we aren’t born with the ability to model other’s mental states. A study [1] shows that children first pass False-belief tasks at around 3-4 years of age, around the same time as children first learn to tell lies, suggesting that learning to lie is a pre-cursor to possessing ToM. This does make sense, as lying would only help if the other participant is capable of having false-beliefs. Language and communication are also acquired skills. Theory of Mind: Relevance to Conversational AI: Having ToM allows for certain mechanisms that would not be possible otherwise. Some are listed below:  The ability of the agent to recognize its own errors in perceiving (mis-hearing), i. e. , discover its own false-beliefs and ask for clarifications. This also leads to a higher order reasoning capability of the agent.  The ability of the agent to dynamically model its counterpart throughout the conversation and adjust its own behaviour inorder to maximize the success of the dialog. Dynamic response and prosody generation, turn-taking, barge-in handling, etc. are such examples. Do Machines have a Theory of Mind?: One of the important goals of AI is to blend in the lives of Humans and solve problems with humans-in-the-loop, achieving this requires modeling humans and other machines around the agent, similar to how we humans do. Some studies [3, 4] have shown that specially designed Multi-Agent Reinforcement Learning algorithms pass the Sally-Anne False-belief task. However, False-belief tasks have not been designed for/tested against chat/voice bots. In this section, we test multiple Language Models (LM) against the Sally-Anne and Smarties tasks, and check whether they pass the tests or not. Methodology: All of the experiments were done using Huggingface’s Hub has inference interface. These experiments can be easily re-ran, however, it is not guarrenteed to get the same results since the inference is non-detereministic. The tasks are widely used and are available in Wikipedia and other scientific papers on which some/all of the LMs may have been trained on, hence these tests are not conclusive. Sally-Anne Task: Input Text: Sally and Anne are inside a room. Sally has a basket with one marble inside it. Anne has an empty box with her. Sally leaves the room without her basket. Anne takes the marble out of Sally's basket and puts it in her own box. Sally comes back inside the room. Sally will look for her marble in If the LMs continue with her basket, the basket or anything similar, the LM passes the test, else it doesn’t. Smarties Task: Input Text: Sally is presented with a box labeled  Candies . Sally is asked,  what is in the box? . Sally replies with  candies . The box is opened and is revealed that the box actually contains pencils. Sally is asked,  What would someone else think is in the box? . Sally answers If the LMs continue with candies or anything similar, the LM passes the test, else it doesn’t. Results:       Language Model   # Params   Sally-Anne   Smarties         DistilGPT2   82M   Fail   Fail       GPT-Neo-125M   125M   Fail   Fail       GPT-Neo-1. 3B   1. 3B   Fail   Fail       GPT-2   1. 5B   Pass   Pass       GPT-Neo-2. 7B   2. 7B   Pass   Pass       GPT-J-6B   6B   Pass   Pass   The largest three of the models pass both the tests. This suggests that scale might help LMs achieve some basic reasoning capabilities. This result is not surprising, since larger LMs usually do better in reasoning benchmarks. P. S. The most entertaining response award goes to DistilGPT2 for  I don't give a fig about the box  for the Sally-Anne task. This is not made up, I swear! Implications for Goal-Oriented Conversational AI: Open-domain chat has one important goal, engagement with the user. The user engages with the bot if the bot is entertaining the user. For this statement to hold true, the bot should appropriately make responses, which in-turn requires modeling the user, i. e. , having a Theory of Mind. The degree of engagement can be seen as a measure of degree of ToM of the bot. Testing ToM is straight-forward for Open-domain (chit-chat) bot. However, this is tricky for goal-oriented bots, as they are designed to handle dialog under a specific domain. False-belief task defined on one domain maybe out-of-domain for another domain. Open-domain dialog is a strict generalization of goal-oriented dialog. However, goal-oriented may have goals which are defined differently from engagement. In many call-center settings, call resolution is the most important goal. However, when voice bots are used in-place of human agents in call centers, a new and different behaviour of users arises: call drop. Users simply drop from the call if they:  get frustrated (due to mishearing, poor reasoning capabilities etc. ).  think the bot is incapable of answering their queries, even if the bot is capable. This is a false-belief of the user, and the bot is unable to correct the user’s false-belief. Call drops occur in a major chunk of the calls (40-50%). Most bots in the industry are designed in a way that assumes the user trusts the bot and has infinite patience. The bot’s behaviour is apparrently designed to optimize for resolving queries of the user, however, not to inspire trust in the user that the bot is capable to resolve queries. There are two possible ways to “solve” this problem:  Explicit: Design the product in a way that inspires trust. Come up with the best possible responses for all possible combination of dialog history and user states.  Implicit: Design the product in a top-down fashion rather than a bottom-up. Many believe that optimizing components with their local objective (Word-error-rate for ASR, F1 Scores for Intent classifiers etc. ) would lead to a higher resolution rate. In biological systems, the higher-order function (survival) dictates lower order function (communication, language). Learning to communicate better can not ensure survival on its own. However, learning to survive may lead to a better ability to communicate. In other words, optimize ML models against objective (resolution rate) in-addition to the local objective. This will force the bot to behave in a way that inspires trust from the user and effectively learn to have theory of mind of the users. The first method is the industry standard, and it doesn’t seem to be working well. The second method has the clear advantage of being data-driven and scalable. References[1] Astington, J. W. , &amp; Edward, M. J. (2010). The Development of Theory of Mind in Early Childhood. [2] Demetriou, A. , Mouyi, A. , &amp; Spanoudis, G. (2010). “The development of mental processing”, Nesselroade, J. R. (2010). “Methods in the study of life-span human development: Issues and answers. ” In W. F. Overton (Ed. ), Biology, cognition and methods across the life-span. Volume 1 of the Handbook of life-span development (pp. 36–55), Editor-in-chief: R. M. Lerner. Hoboken, New Jersey: Wiley. [3] Rabinowitz, N. C. , Perbet, F. , Song, H. F. , Zhang, C. , Eslami, S. M. , &amp; Botvinick, M. M. (2018). Machine Theory of Mind. ICML. [4] Nguyen, T. N. , &amp; González, C. (2020). Cognitive Machine Theory of Mind. CogSci. "
    }, {
    "id": 43,
    "url": "/end-of-utterance-detection/",
    "title": "End of Utterance Detection",
    "body": "2022/04/24 -  This blog post is based on the work done by Anirudh Thatipelli as an ML research fellow at Skit. ai End Of Utterance Detection - When does a speaker stop speaking?End-of-utterance detection is the problem of detecting when a user has stopped speaking in a conversation.  In the above image, there are four turns in total that are time-aligned. . The system initiates the conversation by speaking first (“How may I help you?”), then the user (“I want to go to Miami. ”), then the system again (“Miami?”) and finally the system (“Yes. ”).  The speaker who utters the first unilateral sound both initiates the conversation and gains possession of the floor. Having gained possession, a speaker maintains it until the first unilateral sounds by another speaker, at which time the latter gains possession of the floor. MotivationDespite going through many advances, the performance of spoken dialogue systems remains unsatisfactory. For example, turn-taking is a fundamental aspect of natural human conversation that helps to decide which participant has the floor in a conversation and who can speak next. Humans use many multimodal cues like prosodic features, gaze, etc to determine who has the floor in a particular conversation. The interaction is very smooth with very less gaps and overlaps between participants’ speech, making its modeling difficult. Currently, dialogue systems use a silence threshold to determine whether it should start speaking. This approach is too simplistic and can lead to many issues. The system can interrupt the user mid-utterance, known as cut-in. Or it can wait too long and leads to sluggish responses and possible misrecognition, causing an increase in latency. As speech-dialogue systems become more ubiquitous, it is essential to design dialogue systems that can predict end of utterance and predict turns. A dialogue system designer should also consider the trade-offs between cut-ins and latency. For Skit, an effective turn-taking system will improve customer service and decrease call-drop rate. Imbibing turn-taking capabilities into our product will make it more natural and improve the conversations with customers. Previous approaches to solve the problemOne of the earliest models to study conversations was designed by Harvey Sacks et al in which he divided a conversation into two units of speech: Turn-constructional units (TCU) and Transition-relevant place (TRP) respectively.  Turn-constructional units are utterances from one speaker during which other participants assume the role as listeners. And each TCU is followed by a TRP, where a turn-shift can occur by the following rules:    The current speaker may select a next speaker (other-select), using for example gaze or an address term. In the case ofdyadic conversation, this may default to the other speaker.     If the current speaker does not select a next speaker, then any participant can self-select. The first to start gains the turn.     If no other party self-selects, the current speaker may continue.  To identify these TCUs and TRPs, researchers segment the speech into Inter-Pausal Units (IPUs), which are stretches of audio from one speaker without any silence exceeding a stipulated amount(say, 200 ms). A voice activity detection(VAD) can detect these IPUs. Hence, a turn can be considered as a sequence of IPUs from a speaker, that are not interrupted by IPUs from another speaker. To identify TRPs(turn-yielding cues) and non-TRPs(turn-hlding)cues, many cues such as syntactic completion, prosody and non-verbal cues like eye-contact have been investigated. However, it is very complicated to directly detect such cues from the data. This problem is compounded by the absence of facial cues in our data. End of utterance task can be also defined as the detection of TRPs, i. e. when the user’s turn is yielded and the system can start to speak. There are a multitude of works done in this regard, that can be divided into three types:  Silence-based models. The end of the user’s utterance is detected using a VAD. A silence duration threshold is used to determine when to take the turn. As discussed above, this is too simplistic and can lead to misrecognitions.  IPU-based models. Potential turn-taking points (IPUs) are detecting using a VAD. Turn-taking cues in the user’s speech are processed to determine whether the turn is yielded or not (potentially also considering the length of the pause).  Continuous models. The user’s speech is processed continuously to find suitable places to take the turn, but also for identifying backchannel relevant places (BRP), or for making projections. We will go through each of the approaches in the following sections: Silence-based models: As mentioned above, existing architectures use a fixed silence duration detection threshold to determine if the speech has ended. VAD utilizes energy and spectral features to distinguish between noise and speech in the audio. Two types of parameters are taken into consideration while designing these kinds of models.    After the system has yielded the turn, it awaits a user response, allowing for a certain silence (a gap). If this silence exceeds theno-input-timeout threshold (such as 5 s), the system should continue speaking, for example by repeating the last question.     Once the user has started to speak, the end-silence-timeout (such as 700ms) marks the end of the turn. As the figure shows,this allows for brief pauses (shorter than the end-silence-timeout) within the user’s speech.   These simplistic models break down if the user takes too long to respond. Or when the system might interrupt the user’s speech.  Tuning the threshold for different domains is extremely difficult and user satisfaction will be affected. IPU-based models: These systems are built on an assumption that the system should not start to speak while the user is speaking. Turn-taking cues at the end of pauses are used to determine whether a turn has ended. These approaches run the gamut from hand-crafted rule-based semantic parsers to machine-learning and reinforcement learning models. Sato et al’s work inputs over 100 different kinds of features like syntactic, semantic, final word, and prosody to decision trees to model when to take a turn. Albeit simplistic, their model achieved an accuracy of 83. 9%, compared to the baseline of 76. 2%. However, this approach can misclassify the IPU as a pause and uses a fixed threshold of 750 ms for pauses. To overcome this limitation, Ferrer et al condition a decision-tree classifier on the length of the pause after IPU continuously and classify on the prosodic features and n-grams of the words. Raux and Eskenazi cluster silences based on dialogue features and set a single threshold for each cluster, minimizing the overall latency by over 50% on the Let’s Go dataset. Another shortcoming with the above approaches is that they are trained on human-computer dialogue corpus. But we want to learn a model for human-human dialogues. Transferring models from human-human to human-computer based systems is not feasible. So, some authors like (Raux, Eskenazi &amp; Meena et al. use bootstrapping. First, a more simplistic model of turn-taking is implemented in a system and interactions are recorded. Then, the data is then manually annotated with suitable TRPs, and trained using a machine learning model like LSTM. Another approach is a Wizard-of-Oz setup, where a hidden operator controls the system and makes the turn-taking decisions as used in Maier et al. Some previous approaches utilize reinforcement learning as well. For example, Jonsdottir et al train two agents to talk to each other, picking up prosodic cues and develop turn-taking skills. Khouzaimi et al. train a dialogue management model intending to minimize the dialogue duration and maximize the completion task ratio. But these approaches are trained in simulated environments and it is unclear if they transfer to real users. Continuous models: Continuous models process the utterances in an incremental manner. These modules process the input frame-by-frame and pass their results to subsequent modules. It enables the system to make continuous TRP predictions, project turn completions and backchannels. Unlike previous approaches, the processing starts before the input is complete. The processing time is improved, and the output becomes more natural. There is no need to train the model for end-of-turn detection. It enables a deeper understanding of utterances and project backchannels and even interrupt the user.  One of the first works in incremental processing was Skantze and Schlangen on the task of number dictation. A benefit of incremental models is revision, as shown by Skantze and Hjalmarsson. For example, the word “four” might be amended with more speech, resulting in a revision to the word “forty”. Another work by Skantze doesn’t train the model for end-of-turn detection. The audio from the speakers is processed frame-by-frame (20 frames per second) and fed to an LSTM. The LSTM predicts the speech activity for the two speakers for each frame in a future 3s window. The model outperforms human judges in this task. In an extension to this work, Roddy et al. propose a new LSTM architcture where the acoustic and linguistic features get processed in separate LSTM systems with different timescales. Datasets: Most of the aforementioned works evaluate their performance on dialogue based datasets like:  HCRC MapTask Corpus Mahnob Corpusthat have a limited purpose and may not generalize well to our problem. Conclusion: While significant work has been done in end-of-utterance detection, most of these models have shortcomings. Firstly, most are trained on dialogue-based datasets only without accounting for speech-level features. Secondly, these datasets are well-curated with less noise in the background which is not the case for our datasets. To account for noise and model audio and text jointly, we will need to retrain our models with new baselines. References:  Flexible Turn-Taking for Spoken Dialog Systems Turn-taking in Conversational Systems and Human-Robot Interaction: A Review Learning decision trees to determine turn-taking by spoken dialogue systems Rhythms of Dialogue.  simplest systematics for the organization of turn-taking for conversation.  Towards a general, continuous model of turn-taking in spoken dialogue using LSTM recurrent neural networks IS THE SPEAKER DONE YET? FASTER AND MORE ACCURATE END-OF-UTTERANCE DETECTION USING PROSODY Optimizing Endpointing Thresholds using Dialogue 2Features in a Spoken Dialogue System Towards Deep End-of-Turn Prediction for Situated Spoken Dialogue Systems Learning smooth, human-like turntaking in realtime dialogue Optimising Turn-Taking Strategies With Reinforcement Learning Incremental Dialogue Processing in a Micro-Domain  Towards incremental speech generation in conversational systems Towards a General, Continuous Model of Turn-taking in Spoken Dialogue using LSTM Recurrent Neural Networks Multimodal Continuous Turn-Taking Prediction Using Multiscale RNNs"
    }, {
    "id": 44,
    "url": "/woc/",
    "title": "TTS Enhancement",
    "body": "2022/03/09 - Problem StatementText-To-Speech (TTS) systems of Skit, as well as TTS systems in general, have a tendency to mix some ambient noise along with the speech it outputs. This aim of this research project was to remove that noise and quantify how well the noise has been removed using standard metrics. Listen to the clean speech sample here for reference-     and the distorted sample-     IntroductionSpeech enhancement can be done using the traditional signal processing techniques or using deep learning techniques. In this project, we mainly focused on the signal processing aspects of noise reduction. Signal processing techniques can be further divided into 3 more categories- Spectral Subtractive algorithms: The main principle is as follows- assuming additive noise, one can obtain an estimate of the clean signal spectrum by subtracting an estimate of the noise spectrum from the noisy speech spectrum. The noise spectrum can be estimated and updated, during periods when the signal is absent. The assumption made is that noise is stationary or a slowly varying process and that the noise spectrum does not change significantly between the updating periods. The enhanced signal is obtained by computing the IDFT of the estimated signal spectrum using the phase of the noisy signal. Statistical Model based algorithms: Given a set of measurements that depend on an unknown parameter, we wish to find a nonlinear estimator of the parameter of interest. These measurements correspond to the set of DFT coefficients of the noisy signal and the parameters of interest are the set of DFT coefficients of the clean signal. Various techniques from estimation theory which include maximum-likelihood (ML) estimators and the Bayesian estimators like MMSE and MAP estimators are used for this purpose. Subspace algorithms: These algorithms are based on the principle that the clean signal might be confined to a subspace of the noisy Euclidean space. Given a method for decomposing the vector space of the noisy signal into a direct sum of the subspace that is occupied by the clean signal and a subspace occupied by the noise signal, for example SVD, we could estimate the clean signal simply by nulling the component of the noisy vector residing in the noisy subspace. ContributionsFilters: Speech enhancement can be done using the traditional signal processing techniques or using deep learning techniques. We hypothesised that signal processing techniques would be suitable for task and tested them out. We implemented some of the popular speech enhancement methods which were suitably modified to tackle the problem at hand. Wiener Filter   Block Diagram of Wiener FilterThe input signal w[n] goes through a linear and time-invariant system to produce an output signal x[n]. We are to design the system in such a way that the output signal, x[n], is as close to the desired signal, s[n], as possible. This can be done by computing the estimation error, e[n], and making it as small as possible. The optimal filter that minimizes the estimation error is called the Wiener filter.     MMSE and MMSE Log Filter These fall under the umbrella of Bayesian estimation techniques. We saw above that the Wiener estimator can be derived by minimizing the error between a linear model of the clean spectrum and the true spectrum. The Wiener estimator is considered to be the optimal (in the mean-square-error sense) complex spectral estimator, but is not the optimal spectral magnitude estimator. Acknowledging the importance of the short-time spectral amplitude (STSA) on speech intelligibility and quality, several authors have proposed optimal methods for obtaining the spectral amplitudes from noisy observations. In particular, we are looking for sought that minimized the mean-square error between the estimated and true magnitudes: [e = E{ (\hat{X_k} - X_k)^2 }] where \(\hat{X_k}\) is the estimate spectral magnitude at frequency \(\omega_k\) and \(X_k\) is the true magnitude of the clean signal. The MMSE Log is an improvement upon the MMSE estimator. Although a metricbased on the squared error of the magnitude spectra is mathematically tractable, it may not be subjectively meaningful. It has been suggested that a metric based on the squared error of the log-magnitude spectra may be more suitable for speech processing. So we minimize : [e = E { (log \hat X_k - log X_k)^2 }] and we notice a significant improvement in the results compared to the original MMSE estimator.         Berouti’s Oversubstraction This method consists of subtracting an overestimate of the noise power spectrum, while preventing the resultant spectral components from going below a preset minimum value (spectral floor).       [\hat X(\omega)=\begin{cases}   Y(\omega)   ^2 - \alpha   \hat D(\omega)   ^2&amp; \text{if }   Y(\omega)   ^2 \geq (\alpha + \beta)   D(\omega)   ^2 \ \beta   \hat D(\omega)   ^2 &amp; \text{else} \end{cases}]   where \(\alpha (\geq 1)\) is the oversubtraction factor and \(0 \leq \beta \leq 1\) is the spectral floor parameter. When we subtract the estimate of the noise spectrum from the noisy speechspectrum, there remain peaks in the spectrum. Some of those peaks are broadband (encompassing a wide range of frequencies) whereas others are narrow band, appearing as spikes in the spectrum. By oversubtracting the noise spectrum, that is, by using \(\alpha\), we can reduce the amplitude of the broadband peaks and, in some cases, eliminate them altogether. This by itself, however, is not sufficient because the deep valleys surrounding the peaks still remain in the spectrum. For that reason, spectral flooring is used to “fill in” the spectral valleys and possibly mask the remaining peaks by the neighbouring spectral components of comparable value. The valleys between peaks are no longer deep when \(\beta &gt; 0\) compared to when \(\beta = 0\). The parameter \(\beta\) controls the amount of remaining residual noiseand the amount of perceived musical noise. If the spectral floor parameter \(\beta\) is toolarge, then the residual noise will be audible but the musical noise will not be perceptible. Conversely, if \(\beta\) is too small, the musical noise will become annoying but the residual noise will be markedly reduced. The parameter \(\alpha\) affects the amount of speech spectral distortion caused bythe subtraction. If \(\alpha\) is too large, then the resulting signal will be severely distorted to the point that intelligibility may suffer. [\alpha = \alpha_0 - \frac{3}{20} \textit{SNR} : \text{ for} -5 \leq \textit{SNR} \leq -20] where \(\alpha_0\) is the desired value of \(\alpha\) at 0 dB SNR and the \(\textit{SNR}\) is the short term SNR estimated at each frame.     The Kalman filter is a general recursive state estimation technique which is modified to work on the speech denoising problem.     Intelligibility Metrics: Along with techniques for speech enhancement, it is important to quantify the degree of enhancement which our methods provide. For this, we tested several metrics as discussed below- Perceptual Evaluation of Speech Quality (PESQ) is a full-reference algorithm and analyzes the speech signal sample-by-sample after a temporal alignment of corresponding excerpts of reference and test signal. PESQ results essentially model mean opinion score (MOS) that cover a scale from 1 (bad) to 5 (excellent). Short-Time Objective Intelligibility (STOI) is an objective metric showing high correlation (\(\rho=0. 95\)) with the intelligibility of both noisy, and TF-weighted noisy speech. Gross Pitch Error (GPE) is the proportion of frames, considered voiced by both pitch tracker and ground truth, for which the relative pitch error is higher than a certain threshold, which is usually set to 20%. Voicing Error Decision (VED) is the proportion of frames for which an incorrect voiced/unvoiced decision is made. F0 Frame Error (FFE) is the proportion of frames for which an error (either according to the GPE or the VDE criterion) is made. FFE can be seen as a single measure for assessing the overall performance of a pitch tracker. Mel Cepstral Distortion (MCD) is a measure of how different two sequences of mel cepstra are. It is used in assessing the quality of parametric speech synthesis systems, including statistical parametric speech synthesis systems, the idea being that the smaller the MCD between synthesized and natural mel cepstral sequences, the closer the synthetic speech is to reproducing natural speech. ResultsWe apply our methods on 2 different datasets: First on the public NOIZEUS dataset and next on a dataset created by the in-house TTS systems of Skit. The results are quite satisfactory when we apply our methods on the NOIZEUS dataset and we found that the Wiener Filter and the Kalman Filters perform the best outperforming one another for different signal-to-noise ratios (SNR).   Effect of filters wrt MCD metric  Effect of filters wrt PESQ metricConclusionHowever they do not perform as well as we want on the TTS dataset. In fact, we observe that our models adversely affecting the input speech. There can be various reasons attributed to this, the primary one being that speech denoising of real life data and TTS Systems are quite different, since both have different noise types. Real life noise is either additive and can be subtracted by noise estimation or can be decomposed as a direct sum of a clean subspace and a pure noise subspace. But the noise in TTS systems are much more subtle and the noise cannot be modelled to be simply additive. Here the noise is generated along with the speech. Hence most of the traditional filters which although work well for real life noise separation, do not work well for this use case. This is where we planned to resort to deep learning models like the Facebook denoiser and SeGAN. Code: You can find more information on the Github Repository. References Berouti’s Spectral Subtraction Kalman Filter for Speech Enhancement Speech Enhancement by Philipos C. Loizou A comparative study of pitch extraction algorithms on a large variety of singing sounds, PESQAuthor: Ananyapam De, a final year student at IISER Kolkata, majoring in Statistics, while minoring in the Computational Sciences. "
    }, {
    "id": 45,
    "url": "/Turn_Taking_Dynamics_in_Voice_Bots/",
    "title": "Turn Taking Dynamics in Voice Bots",
    "body": "2022/03/07 - One of the challenges in building an interactive voice bots is accounting for turn taking behaviour. Turn-taking is a difficult problem to get right, even for humans. In all our circles, we’d know of at least one person who likes to interrupt a lot and doesn’t have good turn taking etiquette.  Having a conversation with such a person can be quite irritating as one feels one is not getting heard or even getting a chance to finish one’s sentence. Turn-taking is even more difficult in a multi-party setting. You might remember the last group call you had and just when you were about to take the turn, someone else jumped right in (because you waited for a tad bit too long) and you never got to speak. Turn-taking behaviour also differs culturally. In some cultures, interruptions and barge-ins are a lot more natural. There is also a difference in the inter-turn pause duration. These factors often lead to an unnatural conversation flow when speaking to a person from a different culture. Note : Bots with explicit turn-taking signalling like wake-words are out of scope for this blog. Natural Turn Taking Dynamics: Irrespective of nuances, there are aspects of turn taking behaviour which are globally present in natural human-human conversation and one’s that we would want to imbibe in a human-bot interaction as well.  Barge-ins: These are situations when one agent interrupts the other. They occur very commonly. Examples of situations are : when one feels the other person is making a mistake or when ones feels the need to add some essential information, one naturally barges in.  Full Duplex Conversations : A half duplex conversation is one where turns are alternatively taken, like playing a tennis match, however in natural conversations, there are often instances when both people are saying something at the same time.      backchannels : words and fillers like “okay”, “alright” or “hmm” provide a lot of context about the state of the other person(for example attentiveness), especially when one is talking over the phone and visual cues are absent.    corrections : at times, when a person is saying something, one might want to make a small correction. For example, if there is an announcement being made “for the next meeting, you are supposed to finish submissions by 12th December, at so and so time…. ”. When the person is saying 12th December someone might correct by saying 13th December. This information is assimilated by the person and they often correct themselves. So, humans have the ability to hear and understand even while speaking and are active listeners.         Fig 1: Full duplex vs half duplex conversations.   Minimal inter-turn pauses : if you’ve ever spoken with a voice assistant, one of the first observations is that it takes too long to start speaking after you are done and the other way around. Human conversations have a much lower turn taking latency. If this latency is near optimal, it also lends to a feeling that the other person is understanding you and the left over impression is that of a conversation gone well. Human’s have an average pause duration of 200ms as shown below, while bots have a much higher latency.      Fig 2: Turn Taking Pause duration as measured from the Switchboard corpus. Image is taken from [1].   Turn taking cues : often in natural conversations, people produce small vocal cues like filler words “umm” or “uhhh” to convey that they want to say something and take the turn.  Turn yielding cues : there are markers is conversations when one knows that the person is done speaking. This is how we are able to separate pauses, which happen when a person is thinking in between his utterance vs one when he is done speaking. Turn-Taking Dynamics in Voice Bots: Below, we discuss different versions of turn-taking dynamics implemented in voice-bots each with more features and increasing levels of difficulty. Version - 1. 0: These are some characteristics of a bare bone turn-taking behaviour that one would need in a voice bot deployment.  Initial patience : the time that the bot waits for the person to starts speaking Silence detection : if the bot detects silence for a certain duration after the person has started speaking, it assumes the person’s turn is over.  Max turn duration : it doesn’t make sense to just be listening (because of error compounding, loss of context, maybe one is hearing just noise), so usually voice bots have a maximum duration to which they listen to the user. Version - 2. 0: This version add robustness for real life situations, make the bot more human-like and tries to reduce the latency between turns.  VAD instead of silence detection : Often existence of background noise, speech and other signals causes the bot to keep listening. Instead one could train a Voice-Activity Detection system rather than use silence detection, to have robustness to background events and to listen to the user only when they are speaking.  Variable thresholds for silence detection and max duration : In some states for example, when the bot is expecting a yes/no answer, it makes sense to use smaller thresholds. In general dynamic thresholding should be used.  For turn-switching, instead of a simple VAD, use an IPU based model discussed here. This uses a smaller VAD threshold + cues to predict the turn is over. One could start with some verbal cues for example phrase completion.  Adding backchannels as bot responses : So far we’ve only discussed aspects of perception, but backchannels are a very useful response feature. It makes the user feel that the bot is more attentive and is actively listening.      One could also add filler words in the main channel, when the bot is taking too long to produce a response in cases of high latency. This would prevent the user from asking a question to verify if the bot is there or not. Without this, the user’s speech would lead to further increase in latency as it would be perceived as a case when the user wants to take the turn and say something useful.    Version - 3. 0: There are no good baselines for these and working on improvements would constitute state of the art performance.  Multi-party situations : These are a lot more complex and require modelling multiple parties. An application could be when the bot is overseeing a human-machine interaction say, between a call centre agent and a human. Another common use is when during a typical 2 party interaction, someone interrupts the user. This requires the bot being aware that the user is speaking to someone else and then waiting.  Full - Duplex Conversations : Unlike human-human conversations a bots can attentively listen at the same time, while saying something. This offers a possibility of redesigning interactions which can leverage this feature.  Personalisation of Turn taking behaviour : This involves changing the parameters based on user characteristics. One could entrain one’s system to be more in line with the user’s behaviour. At times when the user is angry it might involve changing the durations to feel that they are being heard. References ::  Turn-taking in Conversational Systems and Human-Robot Interaction: A Review"
    }, {
    "id": 46,
    "url": "/feature-disentanglement1/",
    "title": "Feature Disentanglement - I",
    "body": "2022/02/22 - The main advantage of deep learning is the ability to learn from the data in an end-to-end manner. The core of deep learning is representation, the deep learning models transform the representation of the data at each layer into a condensed representation with reduced dimension. Deep Learning models are often also termed as black-box models as these representations are difficult to interpret, understanding these representations can give us an insight about which feature of the data is more important and will allow us to control the learning process. Recently there has been a lot of interest in representation learning and controlling the learned representations which give an edge over multiple tasks like controlled synthesis, better representations for specific downstream tasks. Data Representation and Latent CodeAn image \((x)\) from the MNIST dataset has 28x28 = 784 dimensions which is a sparse representation of the image that can be visualized. But all these dimensions are not required to represent the image. The content of the images can be represented in a condensed form using lesser dimensions called latent code. Although the actual image has 784 dimensions \(x \in R^{784}\), one way of representing MNIST image can be with just an integer ie: \(z \in \{0, 1, 2, …, 9\}\). This representation \(z\) reduces the dimension of representing the image \(x\) to 1 which captures the content of which number is present in the image and the variability in the dataset. This is one example of discrete latent code for the MNIST dataset, a continuous latent code will contain more information about the image such as the style of the image, position of the number, size of the number in the image, etc.   Fig 2: Sample Images of MNIST from [1]AutoEncoder: Autoencoder[2] models are popularly used to learn such latent code in an unsupervised manner by compressing the image to a fixed dimension code \(z\) and generating the image back using this latent code with an encoder-decoder model.   Fig 2: Autoencoder architecture from Autoencoders[2]The encoder \(q_{\phi}(z \mid x)\) of the autoencoder compresses the image to a fixed dimension\((d)\) latent code\((z)\), and the decoder \(p_{\theta}(x \mid z)\) is a conditional image generator. The dimension of z has to be such that, the image can be completely reconstructed by the decoder with the latent code. Choosing the dimension of the latent code is a problem on its own[3]. The autoencoder models trained will successfully encode the images into a latent code \(z\), but there is no guarantee that the latent code can be easily inferred, ie: we do not know where in the d-dimensional space the model encoded the image into, and thus difficult to choose a latent code to generate image during inference. So the conclusion is we have no idea how and where the encoder encodes the images, so we do not have control over synthesis during inference. The following figure shows the latent code learned by the AutoEncoder model with different training, as we can observe the latent space keep changing the range and quadrant and thus difficult to infer.   Fig 3: Latent code of MNIST images learned by an Auto Encoder [4]Variational AutoEncoder(VAE): Variational autoencoders(VAE) [6] solve this problem by forcing the latent code (z) to be close to a known prior distribution(Gaussian), this gives us control over the latent space. During inference, the latent space can be sampled from this known distribution for image generation. The following figure shows the latent code learned by VAE with different training, and the latent space across training is centered to the mean 0 across dimensions.   Fig 4: Latent code of MNIST images learned by a VAE [4]VAE allows us to have control over the latent space and sample from the known prior distribution. But this again does not give us control over the generation of the image. Say if you want to generate an image of the number ‘3’ or ‘7’, you cannot do that(at least not directly). This is where the term “disentanglement” comes into play. DisentanglementFeature disentanglement is isolating the source of variation in observation data. There is a lot more factors/feature of an MNIST image other than the number itself, such as the location of the number in the image, size of the image, angle of the number, etc. These factors are independent of each other. Feature disentanglement involves separating underlying concepts of “Big one in the left”: ie: size(big), number(one), location(left). Our interest here is to see if we can isolate these factors in the latent code so that we can have control over the generation of the images. So we want the encoder to disentangle the representation into different factors and then we generate the image with desired factors say “small seven in the top rotated 30 degrees”.   Fig 5: Generated MNIST images by InfoGAN [5] varied digit, thickness and roatation. Beta-VAE: Beta-VAE is a variant of VAE which allows disentanglement of the learned latent code. Beta-VAE adds hyperparameter to the loss function which modulates the learning constraint of VAE.   Fig 6: Loss function of beta-VAE [7]. The first part of the loss function takes care of the reconstruction of the image, it is the second term that learns the latent code of VAE. Different dimensions that span across Gaussians are independent, so by making the prior distribution gaussian, we force the dimensions of the latent code to be independent of each other. So increasing the weight of the second part of the loss, makes the latent code to be disentangled and independent. But this also brings a tradeoff between disentanglement and the reconstruction capability of the VAE. Although Beta-VAE models are good in disentangling the features, the reconstruction ability of this model is not the best.   Fig 7: Samples generated by beta-VAE [7]. Beta-TCVAE: beta-TCVAE decomposes the KL divergence[10] term of the loss function of VAE into reconstruction loss, Index-code mutual information[8] between data and latent variable, Total Correlation[9] of z, and Dimension wise KL divergence[10] of \(z\)(respectively in the following formula). This helps to break the overall KL Divergence of \(z\) into dimension-wise quantities, which will focus on each dimension of the latent code \(z\). In this formulation, the beta hyperparameter is only on the Total Correlation term which is more important for disentanglement without affecting the reconstruction. So, Beta-TCVAE has better reconstruction ability than Beta-VAE with similar disentanglement property. [\mathcal{L}{\beta-\mathrm{TC}}:=\mathbb{E}{q(z \mid n) p(n)}[\log p(n \mid z)]-\alpha I_{q}(z ; n)-\beta \operatorname{KL}\left(q(z) | \prod_{j} q\left(z_{j}\right)\right)-\gamma \sum_{j} \operatorname{KL}\left(q\left(z_{j}\right) | p\left(z_{j}\right)\right)] where \(\alpha = \gamma = 1\) and only \(\beta\) is varies as the hyperparameter.   Fig 8: Samples generated by beta-TCVAE [8]. In future posts, we will examine many new methods for feature disentanglement and how these methods can be applied to speech signals. References :: [1] : A Survey of Handwritten Character Recognition with MNIST and EMNIST (2019) [2] : Autoencoders (2021) [3] : Squeezing bottlenecks: Exploring the limits of autoencoder semantic representation capabilities (2016) [4] : Disentangled Representations - How to do Interpretable Compression with Neural Models (2020) [5] : InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets (2016) [6] : Auto-Encoding Variational Bayes (2013) [7] : beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework (2017) [8] : Isolating Sources of Disentanglement in Variational Autoencoders [9] : Wikipedia [10] : Wikipedia "
    }, {
    "id": 47,
    "url": "/gsoc-2022/",
    "title": "Google Summer of Code, 2022",
    "body": "2022/02/18 - Google Summer of Code - 2022This page contains ideas which we’d like to get help from GSoC Contributors. But before all that, if you haven’t heard about Skit. What is Skit?: We are a series B funded, AI-first SaaS voice automation company specializing in delivering multilingual voice bots for contact center automation. We have our Speech Bots deployed in major banks and large enterprises in several verticals. We have a foothold in India and are expanding in the US and South-East Asia. We have been listed in Forbes 30 Under 30 Asia 2021 and have been named by Gartner as a Cool vendor in Conversational and NLT Widen Use Cases. Our goal is to build the most natural and robust multi lingual voice bot with state of the art human-machine interaction capabilities. We build voicebots, so that agents don’t have to sit and answer user queries 24/7 for 365 days. You can get to know more about us over here Our tech blog is present here Communication: You can reach out to us at our skit-gsoc community discord, link to join here : https://discord. gg/Y9sJwz5Sw8 GSoC 2022 Ideas: Idea 1: Enhancements to dialogy via core code or plugins. Project Description: Dialogy is a framework to build machine-learning solutions for speech applications speech dialogue systems. The main principles which form the backbone of dialogy are:  Plugin-based: Makes it easy to import/export components to projects.  Stack-agnostic: No assumptions made on ML stack; your choice of machine learning library will not be affected by using Dialogy.  Progressive: Minimal boilerplate writing to let you focus on your machine learning problems. At current shape, dialogy allows one to train models, test performance on metrics and deploy applications. We want to add more features which are desirable in an complete SLU framework. These features could include  Hyperparameter Tuning- There are multiple hyperparameters involved in using a transformers. A hyperparameter tuning integration would allow developer with a seamless way to experiment with hyperparams and train optimal models.  Model interpretability via Captum, LIT integration- Interpretability is necessary from both business and development perspectives. Such tools would allow developers to explain why a certain prediction was made, as well as discover biases and faults in the model.  Integration with experiment tracking platforms- It gets difficult to keep track of models being trained by developers- some are meant for production releases, some are meant for experimentation. An experiment tracking integration would enable developers to manage their models and results easily. GitHub Link(s)::  dialogy dialogy-template-simple-transformersExpected Outcomes: Dialogy as a platform would have following features after the project  Integrated with experiment tracking for better project management Integrated with model explainability and interpretability frameworks allowing users to use these tools Integrated with an hyperparameter tuning service/librarySkills Required: Understanding of Machine Learning frameworks like PyTorch, Huggingface, etc. And of course Python. Possible Mentors:  Himansu - linkedin, github, email - himansu@skit. ai Jaivarsan - linkedin, github, email - jaivarsan@skit. aiExpected Size: 175 hours Difficulty: Medium Idea 2: Speaker AnonymizationProject Description: The goal of this project is to explore and implement methods to anonymize speech to remove the speaker information from the signal by distorting the speaker prosodic features or with techniques like voice conversion. The speaker information in the signal can be used to attack the speaker verification systems which leads to privacy and security concerns. The idea is to explore simple signal/speech processing techniques which are faster in both implementation and deployment, as well as neural methods which use deep learning models and architectures to resynthesis the speech signal with the original speaker’s information removed. Removing the speaker’s information can help us to use speech datasets without worrying about privacy attacks on the speakers in the dataset. This project involves the implementation of various research papers in the field and improving upon them and creating a python package for speaker anonymization. GitHub:  aunomExpected Outcomes:  Literature Review of research papers in the field Implementation of various research papers which are interesting to the project in python.  Github repository for experiments and final python package for speaker anonymization with proper documentation.  Demonstration of the methods implemented and final package.  Talk/presentation on the work done during the GSoC period. Skills Required:  Experience in Signal/Speech processing (Preferred) Experience in Deep learning for speech tasks like Automatic Speech Recognition, Speech synthesis, speaker-related tasks etc.  Python and deep learning frameworks like PyTorch/TensorFlow. Possible Mentors:  Shangeth Rajaa - linkedin, github, email - shangeth. rajaa@skit. ai Swaraj - linkedin, github, email - swaraj@skit. aiExpected Size: 175 Hours Difficulty: Medium Idea 3: Improving kaldi-serve performanceProject Description: ​​Kaldi Serve is a plug-and-play abstraction over the Kaldi ASR toolkit, designed for ease of deployment and optimal runtime performance. It currently has the following key features:  Real-time streaming (uni &amp; bi-directional) audio recognition.  Thread-safe concurrent Decoder queue for server environments.  RNNLM lattice rescoring.  N-best alternatives with AM/LM costs, word-level timings, and confidence scores.  Easy extensibility for custom applications. We are mainly looking at improving the runtime performance of the ASR pipeline by offloading Decoder computation to the GPU to be performed in mini-batches, and implementing a request queueing mechanism within the gRPC server to be able to utilize the parallel computing capability in order to boost latencies under high concurrent loads. GitHub:  kaldi-serveExpected Outcomes: Integration with Kaldi Batched Threaded NNet3 CUDA pipeline for enabling batched computation in kaldi-serve gRPC application. Skills Required: Having some combination of these:Languages: C++, CUDA, PythonFrameworks: Kaldi ASR, Pytorch, gRPC, Pybind11Basics: Speech Recognition, Language Modeling, Deep Learning Possible Mentors:  Prabhsimran - linkedin, github, email - prabhsimran@skit. aiExpected Size: 175 hours Difficulty: Medium "
    }, {
    "id": 48,
    "url": "/speaker-entrainment/",
    "title": "Speaker Entrainment",
    "body": "2022/02/04 - In this post, we will discuss the phenomenon of speaker entrainment and the insights we gained when designing a voice-bot that entrains on the user’s speech. This work was done by me as a ML Research Intern at Skit, supervised by Swaraj Dalmia. IntroductionSpeaker Entrainment (also known as accomodation or alignment) is a psycho-social phenomenon that has been observed in human-human conversations in which interlocutors tend to match each other’s speech features. Believed to be crucial to the success and naturalness of human-human conversations, this can look like matching style related aspects such as pitch, rate of articulation or intensity, or content related factors, such as lexical patterns. This phenomenon essentially helps one manage the social “distance” between the two speakers, and hence serves to build trust. This trust has the potential to increase call resolution rates and improve customer satisfaction in task-oriented dialog systems.   User study for speaker entrainment. Much research has gone into what features are relevant for speaker entrainment, such as phonetic features [1], linguistic features such as word choice [2], structure/syntax [3], style [4] and acoustic-prosodic features (pitch, intensity, rate of articulation, NHR, jitter, shimmer) [5, 6, 7]. Based on this, our research work in this project aims at building a baseline bot using low-level understanding of the above features to establish the statistical significance of speaker entrainment as well as understand its potential to improve customer experience. We also publish a demo to show what this looks like in real-world conversations. MethodologyThere have been a few implementations of speaker entrainment modules in research, such as Lubold et al. (2016) [8], which discusses a system of entrainment based only on modelling pitch and Levitan et al. (2016) [9] which details a system based on \(f_0\), intensity and rate of articulation. Subsequently Hoegen et al. (2019) [10] discusses a system of modelling acoustic and content (lexical) variables separately, and Entrainment2Vec (2020) [11] details a graph based model of entrainment with vector representations in multi-party dialog systems.   Basic structure of a Speaker Entrainment dialog system (reproduced from [14]). For our baseline model, we choose mean \(f_0\), intensity and rate of articulation (calculated by utterances/sec). We average over these over the past three utterances (two speaker utterances and one bot utterance) to calculate the value for next bot utterance. This is inspired from [10] and prevents drastic jumps in the bot’s voice profile, which might lead to unnaturalness. For the value of any feature \(F\) at turn \(i\) we have, [F_{bot, i} = \frac{F_{user, i-1} + F_{bot, i-1} + F_{user, i-2}}{3}] as the value of the feature at \(i\)-th turn in the bot’s speech. Experimental SetupWe record 11 scripts with varying measures in each of the three features (e. g. pitch rising, intensity low/high, rate of articulation low) and one with an angry user using the entraining bot and a control bot which does not entrain. We involve 30 participants in this experiment who are asked to rate the bots on factors such as likeability and naturalness. We then conduct a paired right taled t-test to determine the statistical significance of speaker entrainment over the three features and combinations thereof. The questions have been inspired from Shamekhi et al. [12], which are posed comparatively, e. g. “Does Bot A sound more natural than Bot B?”. These are answered on a comparative scale as well (from Strongly Disagree to Strongly Agree) to encourage decisiveness in differentiation among participants. Note that if one assumes the comparative scale comes from a difference of scores that the participant evaluates internally, the paired t-test can still be conducted. This is because if we assume \(X_C\) to be the comparative score arising from the difference of \(X_A\) and \(X_B\) (score for Bot A and Bot B respectively), we have, [X_C = X_B - X_A] Note that for performing a t-test we only need the difference in mean and the sum of squares of standard deviation. [t=\frac{E[X_B]-E[X_A]}{\sqrt{\frac{Var(X_B)}{n}+\frac{Var(X_B)}{n}}}] This can be easily rearranged to [t=\frac{E[X_c]}{\sqrt{\frac{Var(X_c)}{n}}}] With this t-value, knowing what side of the tail our data lies on, we use the right tailed cumulative distribution and calculate the p-values accordingly. ResultsWe find that the entrained bot performs better than the non-entrained bot in most cases. Keeping our \(\alpha=0. 01\), we reject the null hypothesis for entrainment in multiple feature-sets, namely: pitch, intensity, pitch and rate of articulation combined and loudness and rate of articulation combined. In most other cases, including that of the angry user, we find that the entrained bot performs better as well, however the p-values aren’t as signficant. There are cases when the non-entrained bot performs better than the the entrained bot, but they have a large overlap with the cases in which our perception module performs inaccurately, e. g. rate of articulation is higher in shorter utterances due to lesser number of silent periods. Issues: There are a few issues with our investigations in terms of the insights we can draw. Firstly, in cases of the entrained bot performing better, it is difficult to disentangle whether this is a result of the changed voice sounding better in absolute (e. g. perhaps the participants have a preference to higher pitched/faster speaking voices as a result of shared socio-cultural factors). Secondly, in the cases of entrained bot performing worse, it is again difficult to disentangle if this poor performance arises from entraining poorly or whether speaker entrainment over this feature leads to bad performance in general. Future WorkLevitan (2020) [13] is an inspiration of future directions in speaker entrainment research. So far, there is a significant lacunae in speaker entrainment research as far as incorporation of deep learning is concerned, be it in the perception module (i. e. rich representation spaces for audio) or control module (TTS with a natural control over features and emotions). Classifying user speech should also be incorporated to understand the degree of entrainment that is necessary, rather than just entraining for every user, which can look like discriminating based on the style of conversation such as High Involvement and High Consideration (described in Hoegen et al. [10]). This layer of classification serves to decide the quality/degree of entrainment, which has the potential to improve customer experience. Furthermore, this layer can help detect angry/dissatisfied users as well to plan appropriate course of action by detecting high intensity speech, hyper-articulation, etc. One can stand to improve the quality of experimentation as well. Apart from having more granular options for providing opinions (like a 7-point scale), we can have more speakers as users and more voice profiles for the bot to disentangle the experiment’s results from inherent voice qualities. Moreover, any improvement in the entrainment module will improve the quality of results as well. ConclusionMany psycho-sociologists deem speaker entrainment to be crucial to naturalness and trust building in human-human conversations. Recent advancements discussed in Levitan (2020) [13] imply that speaker entrainment is more nuanced than previously thought, but this means if implemented and modelled well, speaker entrainment has the potential of significantly changing the way voicebots interact with users. References :[1] J. S. Pardo, “On phonetic convergence during conversational interaction,” J. Acoust. Soc. Am. , vol. 119, no. 4, pp. 2382–2393, 2006. [2]	K. G. Niederhoffer and J. W. Pennebaker, “Linguistic style matching in social interaction,” J. Lang. Soc. Psychol. , vol. 21, no. 4, pp. 337–360, 2002. [3]	D. Reitter, J. D. Moore, and F. Keller, “Priming of syntactic rules in task-oriented dialogue and spontaneous conversation,” 2010. [4]	C. Danescu-Niculescu-Mizil and L. Lee, “Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs,” ArXiv Prepr. ArXiv11063077, 2011. [5]	R. Levitan and J. Hirschberg, “Measuring Acoustic-Prosodic Entrainment with Respect to Multiple Levels and Dimensions,” 2011. [6]	R. Levitan, A. Gravano, L. Willson, Š. Beňuš, J. Hirschberg, and A. Nenkova, “Acoustic-prosodic entrainment and social behavior,” in Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human language technologies, 2012, pp. 11–19. [7]	N. Lubold and H. Pon-Barry, “Acoustic-prosodic entrainment and rapport in collaborative learning dialogues,” in Proceedings of the 2014 ACM workshop on Multimodal Learning Analytics Workshop and Grand Challenge, 2014, pp. 5–12. [8] N. Lubold, H. Pon-Barry, and E. Walker, “Naturalness and rapport in a pitch adaptive learning companion,” Dec. 2015, pp. 103–110. doi: 10. 1109/ASRU. 2015. 7404781. [9]	R. Levitan et al. , “Implementing Acoustic-Prosodic Entrainment in a Conversational Avatar,” in Proc. Interspeech 2016, 2016, pp. 1166–1170. doi: 10. 21437/Interspeech. 2016-985. [10]	R. Hoegen, D. Aneja, D. McDuff, and M. Czerwinski, “An End-to-End Conversational Style Matching Agent,” Proc. 19th ACM Int. Conf. Intell. Virtual Agents, pp. 111–118, Jul. 2019, doi: 10. 1145/3308532. 3329473. [11]	Z. Rahimi and D. Litman, “Entrainment2vec: Embedding entrainment for multi-party dialogues,” in Proceedings of the AAAI Conference on Artificial Intelligence, 2020, vol. 34, no. 05, pp. 8681–8688. [12] A. Shamekhi, M. Czerwinski, G. Mark, M. Novotny, and G. Bennett, “An Exploratory Study Toward the Preferred Conversational Style for Compatible Virtual Agents,” Oct. 2017, Accessed: May 28, 2021. Online Available. [13] R. Levitan, “Developing an Integrated Model of Speech Entrainment,” in Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, Yokohama, Japan, Jul. 2020, pp. 5159–5163. doi: 10. 24963/ijcai. 2020/727. [14] Levitan, Rivka, et al. “Implementing Acoustic-Prosodic Entrainment in a Conversational Avatar. ” Interspeech. Vol. 16. 2016. "
    }, {
    "id": 49,
    "url": "/speech-first-conversational-ai/",
    "title": "Speech-First Conversational AI",
    "body": "2022/02/02 - We often get asked about the differences between voice and chat bots. The mostcommon perception is that the voice bot problem can be reduced to chat bot afterplugging in an Automatic Speech Recognition (ASR) and Text to Speech (TTS)system. We believe that’s an overly naive assumption about spoken conversations,even in restricted goal-oriented dialog systems. This post is an attempt todescribe the differences involved and define what Speech-First ConversationalAI means.  Speech is the most sophisticated behavior of the most complex organism in theknown universe. - source Conversational AI systems solve problems of conversations, either using text orvoice. Since conversations are specific to humans, there are manyanthropomorphic expectations from these systems. These expectations, while stillstrong, are less restraining in text conversations as compared to speech. Speechis deeply ingrained in human communication and minor misses could lead toviolation of user expectations. Contrast this with text messaging which is a,relatively new, human-constructed channel1 where expectations are differentand more lenient. There are multiple academic sources on differences between speech and text, herewe will describing a few key differences that we have noticed while buildingspeech-first conversation systems in more practical settings. Signal: In addition to the textual content, speech signals contain information about theuser’s state, trait, and the environment. Speech isn’t merely a redundantmodality, but adds valuable extra information. Different style of uttering thesame utterance can drastically change the meaning, something that’s used a lotin human-human conversations. Environmental factors like recording quality, background ambience, and audioevents impact signals’ reception and semantics. Even beyond the immediateenvironment, a lot of socio-cultural factors are embedded in speech beyond thelevel they are in text chats. Because the signals are rich, the difficulty of afew common problems across text and speech, like low-resource languages, ishigher. Noise: Once you go on transcribing audios utterances using ASRs, transcription errorswill add on to your burden. While ASR systems are improving day-on-day, therestill is error potential in handling acoustically similar utterances. Overall,an entirely new set of problems like far-field ASR, signal enhancement, etc. exist in spoken conversations. Additionally many noisy deviations from fluent speech are not mere errors butdevelop their own pragmatic sense and convey strong meaning. Speechdisfluencies are commonlyassumed behaviors of natural conversations and lack of them could even causediscomfort. Interaction Behavior: We don’t take turns in a half-duplex manner while talking. Even then, mostdialog management systems are designed like sequential turn-taking statemachines where party A says something, then hands over control to party B, thentakes back after B is done. The way we take turns in true spoken conversationsis more full-duplex and that’s where a lot of interesting conversationalphenomenon happen. While conversing, we freely barge-in, attempt corrections, and show otherbackchannel behaviors. When the other party also start doing the same andutilizing these both parties can have much more effective and groundedconversations. Additionally, because of lack of a visual interface to keep the context, userrecall around dialog history is different and that leads to different flowdesigns. Personalization and adaptations: With all the extra added richness in the signals, the potential ofpersonalization and adaptations goes up. A human talking to another human doesmany micro-adaptations including the choice-of-words (common with textconversations) and the acoustics of their voices based on the ongoingconversation. Sometimes these adaptations get ossified and form sub-languages that needdifferent approaches of designing conversations. In our experience, peopletalking to voice bots talks in a different sub-language, a relativelyunderstudied phenomenon. Response Generation: Similar to the section on input signals, the output signal from the voicebot is also (should also be) extremely rich. This puts a lot of stake inresponse production for natural conversation. The timing and content of sounds,along with their tones impart strong semantic and pragmatic sense to theutterance. Clever use of these also drive the conversations in a more fruitfuldirection for both parties. Possibilities concerning this area of work is extremely limited in textmessaging. Development: Finally, working with audios is more difficult than text because of additionaland storage processing capabilities needed. Here is an audio utterance for thetext “1 2 3”:     ❯ file counts. wav # 48. 0 kB (48,044 bytes)counts. wav: RIFF (little-endian) data, WAVE audio, Microsoft PCM, 16 bit, mono 8000 HzCompare this with just 6 bytes needed for the string itself (echo  1 2 3  | wc--bytes). These differences lead to gaps that are difficult to bridge and that’s whatkeeps us busy at Skit. If these problems interest you, you should reach out tous on join@skit. ai.       Epistolary communication aside.  &#8617;    "
    }, {
    "id": 50,
    "url": "/evaluating-an-asr-in-a-spoken-dialogue-system/",
    "title": "Evaluating an ASR in a Spoken Dialogue System",
    "body": "2022/01/21 - An ASR (automatic speech recognition) is an integral component of any voice bot. The most popular metric that is used to evaluate the accuracy of an ASR model is WER or the word error rate. In this blog, we discuss metrics that can be used to evaluate an ASR, their flaws and suggestions for improvement in the context of a conversational agent. The ASR module takes as input a spoken utterance and outputs the most likely transcription. Most ASR’s output multiple alternatives with certain confidence scores. Some ASR’s including Kaldi’s implementations output N-Best alternatives in an order not necessarily reflective of the confidence. Some ASR systems output likelihood information of word sequences in the form of word-lattices or confusion networks [1] with probability information. What is WER ?: Word Error Rate measures the transcription errors, treating words as the smallest unit. It takes 2 inputs, the actual transcript and a hypothesis transcript. There are two types of WER:  Isolated word recognition (IWR-WER) Connected Speech Recognition (CSR-WER)Isolated Word Recognition WER This considers the words in isolation and is not based on any alignment. It simply measures the number of non-hits. [IWR-WER = 1-\frac{H}{N}] where,H = number of hitsN = total matched I/O words Connected Speech Recognition WER This is calculated on the basis of alignment and uses the Levenshtein distance for words which measures the minimum edit distance. This is efficiently calculated using dynamic programming. It calculates the best alignment which minimises the number of substitutions, insertions and deletions necessary to map the actual transcript to the hypothesis transcript, giving equal weight to all the operations. WER is not input/output symmetric, as N is the total number of words in the actual transcripts. [CSR-WER = (I+S+D)/(N)] where,I = insertionsS = substitutionsD = deletionsN = total number of words in the actual transcript An example calculation of the best alignment to calculate the CSR WER is shown below.   Fig 1: Taken from Speech and Language Processing by Jurafsky and Martin [2]. The CSR WER for the above hypothesis is = (6+3+1)/13 = 0. 77 The upper bound of CSR WER is not 1, but \(\frac{max(N1, N2)}{N1}\) where N1 is length of true transcripts and N2 is length of hypothesis transcript. Statistical Significance of WER ?: Improvements in WER’s over a test set is one of the standard ways of evaluating upgrades in an ASR. Often, what is missed out is in this evaluation is whether the gain is statistically significant or not. One of the standard statistical tests is the Matched-Pair Sentence Segment Word Error (MAPSSWE) test, introduced in Gillick and Cox (1989) [2]. For an example on how to calculate the statistic, consult the book by Jurafsky. Issues with WER:  It gives the same importance to words like “a”, “the” compared to verbs and nouns that carry more semantic value.  WER is a purely 1 : 1 transcript based metric, and doesn’t take into account the rich output of ASR systems like alternatives or word lattices.  The concept of edit-distance that WER is based on is appropriate for a dictation machine where the additional cost is that of correcting the transcripts rather than applications where communicating the meaning is of primary importance It doesn’t take into account the performance of an ASR in the context of the dialogue pipeline. For example if a higher WER makes no difference in the information retrieval for the downstream SLU then is the improvement worth it ?     An example of a context dependent metric is discussed in [4].     It is not a true % based metric, because it has no upper bound therefore it doesn’t tell you how good a system is, but only that one is better than another. Even for the later, it provides only a heuristic for ranking of performance.      Consider two ASR systems, ASR-1 that replaces 2 wrong words for every word it listens too, and another ASR system that replaces 1 wrong word for every word it listens to. Both communicate zero information, but the WER for ASR-1 is 2 and the WER for ASR-1 is 1. This 50% difference in WER is not reflective of performance, since both the systems communicate no correct information whatsoever.    Debunking Conventional Wisdom: One might assume that better ASR’s improve the performance of all downstream SLU systems. A paper in 2003 [7] found this was not always the case.  Their model had a 17% better slot accuracy despite a 46% worse WER performance. They have this gain by using a SLU model as the language model for speech recognition. Therefore it is not necessary that an oracular ASR will solve downstream inaccuracies. It is important to be aware that there might be non-linear correlations in metrics for the ASR vs the downstream task. WER Variants:: A few WER variants are discussed below. Metrics that look at a different granularity than that of words :**  Sentence Error Rate (SER) : The percentage of sentences with at least one word error.  Character Error Rate (CER) : Similar as WER with the smallest units as characters and not wordsThe next two error rates are variations of WER, are are bounded between [0,1].  Match Error Rate (MER) [3] : Measures the probability of a given match being incorrect. [MER = \frac{I+S+D}{I+S+D+H} = 1 - \frac{H}{I+S+D+H}] where,H = number of hitsN = total number of words in the actual transcript MER is always &lt;= WER.  Word Information Lost (WIL) [3] : Information theoretic measure based on entropy. For more details look at the paper. Example values, comparing WER, MER and WIL are shown below:   Fig 2: Taken from [3]. Note : For implementations of WER, WIL, and MER, have a look at Jiwer. Additional Error Analysis: Apart from looking at just single metrics for evaluating an ASR there are few additional metrics that one should use to evaluate a goal oriented ASR deployed for a given application:  Which speaker demographic is most often misrecognised ? What (context-dependent) phones are least well recognised ? Which words are most confused ? Generate a confusion matrix of confused words. These often help prioritise improvements in terms of what might benefit the goal the most. Semantics based MetricsWER doesn’t look at the meaning of what has been transcribed despite the fact that is the semantics that are most relevant in an ASR that is a part of a spoken dialogue system. Concept accuracy: It is a simple metric that looks at the accuracy of the concepts that are of relevance in the transcripts. Example:Reference - I want to go from Boston to Baltimore on September 29Hypothesis - Go from Boston to Baltimore on December 29 The WER is \(45\%\). However if one looks at concept accuracy, it is 2/3. Out of the 3 concepts : “Boston”, “Baltimore” and “September 29” it gets 2 of them right. WER with embeddings: In this paper [9] they look at augmenting the WER metric for an ASR that is used for a Spoken Language Translation (SLT) task. They argue that some morphological operations, like adding a plural doesn’t impact the translation task and that such substitution errors should be penalised differently. To decide which ones to penalise they use word embeddings. They call their new metric, WER-E i. e. WER with embeddings. The only change in this metric is that the substitution cost in WER is replaced by the cosine distance between the two words, so near identical words get assigned a very low cost. Other Metrics: There are lots of different papers that augment the WER metric or introduce a new metric specific to a downstream task. Mentioning a few of them below :  In [10] a new measure called Automatic Transcription Evaluation for Named Entity (ATENE) is introduced for the NER downstream task 3 new evaluation metrics are introduced in [12], where the downstream application is Information Retrieval SemDist metric is introduced in [13] for downstream SLUDownstream applications aside, it is quite pertinent to evaluate and benchmarks ASR across different social and demographic groups to evaluate bias and fairness. Systems don’t exist in isolation from the society in which they are deployed in and it is important that ML Engineers pay head to such metrics while deploying ASRs. References :: [1] : N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses (2021) [2] : Speech and Language Processing (3rd ed. draft) by Dan Jurafsky and James H. Martin [3] : From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition (2004) [4] : Automatic Human Utility Evaluation of ASR Systems: Does WER Really Predict Performance? (2013) [5] : http://www. cs. columbia. edu/~julia/courses/CS4706/asreval. pdf [6] : Meaning Error Rate: ASR domain-specific metric framework (2021) [7] : Is word error rate a good indicator for spoken language understanding accuracy (2003) [8] : Automatic speech recognition errors detection and correction: A review (2015) [9] : Better Evaluation of ASR in Speech Translation Context Using Word Embeddings (2016) [10] : How to Evaluate ASR Output for Named Entity Recognition ? (2015) [11] : Why word error rate is not a good metric for speech recognizer training for the speech translation task? (2011) [12] : Evaluating ASR Output for Information Retrieval (2007) [13] : Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding (2021) [14] : Which ASR should I choose for my dialogue system? (2013) [15] : Rethinking evaluation in ASR : Are out models robust enough ? (2021) "
    }, {
    "id": 51,
    "url": "/complexity-of-conversations/",
    "title": "Complexity of Conversations - I",
    "body": "2022/01/18 - Consider a restaurant booking voice bot built using a frames and slots approach. While this can easily solve the problem of booking with high automationaccuracy, such slot-filling framework can’t carry on a meaningful conversationin a debate unless you over-engineer the frames and slots to monstrouscomplexity. Booking a restaurant is a form of conversation that’s innatelysimpler than arguing with someone in a debate competition. We can roughly saythat these two conversations lie in different complexity classes. In this firstpost of a series, we will lay down a few factors that will help us define a mapof conversations arranged according to their complexities. At Skit we build many kinds of task-oriented dialog systems for call centerautomation. A very crude categorization of such systems, for us, is based on theinteraction with a sibling call handling system1 and the direction ofintention, user or agent initiation. While we have used multiple approaches to measure difficulty of conversationsfor our product delivery purposes, it’s interesting to see if a purer frameworkcould be built around this. Similar to computational complexity, this can tellus which problems are tractable under an algorithm. It can also help inidentifying the path towards the next generation of human machine conversationalsystems. We will cover a few thoughts around a few core constructs of the framework next. First is the definition of success in a conversation, second around thedifficulty of doing so, and third about the algorithms and their complexities. Success: The definition of success of a conversation depends on alignment between goalsof the involved parties. A regular goal oriented conversation with user initiation has a simple successdefinition. For example, a call with user asking for temperature of a place canbe called successful if the temperature is provided. The metric here could besomething like the following: [\text{Resolution%} = \frac{\text{Calls where user goals were met}}{\text{Total calls}}] This simple formulation becomes tricky as the alignment between user and botgoals becomes inexact. For example when the bot is calling the user forpayment reminders, it might not just want to remind and collect next remindertime, but also want to persuade the users to pay as early as possible. In suchcases, you might want to use another rate for favorable outcomes: [\text{Favorable%} = \frac{\text{Calls with favorable outcomes}}{\text{Resolved calls}}] Another example where this works is in argumentative conversations whereholding a reasonable conversation and reaching conclusion is important(resolution), but winning the argument (the favorable outcome) is what definessuccess. Difficulty: We can look at difficulty of conversations from multiple levels. For thesmallest unit of dialog, a turn2, parsing and generating every utterance in aconversation can be rated for difficulty. Here are a few factors that drivedifficulty for a turn:  Knowledge needed for understanding an entity. This could be general orspecific to a situation, involving connection with a dynamic or staticknowledge source.  Speech Acts. Simpler acts like greeting are easier to handle, whilesomething like pleading is hard.  Expression complexity, intentional or unintentional. For speech systems, thisis even more varied because of the richness of acoustic signals that adds tothe underlying text. For example sarcasm could be expressed by changing thetone of speech and not just via textual constructs. But these are not sufficient since higher order behaviors across multiple turnsalso make conversations difficult. As an example, consider negotiation for theprice in a market. In this situation, you need to use the conversational contextacross turns to decide your next steps in a way that’s harder than situationswhere context dependency is lesser. Algorithms: The frameworks of developing, and running, voice bots are the last pieces thatwill help us to map out the tractability of problems. A common method in theindustry is the frame-filling model that roughly needs learning intents andentities for each utterance. These frameworks, or algorithms, can be measured on their resource consumption. We can start with sample complexity of conversations as the resource and createstatements like the following:  Under framework \(f\), you need an order of \(N\) data points to supervise avoice bot of class \(k\) to achieve a success rate of \(R(N)\)3. This can be mapped to the statistical learning problem and abstractions can betranslated from there. Having set the groundwork here, we will tackle the more interesting problem ofcomplexity class definitions in a later post.       The sibling system could be non-existent, or backend human agents who cantake over the more co plex conversations, or complex parts of runningconversations.  &#8617;        We can cover backchannel events also in a kind of background turn.  &#8617;        Including the other factors around PAC learning.  &#8617;    "
    }, {
    "id": 52,
    "url": "/on-using-asr-alternatives-for-a-better-slu/",
    "title": "On using ASR Alternatives for a Better SLU",
    "body": "2021/11/29 - This blog discusses some concepts from the recently published paper by members of the ML team at Skit (formerly Vernacular. ai). The paper is titled “N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses” and was published in ACL-IJCNLP’21. IntroductionVoice bots in the industry heavily rely on the use of Automatic Speech Recognition (ASR) transcripts to understand the user and capture intents &amp; entities which are then used to resolve the customer’s problem. ASR’s however are far from perfect, especially on noisy real world data and on instances with acoustic confusion. The downstream Spoken Language Understanding (SLU) components would benefit greatly if they take the ASR’s confusion into account. Example use-case with confounding ASR transcripts (over voice): Bot: When would you like to make the reservation ?User (actually said): right nowASR alternatives for given user’s speech: - like, now- right now- write noOften the downstream SLU services which act on ASR transcripts use only the most probable alternative (also called 1-best alternative), thereby leaving out a lot of other information that exists in the form of alternative probabilities. This paper presents a simple way of using the information that exists in the alternatives to get SOTA performance on a standard benchmark for a SLU system. Types of ASR OutputsBefore we get into how to best use ASR’s confusion to increase the performance of the SLU, we discuss the different ways an ASR outputs the probable word sequence probabilities.  N-best alternatives : this is a list of the top N alternative sentences for the given spoken utterance. These are usually ranked based on probability of occurrence.  Word lattices : As shown in the below figure, they have no a-priori structure per se. Every path from the start node to the end node represents a possible hypothesis transcript. Every transition adds a word to the hypothesis.  Word confusion networks : They are a more compact and normalised topology for word lattices. They enforce certain constaints such that competing words should be in the same group and there by also ensure aligment of words that occur at the same time interval. Though these graphs capture lesser number of possibilities, this topology can be used to get as high recognition accuracy as using word lattices. The transitions probabilities of both, word lattices and word confusion networks are weighted by the acoustic and language model probabilities.   Fig 1: The structure of a word lattice as contrasted with a word confusion network. DatasetThe task that is used to compare the modelling approaches is the DSTC - 2 challenge where one is required to predict intent act-slot-value triplets. Pairs of sentences i. e. a sentence whose intent is required to be predicted along with its context is given. For each sentence the top 10 best ASR alternatives are also provided. Modelling ApproachThe central idea of the paper is to leverage pre-trained transformer models BERT and XLMRoBERTa and fine-tune them with a simple input representation shown below. The input consists of the ASR alternatives seperated by a seperator token concatenated with the context. A segment id (not shown below) is also used, which is used to contrast the context (in green) with the alternatives (in purple).   Fig 2: Input representation used to fine-tune transformer models. On top of the transformer model a semantic tuple classifier (STC) is applied to predict the act-slot-value triplets. Using this approach, we achieve a performance equivalent to the prior state-of-the-art model on DSTC-2 dataset. We get comparable F1 and SOTA accuracy. The previous SOTA model, WCN-BERT uses word confusion networks.   Fig 3: Model Architecture, The input representation is encoded by a transformer model which forms aninput for a Semantic Tuple Classifier (STC). STC uses binary classifiers to predict the presence of act-slot pairs,followed by a multi-class classifier that predicts the value for each act-slot pair. Here, using a simple ASR output such an N-best alternatives we get a comparable performance to the SOTA model that uses a much more informative probability graph such as a word confusion network. Ablation ExperimentsTwo ablation experiments are performed. One on low data regimes and another to check the impact of the context on performance. Low Data Regime: Here, the baseline models are compared with our approach using 5%, 10%, 20% and 50% of the training data respectively. In all these situations our approach beats SOTA by a considerable margin, proving that our training approach effectively transfer learns. We hypothesise, that this is due to the structural similarity between the input representations of the initial training of these open sourced models and the fine-turning that is done on DSTC-2 dataset. It also demonstrates that n-best alternatives are a more natural representation to fine-tune transformer models compared to word lattices or word confusion networks. Context Dependency: In this experiment we wanted to test the impact of adding context (last turn) to performance and to check if it is relevant at all. An improvement of around ~1. 5% F1 score is obtained using context. An example situation where context is relevance is shown below. Dialog context can help in resolving ambiguities in parses and reducing the impact of ASR noise.   Fig 4: Example to demonstrate context dependence. Lastly, this methodology can be used by users of third-party ASR APIs which do not provide word-lattice information and thereby is more accessible. Implementation and CitationThe code for the project can be found here. If you use our work, please cite using the following BibTex Citation: @inproceedings{ganesan-etal-2021-n,  title =  N-Best {ASR} Transformer: Enhancing {SLU} Performance using Multiple {ASR} Hypotheses ,  author =  Ganesan, Karthik and   Bamdev, Pakhi and   B, Jaivarsan and   Venugopal, Amresh and   Tushar, Abhinav ,  booktitle =  Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers) ,  month = aug,  year =  2021 ,  address =  Online ,  publisher =  Association for Computational Linguistics ,  url =  https://aclanthology. org/2021. acl-short. 14 ,  doi =  10. 18653/v1/2021. acl-short. 14 ,  pages =  93--98 ,}References : Tür, G. , Deoras, A. , &amp; Hakkani-Tür, D. (2013, September). Semantic parsing using word confusion networks with conditional random fields. In INTERSPEECH (pp. 2579-2583). "
    }, {
    "id": 53,
    "url": "/authentication-in-grpc/",
    "title": "Authentication in gRPC",
    "body": "2021/10/31 - In gRPC, there are a number of ways you can add authentication between clientand server. It is handled via Credentials Objects. There are two types of credential objects:  Channel Credentials: These are handled on the channel level, i. e whenthe connection is established and a channel is created.  Call Credentials: These are handled on per request level, i. e for everyRPC call that is made. These Credential objects can also be combined tocreate CompositeChannelCredentials with one Channel Credential and oneCall Credential object. Now let us see how we can use these credential objects. Client-Side TLS/SSL Authentication: gRPC provides a way to establish a connection without any secure connection i. e just like HTTP. // client. goconn, _ := grpc. Dial( localhost:5000 , grpc. WithInsecure())// server. golis, err := net. Listen( tcp ,  :5000 )s := grpc. NewServer()s. Serve(lis)For secure communication, we will create TransportCredentials which is a typeof ChannelCredential object. // client. gocreds, _ := credentials. NewClientTLSFromFile(certFile,   )conn, _ := grpc. Dial( localhost:5000 , grpc. WithTransportCredentials(creds))// server. golis, _ := net. Listen( tcp ,  localhost:50051 )creds, _ := credentials. NewServerTLSFromFile(certFile, keyFile)s := grpc. NewServer(grpc. Creds(creds))s. Serve(lis)You can read more about generating own ssl certificates here. In the case where you don’t own the client, it means you are creating a gRPCAPI for public use, you cannot give your certificate to everyone using yourclient. In that case, we rely on well known CertificateAuthority like LetsEncrypt, Amazon, etc. to generate acertificate. So let us change our client code a little. // client. goconfig := &amp;tls. Config{ InsecureSkipVerify: false,}conn, err := grpc. Dial(address, grpc. WithTransportCredentials(credentials. NewTLS(config)))// server code remains the sameIn this case what happens is that grpc loads the certificates of well-knownCertificate Authorities from the OS and sends it to the server, hence no needto manually provide a certificate. Token-Based Authentication / OAuth2: Many a time we want to differentiate a client by issuing them different tokens. TLS Authentication is a good way to secure your connection but it does not tellus from which client the request is coming from. We will send the token inrequest metadata just like HTTP Headers. gRPC has google. golang. org/grpc/metadata package to send and receive metadatawith a RPC request. // client. go// add metadata to request contextmd := metadata. Pairs( Authorization ,  Bearer xxx-xxx-xxx )ctx := metadata. NewContext(context. Background(), md)// use this context to call rpc_, err = client. CallRPC(ctx, requestObject)We will use UnaryInterceptor on the server which acts as middleware andchecks for the token for all the requests. // create a middlewarefunc AuthInterceptor(ctx context. Context, req interface{}, info *grpc. UnaryServerInfo, handler grpc. UnaryHandler) (interface{}, error) {  meta, ok := metadata. FromContext(ctx)  if !ok {   return nil, grpc. Errorf(codes. Unauthenticated,  missing context metadata )  }   // Take care: grpc internally reduce key values to lowercase  if len(meta[ authorization ]) != 1 {    return nil, grpc. Errorf(codes. Unauthenticated,  invalid token )  }  if meta[ authorization ][0] !=  xxx-xxx-xxx  {    return nil, grpc. Errorf(codes. Unauthenticated,  invalid token )  }  return handler(ctx, req)}// pass this when creating serverserver := grpc. NewServer(  grpc. UnaryInterceptor(AuthInterceptor),)But above code only works for non-streaming RPCs. For Streaming RPCs you canimplement StreamInterceptor. Instead of implementing it again you can usethis package. I hope this article helps you with authentication in gRPC. If you are interested in working with cutting-edge technologies, come work withSkit. Apply here. "
    }, {
    "id": 54,
    "url": "/Code-Mixing-Seminar/",
    "title": "Seminar - Code Mixing in NLP and Speech",
    "body": "2021/08/24 - Below are some pointers and insights from the papers that we covered in the recently concluded seminar on Code-mixing in NLP and Speech. During the seminar we covered 6 papers, two each from NLP, Speech Synthesis and Speech Recognition. Session-1 (NLP)Paper: GLUECoS : An Evaluation Benchmark for Code-Switched NLPPresenter: Shashank Shailabh The main idea of GLUECoS is to introduce a benchmark evaluation task set for code-switched NLP. It is inspired from GLUE (Generalized Language Evaluation Benchmark) and covers tasks such as Language Identification (LID), Part-Of-Speech (POS) Tagging, Named Entity Recognition (NER), Question &amp; Answering (QA), Natural Language Inference (NLI) and sentiment analysis (SENT).  and also introduces benchmark code-mixed datasets. The language pairs that this benchmark covers in English-Spanish and English-Hindi code-mixed datasets. The paper also introduces several Metrics, most of which are covered in the previous blog on Metrics for Code-switching. To arrive at a baseline performance, several cross-lingual word embeddings models such as MUSE (supervised and unsupervised embeddings), BiCVM, Biskip and GCM embeddings are used. For supervised cross-lingual embeddings one requires a parallel mono-lingual corpus of the two languages such that each sentence in one language maps as close as possible to the parallel sentence in the other language. In the unsupervised setting an adversarial loss is used to train cross-lingual embeddings. The other model that is used for the baseline is multilingual models (mBERT). This model is simply trained on monolingual data from each language, and has no explicit loss to map the embedding of the same words from both languages to the same point. Another fine-tuned version of mBert is used which is fine-tuned on synthetically generated code-switched data. Interestingly the mBert model performs better as compared the cross-lingual embeddings and fine-tuning on synthetic data further improves performance. Session-2 (Speech Synthesis)Paper: Building Multilingual End-to-End Speech Synthesisers for Indian LanguagesPresenter: Swaraj The goal of the paper is to build a Multilingual TTS for Indic languages in a low data resource setting. The paper discusses ways of assimilating inputs from different indian scripts both at the level of characters and phonemes, into a common input set. This leverages the similarities in phonemes and the structure of Indian languages which fall in the Abjad class of languages where vowels are diacritics. For phonemes, it looks at 2 approaches : a transliteration approach using the unified parser and the common label set; and another approach that considers a 1:1 phoneme map between the different languages. At character level they present the MLCM character set that is 68 characters in total and combines 8 indic scripts. Since vowels are a major component in indic languages, they consider approaches where the vowel and vowel modifier are mapped to the same and different input token. For modelling they use tacotron-2 and compare the impact of various input tokens on performance for monolingual and multilingual settings. The primary take away from the discussion were the different ways of combining and varying the input representations for training an Indic language TTS. Session - 3 (NLP)Paper: BERTologiCoMix, How does Code-Mixing interact with Multilingual BERT?Presenter: Jaivarsan B This paper addresses the impact (on code-mixed tasks) of fine-tuning with different types of code-mixed data and outlines the changes to mBERT’s attention heads during such fine-tuning. Code-mixing can be treated as a domain adaptation problem, but in this particular context it is to adapt at the grammatical level, not purely at the vocabulary level or style level. For sake of empirical study they evaluate two different language pairs, English-Spanish (enes), English-Hindi (enhi) with the extra three different varieties on them. Those three varieties are namely: randomly-ordered CM (l-CM), grammatically appropriate CM (g-cm) and real-world CM (r-CM), so in total we have 2 lang pairs \(\times\) 3 varieties = 6 possible fine-tuning datasets on mBERT against the plain mBERT. They evaluate these 6 combinations of CM datasets on GLUECoS benchmark. They empirically find out that naturally occurring code-mixed data (r-CM) brings in the best performance improvement after fine-tuning, against synthetic fine-tunings (l-CM, g-CM) and even plain mBERT. Other task specific observations include: fine-tuning with r-CM data helps with SENT, NER, LID and QA CM tasks. General trend observed was easier tasks like LID, POS are solved in earlier layers and as complexity of tasks increases, the effective layer moves deeper. This paper’s other significant contribution is to visualize the differences to these mBERT attention heads post-CM-fine-tuning. They use three methods (including one of their own contribution), which try to answer questions related to: Has anything changed within the models due to pre-training with CM datasets? Attention patterns of which heads have changed? How do attention heads respond to code-mixed probes?. There are changes to these models, which show these attention heads are receptive to CM specific data at different layers of mBERT for different tasks and different language pairs. The most important finding from these probing experiments is that there are discernible changes introduced in the models due to exposure to CM data, of which a particularly interesting observation is that this exposure increases the overall responsivity of the attention heads to CM. Session - 4 (Speech Recognition)Paper: Exploiting Monolingual Speech Corpora for Code-mixed Speech RecognitionPresenter: Kriti Anandan This paper aims to mitigate the problem of scarcity in code-mixed datasets for training ASR models robust to code-mixed data. It introduces two algorithms to synthetically generate code mixed data by using annotated monolingual data sets that are available in large quantities and a small amount of annotated real code-mixed data. Both these algorithms make use of probability distributions derived from real code-mixed data to imitate their characteristics. The code-mixed sentences are framed from sentence fragments extracted from the monolingual corpus. In the paper, first algorithm is a naïve approach that uses the code-mixed language span distributions to weave fragments of two different languages in an alternate fashion where fragment lengths are picked by sampling the distribution. The second algorithm uses two distributions that try to model phone transitions across fragments and within fragments. The authors also demonstrate the usefulness of these synthetically generated datasets by adding them to the existing monolingual dataset for training the acoustic model of an ASR. Results from the best acoustic model show a ~ 6. 85 point drop in WER score. They also conduct some language modelling experiments by using code-mixed transcripts to train the language model of the ASR. The best system uses the best acoustic model trained from the previous experiment and the language model trained on the synthetic text generated by all the algorithms presented, as well as the real code-mixed text. The results via this method show a ~ 8. 17 point drop in the WER score. Session - 5 (Speech Recognition)Paper: Learning to recognize code switched speech without forgetting monolingual speech recognitionPresenter: Shahid Nagra The goal of the paper is to fine tune ASR models on code switched speech without affecting the performance on the same models on monolingual speech. They propose the use of a learning without forgetting algorithm. This approach can also be used when one doesn’t have access to the monolingual data that the model was trained on, which is often the case for open-source ASR models. They conducted 5 experiments where training and fine-tuning is varied across the 3 datasets, monolingual and code-mixed and pooled. The best model performance is got when fine-turning a model on pooled data (code-mixed + monolingual) where only 25% of code-mixed data is used. An extra KL Diverge loss between the fine-tuned and the pre-trained model is added to prevent the increase in WER on the monolingual data. A metric poWER is also introduced which measures the Levenstein distance between the phonemic representation of utterances which takes care of measuring WER in the code-mixed setting. Session - 6 (Speech Synthesis)Paper: Code-Switched Speech Synthesis Using Bilingual Phonetic Posteriorgram with Only Monolingual CorporaPresenter: Shangeth Rajaa This paper aims to synthesize fluent code-switched speech using only monolingual speech corpora and explores the usage of Bilingual Phonetic Posteriorgram (PPG) as the speech representation. PPGs are the posterior probabilities of each phonetic class for a specific frame of one utterance, stacking up the PPG for two languages forms the bilingual PPG. As obtaining a bilingual/code-switched speech corpus for TTS training can be expensive, this paper solves the problem, by training a tacotron2 based code-switched TTS system with only the monolingual corpus of both the languages (English and Chinese) where each language is spoken by a different speaker. The trained model is then evaluated by generating speech for a code-switched text corpus with Mean Opinion Score (MOS) score. The paper introduces the use of Bilingual PPG as a representation of speech as PPGs are speaker-independent and can capture the context of the speech better, PPG features are computed with a pretrained ASR model for each language. They replaced the traditional acoustic model which predicts the acoustic features(Mel Spectrogram) directly from text, to predict the PPG from the text, followed by predicting the Mel Spectrogram from the predicted PPG, speaker embedding and language embedding. The proposed method also includes a residual encoder to encode the speech into a latent space in a Variational AudioEncoder (VAE) setting which helps to model the prosodic features of the audio. The proposed method was compared with a tacotron2 based text to acoustic feature (Mel spectrogram) model baseline. As the proposed method uses a PPG feature, the model was able to disentangle the context information of the speech signal across different speakers and synthesize better code-mixed speech by combining the PPG feature, speaker embedding, language embedding, and prosody features. From the experimental results, we can observe that the proposed method can synthesize high intelligible code-switched speech with a cross-lingual speaker (eg: Chinese speaker for English/code-switched text) with only training on a monolingual corpus. But the proposed method failed to beat the baseline in speech fidelity MOS score. The authors stated that the Bilingual PPG extracted was not accurate enough and needs to be further studied in future work. Do check out the blog on Code-Mixing metrics if you haven’t already. Please write to us @ machine-learning@skit. ai in case you want to join any of our future seminars. "
    }, {
    "id": 55,
    "url": "/Code-Mixing-Metrics/",
    "title": "Code Mixing Metrics",
    "body": "2021/08/09 - We at skit, recently concluded a seminar series on code-mixing, where we covered research papers that looked at approaches to deal with code-mixed data across ASR, TTS and SLU related services. Code-mixing is a common phenomena in the Indian subcontinent. Code-mixing refers to the situation when two or more languages are present in an utterance or a corpus. It is important to formulate, discuss and deliberate on metrics because, it is improvement on metrics across benchmark datasets that spur continuous research in a field. TerminologiesBecause how can one study a phenomena without having a requisite vocabulary.  Matrix Language : The Matrix Language-Frame (MLF) model is one of the dominant models to analyse code-switching. In this framework there is a Matrix Language and an Embedded Language. The embedding language is inserted into the mono-syntactic frame of the Matrix language.  Inter-sentential switching : Occurs outside the sentence or the clause level Intra-sentential switching : Occurs within a sentence or a clause Intra-word switching : Occurs within a word itself, sometimes at phoneme boundaries example : “I’m chalaaoing a car” → “i am driving a car” Language Span: The number of monolingual words between 2 consecutive switch points in the corpus. The language span distribution is the aggregate of all such language spans into a discrete pdf. The performance of models on code-mixed corpora, would depend on the level of code-mixing. A minuscule level of code-mixing could be treated as noise/OOV and ignored. However, as the level of code-mixing increases, the performance of the model will vary. Therefore it is important to quantify the extent of code-mixing in a corpora; “how much” and“how often”. In the first paper covered in the code-mixing seminar, there were several metrics presented, however they were not discussed in detail. We discuss them below. Measuring the amount of Code-MixingWord-frequency based metrics: Code-Mixing Index (CMI) : An utterance level metric that measures the fraction of tokens(words) that are not from the matrix language. [CMI = 100*(1 - \frac{max(w_i)}{n-u}) \text{ if } n&gt;u] where,n = total number of words, irrespective of languageu = language independent words\(w_i\) = number of words in language i\(max(w_i)\) measures the number of words in the matrix language If \(n=u\) i. e. if the utterance contains only language independent words then CMI = 0. One way calculate CMI for the entire corpus is to just calculate the above at a corpus level rather than an utterance level. However, this method doesn’t take into account the switching frequency. Another way is to combine the utterance level CMI as discussed here. Multilingual Index (M-index) : Quantifies the ratio of languages in the corpora based on the Gini coefficient to measure the inequality distribution of languages in the corpus. [M-index = \frac{1-\sum p_j^2}{(k-1). \sum p_j^2}] where,k = number of languages\(p_j\) = number of words in language j divided by total number of words The M-index = 0 when the corpus is monolingual and = 1 when there is equal distribution of token across all the languages i. e. \(p_j\) for all \(j = 1/k\). Language Entropy (LE) : An information theoretic alternative to the M-index. Measures the number of bits required to describe the distribution of language tags. [LE = -\sum_{1…k} p_i*log_2(p_i)] where,k = number of languages\(p_i\) = number of words in language j divided by total number of words This metric is 0 for a monolingual corpus and is bounded by \(log_2(k)\) for equally distributed k languages. Both LE and M-index can be derived from one another. However, just specifying the frequency of words belonging to another language doesn’t provide enough information. A corpus with a higher frequency of language switches per utterance is more complex, therefore we need some measures on the frequency and distribution of code-switching points. Measuring code-switching: Probability of Switching (I-index) : The average number of switch points in the corpus. [f_n = P/(N-1)] where,\(P\) = number of code-switching points\(N-1\) = possible switching points in a corpus that contains n tokens A token is considered a switch point, if the preceding token is from another language. Time-course Metrics: Since, a corpus is a sequential document, it is informative to have a time-series metrics that quantifies the temporal distribution of code-switching across the corpora. Burstiness : Measures whether code switching occurs in bursts or is periodic is nature. It compares the code-switching behaviour in the corpus to a poisson behaviour where code-switching occurs at random. Let, \(\mu_t\) = mean language span and \(\sigma_t\) = s. t. of language span, then [Burstiness = \frac{\sigma_t-\mu_t}{\sigma_t+\mu_t}] Burstiness is bounded by \([-1,1]\). For, corpus that have a periodicity in code-switching points, this value is closer to -1 and for corpuses that have less predictable periodicity have a value closer to 1. Span Entropy (SE) : An information theoretic measure of the language span distribution i. e. the number of bits needed to describe the probability distribution. [SE = -\sum_{1…M} p_l*log_2(p_l)] where,\(p_l\) is the sample probability of a language span of length lM is the maximum language span Memory : The above two metrics don’t make any claims about the time ordering of the language spans i. e. say if a long span occurs below or after the current span, then there would be no difference in the above metric values. Corpus’ with the same burstiness can have very different properties. This metric measures the extent to which the \(i^{th}\) span length is influenced by span lengths occurring before it. This metric is bounded by \([-1,1]\). When it is closer to -1, then the length of consecutive spans in negative correlated i. e. long spans followed by short ones. Evaluating Code-Mixed ASR and TTSASR: Conventional word error rate is not sufficient for measuring the performance of code-mixed models due to cross-transcription, misspellings and borrowing of words. One of the issues for transcription of code-mixed speech is transliteration. For example, say the actual transcripts contain first two words of a sentence in Devanagari and the last 2 in Roman but the transcripts from ASR contain only Roman script. Then unless we have transliteration, we can’t get accurate WER scores. This issue is further compounded by non-uniform transliteration standards, across both the training data and the eval data especially since languages often borrow words from other languages. However, there are no standard metrics that exist as of now. Below two metrics that measure different aspects are discussed. CM-WER : If there are M words on both sides of switch points across all reference transcriptions and N edits in the ASR hypotheses corresponding to words surrounding the switch points in the references, then \(\text{CM-WER} = \frac{N}{M}\) . This metric provides an estimate of how accurately the system predicts words at switch points. poWER : Prononciation Optimised WER. It is defined as the Levenshtein distance between the pronunciation optimized hypothesis (H) and reference (R) sentence, normalised by the number of words in the reference sentence. [\text{poWER} = \text{LevDist}(f(H), f(R))/N] Here, \(f\) = grapheme-to-phoneme (g2p) conversion of each word in the sentence So, for example, the following below sentences would have a poWER of zero. HYP : रूम service आपको कै सी लगREF : room service आपको कै सी लग TTS: Degradation MOS : Annotators listen to a sample audio which is considered to have no degradation. The next audio is rated in comparison to that, in terms of degradation, out of 5. A score of 5 means no degradation, a score of 1 means the highest degradation. While testing a code-mixed TTS, it one can use degradation score of monolingual sentences generated from Multi-lingual TTS as compared to Mono-lingual TTS. Notes:  The above measures can be used to specify different patterns of code switching which have been termed as “code-switching attitudes”. These can be thought of as aspects of personality and are correlated with regional identities and other trait properties. If you have any questions please reach out to swaraj@skit. ai. References Comparing the Level of Code-Switching in Corpora On measuring the complexity of code-mixing- Metrics for modeling code-switching across corpora Simple tools for exploring variation in code-switching for linguists Burstiness and memory in complex systems"
    }, {
    "id": 56,
    "url": "/normalizing-flows-part-2/",
    "title": "Normalizing Flows - Part 2",
    "body": "2021/05/08 - In Part-1, we introduced the concept of normalizing flows. Here, we discuss the different types of normalizing flows. In most blogs that discuss Normalizing Flows, several concepts related to autoregressive flows and residual flows aren’t discussed very clearly. We hope to simplify the explanations of a relatively theoretical topic and make them accessible. Topics Covered: Element-wise Flows Linear Flows Planar and Radial Flows Coupling Flows Autoregressive Flows     Masked Autoregressive Flows   Inverse Autoregressive Flows    Residual Flows Convolutions and NFsElement-wise FlowsThis is the simplest normalizing flow. In this case, the bijective function \(T\) can be broken into individual components i. e.  \(T(x_1, x_2. . . x_d) = (h_1(x_1), h_2(x_2). . . h_d(x_d))\) s. t. every \(h_i\) is a transform from \(R \rightarrow R\). Each dimension has its own bijective function. However, these simple flows don’t capture any dependency between dimensions. Some activation functions like Parametric Leaky RELU (bijective) can be considered element-wise NFs. Linear FlowsLinear flows are able to capture correlations between dimensions and are of the form : \(T(x) = Ax+b\) , where \(A \in R^{D*D}\) and \(b \in R^D\) where \(D\) is the dimensionality of the input. For the transform T to be invertible, only the matrix A needs to be invertible. When matrix A is a non-diagonal matrix, it captures correlation between different dimensions. Linear flows are limited in their expressive power. Starting out with a Gaussian \(z \sim N(\mu, \sigma)\), we simply arrive at another Gaussian \(z' = N(A*\mu+b, A^T*\sigma*A)\), similarly for other distributions. However, they serve as important building blocks for NFs. In terms of computational complexity, computing the determinant and inverse are both of order \(O(D^3)\) where \(D\) is the dimensionality of matrix A. To make the calculation more efficient, there need to be some restrictions on A. Possible restrictions on A:  A is a diagonal matrix: this reduces computation time for the calculation of the det. and inverse to \(O(D)\). However, this is equivalent to element-wise bijections.  A is a triangular matrix: It is more expressive than a diagonal matrix. The computation time for calculation of the determinant is \(O(D)\) and for the inverse is \(O(D^2)\).  A is an orthogonal matrix: Inverse of an orthogonal matrix is simply its transpose and the determinant is either \(\pm 1\). There are some other methods that use LU factorisations, QR decomposition which essentially exploit matrix properties to make the computation of inverse and determinant efficient and at the same time allow A to be more expressive. Planar and Radial FlowsThey were first introduced in Rezende and Mohamed [2015], but aren’t widely used in practice and therefore not covered in details here. Planar flows expand/contract the distribution around a certain direction (specified by a plane) and radial flows modify the distribution around specific points. The effect of the flows can be seen below. The left half shows the effect of planar NFs on an initial distribution of a Gaussian and uniform distribution, and the right half shows the effect of a radial flow.   Fig 1 : Examples of planar and radial flows on initial distributions like Gaussian and uniform distributions. Coupling FlowsCoupling flows are highly expressive and widely used flow architectures. They can either have linear or non-linear invertible transformations. Coupling flows are defined by two sets of mappings, an identity map and a coupling function \(h\). Consider a disjoint partition of the input \(x \in \mathbb{R}\) into two subspaces: \((x^A, x^B)\) \(\in\) \(\mathbb{R}^d \times \mathbb{R}^{D-d}\).  A bijective differentiable coupling function is defined as, \(h(\cdot ;\theta)\) : \(\mathbb{R}^d \rightarrow \mathbb{R}^{D}\) that is parameterized by \(\theta\).  The equations for both the mappings are shown below. \(y^A\) = \(h(x^A, \theta(x^B))\)  \(y^{B}\) = \(x^B\) The bijection \(h\) is called a coupling function, and the resulting function \(g\) that consists of both the mappings is called a coupling flow. Coupling flow is invertible if and only if \(h\) is invertible and has an inverse. Let’s consider an example : Let us consider an input with 4 dimensions \((x1,x2,x3,x4)\). Let \(x^A = (x1,x2)\) and \(x^B = (x3,x4)\). Let \(\theta(x^B) = x3+x4\) and let the coupling function \(h(x^A, \theta(x^B)) = \theta(x^B) * x^A\). Given this example, the coupling function \(h\) is invertible but only if \(\theta\) isn’t zero. Therefore, the invertibility requirement of \(h\) introduces a limitation on \(\theta\) as well. In this case, it can’t be defined as a simple additive function. The restriction for non-zero values needs to be taken into account.   Fig 2 : Example coupling flow architectures. The figure above shows two examples of coupling flow architectures. In figure (a), \(x^{A}\) and \(x^{B}\) are the two input subspaces. The coupling function \(h\) is applied to \(x^{A}\) directly while it is parameterized on \(x^{B}\). In figure (b), there are two subsequent flows, where the size of input subspace on which the coupling function \(h\) is applied, gradually increases with each flow. There are various types of coupling functions. We discuss some of these briefly,  Additive coupling     This is one of the simplest form of coupling functions defined by, \(h(x;\theta) =\) \(x + \theta\),   where \(\theta\) is a constant \(\in \mathbb{R}\)    Affine coupling     \(h(x;\theta) =\) \(\theta_1x + \theta_2\),   \(\theta_1 \neq 0\), \(\theta_2 \in \mathbb{R}\) Both additive and affine coupling were introduced in NICE.     Neural auto-regressive flows     This was first introduced by Huang et al [2018]. The coupling function \(h(\cdot;\theta)\) is modelled as a neural network. Every neural network can be considered as a parameterized function, the parameters being the weights. In this case, the weights of the neural network are defined by \(\theta\). To ascertain that the neural network follows bijectivity, the following proposition is applied :         If \(NN(\cdot)\) : \(\mathbb{R} \rightarrow \mathbb{R}\) is a multilayer perceptron, such that all weights are positive and all activation functions are strictly monotone, then \(NN({\cdot})\) is a strictly monotone function.  So, since the neural network is monotone, it is also invertible and hence a normalizing flow.           More types of coupling flows can be found in this paper. Autoregressive FlowsAn autoregressive flow is a type of normalizing flow where the transformations use autoregressive functions. The term autoregressive originates from time-series models where the predictions at the current time-step are dependent on the observations from the previous time-steps. The probability distribution of an autoregressive model is given by, the equation below where the output at time-step \(i\) is conditioned on all the previous outputs.       [p(x) = \prod_{i=1. . D} p(x_i   x_{1:i-1})]   An autoregressive function can be represented as a coupling flow: Let the coupling function \(h(\cdot ;\theta)\) : \(\mathbb{R} \rightarrow \mathbb{R}\) be a bijection parameterized by \(\theta\), and let \(x_{1:t}\) be the set of inputs Then, an autoregressive model is a function \(g : \mathbb{R}^D \rightarrow \mathbb{R}^D\), in which every entry of the output \(y = g(x)\) is conditioned on the previous entries of the input: [y_t = h(x_t ; \theta_t(x_{1:t-1})] The functions \(\theta_t(\cdot)\) are called conditioners. \(\theta_1\) is a constant whereas \(\theta_2\), \(\theta_3\) … , \(\theta_t\) are arbitrary functions mapping \(\mathbb{R}_{t-1}\) to the set of all parameters. The conditioners need to be arbitrarily complex for effective transformations and hence, are usually modelled as neural networks. Since each output depends only on the previous inputs, the Jacobian matrix of an autoregressive transformation \(g\) is triangular (refer to Part 1 for explanation). The determinant of a triangular matrix is simply a product of its diagonal entries.       [\det{(Dg)} = \prod_{t=1. . k}   \frac{\partial y_t}{\partial x_t}   ]   Masked Autoregressive Flows (MAFs): The time taken to train autoregressive flow models is very high because of the need to sequentially generate outputs. Using MAFs one can parallelize the training process. MAFs were inspired by the observation that on stacking several autoregressive models, where each model had unimodal conditionals, the normalizing flow can learn multi-modal conditionals (MAF [2018]). The architecture consists of stacked MADEs with Gaussian conditionals which are explained below. What are MADEs? MADE stands for Masked Auto-Encoder for Distribution Estimation (MADE [2015]) are essentially auto-encoders with some modifications like masks. In a MADE, the autoregressive property is enforced by multiplying the weight matrices of the hidden layers of the auto-encoder with a binary mask. These masks ensure that the training of the sequential auto-regressive model can be done in parallel. The masking is done such that forward passes with mask are equivalent to conditioning the output only on the earlier sequences of input. This is shown in the below image. On the left, a typical auto-encoder with 2 hidden layers is shown. If one wants to represent the output as an autoregressive sequence specified by the equation below, then masks are multiplied to the weights of the auto-encoder.       [p(x) = p(x_2) * p(x_3   x_2) * p(x_1   x_2, x_3)]   The order of the inputs are \(x_2, x_3, x_1\) The image on the right shows the network weights after masking. For example \(p(x_2)\) doesn’t depend on any of the other inputs since it is the first in the sequence. \(p(x_3)\) is dependent on only the nodes that depend on \(x_2\) and similarly for \(p(x_1)\) The general form for masking is discussed in the MADE paper. The MADE architecture parallelizes the sequential autoregressive computation.   Fig 3 : This image is taken from the MADE paper an explains the idea of masking, so as to parallelizes the sequential autoregressive computation. Density Estimation and Sampling: There are two important concepts when studying flow architectures : density estimation and sampling. Both these passes have different computation complexity and for auto-regressive models, the computation time of one is inversely related to the other.   Fig 4 : Forward pass of MAFs (left) vs. Inverse pass of MAFs (right). The diagrams above demonstrate a forward pass and an inverse pass in a MAF. The output \(x_i's\) depends upon the input \(z_{i}\) and the scalars \(\alpha_{i}\), \(\mu_{i}\) which are computed using \(x_{1:t-1}\).  It is these scalars that define the density parameters of the distribution. This is also called a scale and shift transform. The reason why it is designed this way is that inverting \(f(x)\) does not require us to invert the scalar functions \(\alpha_{i}\) and \(\mu_{i}\). \(f^{-1}(x_{i}) = z_{i} = \frac{x_{i} - \mu_{i}}{exp(\alpha_{i})}\). MAFs can compute the density \(p(x)\) very efficiently. During the training process, all the \(x_i's\) are known. One can therefore parallelize this step using masks. Sampling on the other hand (predicting \(x_{D}\)) is slower as it requires it requires performing \(D\) sequential passes (where \(D\) is the dimensionality of \(x\)) to calculate the previous samples (\(x_{1}, x_{2} . . . x_{D-1}\)) before computing \(x_{D}\). Inverse Autoregressive Flows (IAFs):   Fig 5 : Inverse pass of MAF (left) vs. Forward pass of IAF (right). Since MAFs are better at training by exploiting parallel computation but slower during sampling, we modify the functions to get another flow architecture that is faster at sampling. This gives us Inverse Autoregressive flows (IAFs). The figures above show a comparison between the inverse pass of a MAF and the forward pass in an IAF.  The only difference between both the architectures is that for IAF’s the autoregression is based on the latent variables and not the predicted distribution. The scale(\(\alpha_{i}\)) and shift(\(\mu_{i}\)) quantities are computed using previous data points from the base distribution instead of the transformed distribution. IAF can generate samples efficiently with one pass through the model since all the \(z_i's\) are known. However, the training process is slow, since estimating the \(z_i\) requires i sequential passes to calculate all the required input variables \(z_{1}\), \(z_{2}\), . . \(z_{i-1}\). Aside : According to our understanding, during training, both the weights of the network parameters(\(\alpha, \mu\)) and \(z_i's\) are estimated i. e. the ith latent variable is itself treated as a parameter that is to be learned. One should use IAFs if fast sampling is needed, and MAFs if fast density estimation is desirable. Parallel WaveNet [2017] which was once the state-of-art model for speech synthesis made use of a MAF and an IAF. A fully trained “teacher” network that used MAF architecture was used to train a smaller and parallel “student” network that used IAF architecture. Since the teacher used MAF architecture it could be trained fast. Once the student network, an IAF model, was trained, it could then generate all the audio samples in parallel without depending on the previously generated audio samples and with no loss in audio quality. Residual FlowsDuring training, deep networks find it difficult to reach a convergence point due to the vanishing/exploding gradient. In such cases, adding more layers to the network only results in a higher training error. Residual Networks or ResNets were introduced to solve this problem. ResNets consist of skip-connection blocks. A residual block is shown below. The output of the residual block is represented by the equation below, where \(F(x)\) represented the pass through the neural layers. [M(x) = F(x) + x]   Fig 6 : A residual block. How does this help?Skip connections in a deep neural network allow the back-propagation signal to reach the initial layers of the neural network thereby solving the vanishing gradient problem. Instead of treating the number of layers as an important hyper-parameter to tune, by adding skip connections to our network, we are allowing the network to skip training for the layers that are not useful and do not add value to overall accuracy. In a way, skip connections make our neural networks dynamic, so that they may optimally tune the number of layers during training. Residual flows in their general form aren’t invertible and can only be used in flow architectures after applying some constraints(first introduced in Chen et al. [2019]). This is discussed next. A Lipschitz condition is necessary to enforce invertibility in residual flows. Lipchitz condition :A real valued function : \(f : \mathbb{R} \rightarrow \mathbb{R}\), is Lipchitz continuous if there exists a positive real constant \(K\) such that, for all real \(x_1\) and \(x_2\), \(|f(x_1) - f(x_2)| \leq K| x_1 - x_2 |\) If \(K = 1\) the function is called a short map, and if \(0 \leq K &lt; 1\) the function is called a contraction. Invertible Residual Flows:Given a residual flow of the form, \(F(x)=x+g(x)\), if the mapping \(g(x)\) has a lipschitz constant \(K\), \(0 \leq K &lt; 1\) then the residual flow is invertible. We discuss a short proof of this next. For the residual flow to be invertible, proving that \(F(x)\) is monotonically increasing is sufficient i. e. we need to prove \(F(x+\delta) - F(x) &gt;= 0\) [F(x+\delta) - F(x) = x+\delta + g(x+\delta) - (x - g(x)) = \delta + g(x+\delta) - g(x)] From the Lipschitz condition for \(g\), we have \(|g(x+\delta) - g(x)| \le |\delta|\), since \(0 \leq K &lt; 1\). Therefore, \(F(x+\delta) - F(x)\) is always &gt;= 0 and hence monotonically increasing. The addition of invertible residual blocks greatly enhances the class of neural network flow models and also ensures we can train deeper models without the fear of vanishing gradient. Convolutions and NFsConvolutions are one of the most common operations used in the design of DNNs, however, the computation of their inverse and determinant is non-obvious. Since they aren’t invertible they can’t be used in the family of NF flows. However, recently [Hoogeboom et al. 2019] defined invertible \(d*d\) convolutions and designed an architecture using stacked masked autoregressive convolutions. This is explained next. In the figure below, on the right, the input x is shown along with the kernel matrix w. The convolution operation between these two can be represented as a matrix multiplication of the form \(A*x\) shown on the right.  The matrix A is non-invertible here, however, if we consider \(f = g = h = i = 0\), then, we get a triangular matrix that can represent a normalizing flow.   Fig 7 : A convolution represented as a matrix multiplication. One of the issues of using a filter where \(f = g = h = i = 0\) is that the receptive field is narrow. However, one can rotate the filter and combine two filters to get different receptive fields as shown on the left in the image below. These can then which can be stacked to capture more expressive combinations as shown on the right. These are called “expressive convolutions” in the paper. The grey and orange filters are rotations of the same filter that result in a triangular matrix. They can be stacked to get different receptive fields shown in blue. These blue receptive fields can be stacked further to arrive at more expressive transforms that have the desired receptive field.   Fig 8 : The part in grey is an example of an individual kernel that would result in a triangular matrix. The grey and orange matrices are all rotations of the standard matrix and the blue matrix shows the receptive field when the grey and orange filters are combined. . This concludes our discussion of normalizing flow architectures. Incase of any doubts/questions please reach out to us at kriti@skit. ai or swaraj@skit. ai. References Normalizing Flows IEEE MAFs Adam Kosiorek’s Blog Lilian Weng’s Blog Stanford Lecture"
    }, {
    "id": 57,
    "url": "/whats-new-kaldi-serve-10/",
    "title": "What's New in Kaldi-Serve 1.0",
    "body": "2021/03/25 - Kaldi-Serve is our open source high performance Speech Recognition server framework capable of serving Kaldi ASR models in production environments for real-time inference, and it’s got an upgrade! What’s Changed?: Originally designed as a standalone application, kaldi-serve had some issues mostly pertaining to it’s usability or extensibility for custom use cases thus greatly reducing the core’s potential to just a handful of rigid situations and dependencies. After one too many requests for changes in the architecture to better fit the various production needs of our customers, we realised it’s time for a rewrite of the framework that allows us to better utilise the core’s scope and extend it’s capabilities in the form of general API consumable in most use cases. Library: Kaldi-Serve is now a general extensible library with all the functionality necessary to serve Kaldi ASR models in production with any server framework of your choice, for ex. gRPC, Open CGI, HTTP, etc. or even import it in your own custom offline application. The earlier standalone gRPC server is now and application that extends the core kaldi-serve library and only contains the frontend gRPC methods that call the general API. We’ll talk about how you can extend the library in your own applications below. Python Port: We also made it easier to use kaldi-serve by porting the core library to python which can be easily installed as a package via pip. The transcription interface is much simpler and faster to get up and running. Below is sample code snippet for transcribing a wav file with your Kaldi Chain model using the kaldiserve python package: from io import BytesIOfrom kaldiserve import ChainModel, Decoder, parse_model_specs, start_decoding# chain model contains all const components to be shared across multiple threadsmodel = ChainModel(parse_model_specs( model-spec. toml )[0])# initialize a decoder that references the chain modeldecoder = Decoder(model)# read audio file as byteswith open( sample. wav ,  rb ) as f:  audio_bytes = BytesIO(f. read()). getvalue()with start_decoding(decoder):  # decode the audio  decoder. decode_wav_audio(audio_bytes)  # get the n-best alternatives  alts = decoder. get_decoded_results(10)print(alts)More Features Coming Soon  Ontology GPU Inference MACE Integration"
    }, {
    "id": 58,
    "url": "/new-blog/",
    "title": "Our new Tech blog",
    "body": "2021/02/28 - We are merging past webpages of ourEngineering andML team in this new, central, SkitTech page. From here on, this is going to be the main source of updates onspeech technology work from Skit. ai. Stay tuned to our rss feed for updates. "
    }, {
    "id": 59,
    "url": "/emnlp/",
    "title": "EMNLP 2020",
    "body": "2020/12/21 - Individual summary notes from EMNLP 2020. Amresh says  I have recently been following the branch of model explainability for practicalpurposes. I was delighted to know EMNLP was experiencing a surge of similarresearch. I encountered Minumum Description Length (MDL) as a measure tounderstand a model’s ability to capture linguistic properties. The core ideabeing changing the probing task of identifying labels to transmitting data. Karthik says  The Dialogue and Interactive systems track in EMNLP was very interesting andintroduced novel techniques and approaches to some of the ML problems that I amcurrently working on for example, MAD-X: a framework for effective transferlearning to new languages using adapter base architecture , TOD-BERT:Pre-training Transformer LMs on open-source dialogue data-sets for betterfew-short learning capability this could be helpful to mitigate cold-startproblem of SLU module. Also the generative approach based papers for dialoguestate tracking were an interesting approach that might have few-shot learningcapability as compared to discriminative ones. Lohith says  “Productising” ML Models has become one of the primary answers for most of thecomplex problem we face in day to day life, and explaining how they work, hasbecome a bigger concern. I was happy to see a lot of papers and demos in thatdirection, like the LIT tool, which helps both the developer and the user tounderstand the underlying workings of an ML model and how it looks at each inputand how they help in the decision making process. I was glad to see a lot ofpapers working with various NLP techniques for better and efficient Health Carefor people, which we can all say is more important than ever, given what we haveseen this year. Manas says  The Insights from Negative Results in NLP, Workshop was a highlight for me. Itrevolved around ideas of what an interesting question is and how to connectideas that didn’t work out. These questions feel central to our research effortsin-team, and the keynotes offered a rewarding and scientific route to exploringopen problems in speech tech. Swaraj says  The papers i liked from this year’s EMNLP were : “Incremental Processing in theAge of Non-Incremental Encoders” by Madureira, B. , &amp; Schlangen, D (2020), whichwas similar to how humans incrementally process natural language and provided away to probe the workings of transformer encoders; and “Digital Voicing ofSilent Speech” by Gaddy, D. , &amp; Klein, D. (2020) which was presented very welland i felt they did a good job modelling the EMG features and got goodimprovements on silent speech. There were a lot of papers and workshops oninterpretability of models which is essential considering the direction wherethe field is heading and the gather sessions did a good job of ensuring onedoesn’t miss out on the social aspects of attending a conference. "
    }, {
    "id": 60,
    "url": "/normalizing-flows/",
    "title": "Normalizing Flows - Part 1",
    "body": "2020/12/19 - Normalizing flows, popularized by (Rezende, &amp; Mohamed,2015), are techniques used in machinelearning to transform simple probability distribution functions intocomplicated ones. One of the popular use cases is in generative modelling - anunsupervised learning method - where the goal is to model a probabilitydistribution given samples drawn from that distribution. Why bother about normalizing flows?:    They have been used in many TTS (text-to-speech) models, memorably in theParallel WaveNetmodel (2017) where aclever application of normalizing flows resulted in a 1000 times fastergeneration of audio samples in comparison to the originalWaveNet model. The Parallel WaveNet modelwas also deployed on Google assistant for real-time generation of audio.     Normally a back-propagation pass requires the activation value for eachneuron to be stored in memory. This places a restriction on training deeper,wider models on single GPU’s(since GPU’s have limited memory) and forces oneto use small batch sizes during training. In flow based networks, one doesnot need to store the activations at all, as they can be reconstructedonlineduring the back-propagation. This property was leveraged in the RevNetspaper (2017) which uses invertibleresidual blocks. Reducing the memory cost of storing activationssignificantly improve the ability to efficiently train wider and deepernetworks.     Flowtron (2020), an autoregressiveflow based TTS model does a kind of representation learning using normalizingflows to learn an invertible mapping from a data space to a latent spacewhich can be manipulated to control many aspects of speech synthesis (pitch,tone, speech rate, cadence, accent). Flowtron matches state-of-the-art TTSmodels in terms of speech quality and is able to transfer speechcharacteristics from a source speaker to a target speaker, making the targetspeaker sound more expressive.     If you’ve ever thought about reversible networks, Normalizing flows doprecisely that. Reversibility of flows also means that one can triviallyencode images into the latent space for editing. They also have coolmathematical applications, for example their use in Neural ODEsolvers (2019. ) which use continuousnormalizing flows.  Brief Introduction: Definition: A Normalizing Flow is a transformation of a simpleprobability distribution into a more complex distribution by a sequence ofinvertible and differentiable mappings.  Note: The above formalism is a simplification, for a more precisedefinition one can consult [5]. The formalism allows piecewise continuousfunctions to be used in the construction of the flow which the abovedefinition restricts. Normalizing since the transformed distribution needs to be normalized bythe change of variables formula (discussed below). Flow refers to theseries of invertible transformations which are composed with each other tocreate more complex invertible transformations. When applied as density estimators, some NFs provide a general way ofconstructing flexible probability distributions over continuous randomvariables starting from a simple probability distribution. By constraining thetransformations to be invertible, Flow-based models provide a tractable methodto calculate the exact likelihood for a wide variety of generative modelingproblems. Efficient inference and efficient synthesis: Autoregressive models, such asthe PixelCNN, are also reversible,however synthesis from such models is difficult to parallelize, and typicallyinefficient on parallel hardware. Flow-based generative models likeGlow (and RealNVP) are efficient toparallelize for both training and synthesis. Exact latent-variable inference: Within the class of exact likelihoodmodels, normalizing flows provide two key advantages: model flexibility andgeneration speed. Flows have been explored both to increase the flexibility ofthe variational posterior in the context of variational autoencoders (VAEs),and directly as a generative model.  With VAEs, one is able to infer onlyapproximately the value of the latent variables that correspond to a datapoint. GAN’s have no encoder at all to infer the latents. In flow based generativemodels, this can be done exactly without approximation. Not only does this leadto accurate inference, it also enables optimization of the exact log-likelihoodof the data, instead of a lower bound of it. Mathematical Framework: Let, \(z_0\) be a continuous random variable belonging to a simple probability distribution \(p_\theta(z_0)\) . Let it be a Gaussian with parameters \((\mu, \sigma) = (0,1)\). [z_0 \sim p_\theta (z_0) = N(z_0;0,1)] Normalizing flows transforms the simple distribution, into a desired output probability distribution with random variable \(x\), with a sequence of invertible transformations, \(f_i's\). [z_k = f_\theta (z_0) = f_k…f_2. f_1(z_0) \text{ s. t. each $f_i$ is invertible (bijective)}] The composition of all the individual flows is represented by \(f_\theta\). Since each \(f_i\) is bijective, so is \(f_\theta\). The new density \(p_\theta (z_k)\) is called a push forward of the initial density \(p_\theta(z_0)\) by the function \(f_\theta. \) An example of a transformation obtained by a normalizing flow is shown below, which transforms a base gaussian distribution into a target multi-modal distribution with the help of a bijective function.   Fig 1: The transformation of a base distribution into a target distribution using a bijective function f. The constrains of a distribution being a probability distribution is that \(\int p_\theta (z_0) =1\). However, this doesn’t hold after applying a bijective function (for intuition consider \(f_1 : z \rightarrow z^3\)). Change of Variables Formula: Consider the normalizing flow \(f_1 : Z_0 \rightarrow Z_1\).  If we want the probability distribution of the random variable \(z_1 \sim Z_1\), we need to consider the change of variables formula derived below. Consider the event \(z_0 \sim Z\), mapped to \(z_1 \sim Z_1\) s. t. \(f_1(z_0) = z_1\). Since, the mapping is bijective, the probabilities of the events are the same. Therefore, [p_\theta(z_1)\partial z_1 = p_\theta(z_0)\partial z_0] [p_\theta(z_1) = p_\theta(z_0)* \frac{\partial z_0}{\partial z_1}]       [p_\theta(z_1) = p_\theta(z_0)*   \frac{\partial z_0}{\partial z_1}   ]   (since probabilities are always &gt; 0)       [p_\theta(z_1) = p_\theta(z_0)*   \frac{\partial z_0}{\partial f_1(z_0)}   ]   (\(z_1 = f_1(z_0)\))       [p_\theta(z_1) = p_\theta(z_0)*   \frac{\partial f_1(z_0)}{\partial z_0}   ^{-1}]   In the multivariate case (\(R^D \rightarrow R^D\)) this generalises to:       [p_\theta(z_1) = p_\theta(z_0)*   \det(\frac{\partial f_1(z_0)}{\partial z_0})   ^{-1}]   Considering the sequence of compositional transformations \(f_i's\), one obtains:       [p_\theta(z_k) = p_\theta(z_0)* \prod_{i=1. . k}   \det(\frac{\partial f_i}{\partial z_{i-1}})   ^{-1}]   The term on the right i. e. determinant of the Jacobian accounts for the changeof \(\delta\) volume induced by the transformation. It serves to normalize thetransformed distribution locally, after each flow through a transformation,hence the name, Normalizing Flows. Some Questions at this stage:  The sequence of flows need to be invertible and differentiable. What sort ofconstraints does it introduce in terms of the output distributions that canbe reached? Are there families of distributions that we can’t reach startingfrom a Gaussian?Sampling: In this case, the bijective functions and the initial distribution are given. Sampling points from the output distribution requires calculating the forward pass, i. e. an efficient calculation of the functions \(f_i's\). Density Estimation using Maximum Likelihood: In this case a dataset \(\{a_1, a_2,. . . . ,a_n\}\) is provided and the objective is to learn the probability density function \(p_\theta(A)\) to which the points belong. An initial density \(p_\theta(z_0)\) is chosen. For each \(a_i\) we have:       [p_\theta(a_i) = p_\theta(z_i)*   \det(\frac{\partial f(z_i)}{\partial z_i})   ^{-1}]   where, \(a_i = f(z_i)\)       [\prod_{i=1}^n p_\theta(a_i) =\prod_{i=1}^n [p_\theta(z_i)*   \det(\frac{\partial f(z_i)}{\partial z_i})   ^{-1}]]   We need to maximize the above over all possible flows \(f\) to find the flow \(\hat{f}\) that maximizes the probability.       [\hat{f} =\text{arg max}f \text{ }\prod{i=1}^n [p_\theta(z_i)*   \det(\frac{\partial f(z_i)}{\partial z_i})   ^{-1}]]   Using log likelihood maximization we arrive at:       [\hat{f} =\text{arg max}f \text{ }\sum{i=1}^n [log(p_\theta(z_i)) - log(   \det(\frac{\partial f(z_i)}{\partial z_i})   )]]   The equation shown above is used during training for density estimation. However, since we only know \(a_i's\) the only way to find \(z_i's\) which are used, is to find the inverse mapping i. e. \(z_i = f^{-1}(a_i)\) .  So for density estimation and training, the calculation of both the inverse and determinant of the Jacobian are required. However, calculating the inverse and the determinant of the Jacobian of a sequence of high dimensional transformations can be very time consuming (for dimensionality \(d\) matrix, both are of complexity \(O(d^3)\)). There are various tricks that are used to reduce the complexity of these two operations, one of the popular ones being the use of triangular maps. Triangular maps:: Let \(T\) be a normalizing flow, \(T: z \rightarrow x\) where \(x = (x_1, x_2 . . . . x_d)\) and \(z = (z_1, z_2 . . . . z_d)\). More generally, one can decompose \(T\) into \(T_1, T_2. . . T_d\) such that \(x_i = T_i(z_1,z_2. . . . . z_d)\). Now if we want to introduce an additional constraint on \(T\) i. e. for \(T\) to be a triangular map, each \(T_j\) should be a function of \((z_1,z_2. . . . . z_j)\) i. e. the first \(j\) elements and not all the \(d\) elements. For triangular maps/matrices, both the inverse and the determinant of the jacobian is easy to compute. The jacobian for a triangular map is shown below. The determinant is simply the product of the diagonals and has a complexity of \(O(d)\) instead of \(O(d^3)\). The complexity for the calculation of the inverse is \(O(d^2)\) instead of \(O(d^3)\).   Fig 2: The jacobian for a triangular map. This is taken from here. Note: For an increasing triangular map, \(\frac {\partial T_i}{\partial z_i} &gt; 0\). This will be useful in Part - 2 where the different types/families of Normalizing flows will be considered. References:  Normalizing Flows: An Introduction and Review of Current Methods Flow-based Deep Generative Models Lecture on NFs by Priyank Jaini"
    }, {
    "id": 61,
    "url": "/interspeech/",
    "title": "Interspeech 2020",
    "body": "2020/12/01 - We recently attended the all remote Interspeech2020. Each of us made notes on what they didoverall. But instead of posting those or going with awhat-we-think-about-the-conference style post, we thought to just ask the teamwhat interested them the most in a few sentences. Here are the individualresponses: Abhinav says  I really liked the ZeroSpeech challenge. As usual,they had few really interesting unsupervised problems and solutions. I findtheir tracks really ambitious as evident by this statement on their website“… infants learn to speak their native language, spontaneously, from rawsensory input, without supervision from text or linguists. It should bepossible to do the same in machines”.  Next year, 2021, the target is Spoken LanguageModeling. Looking forward to that too. Amresh says  The Meta Learning Tutorial on day 1 was a detailed session on the topic. Thepromise of performing well on a set of task(s) with less amount of data had myattention. The authors take care of introduction, utility and comparison ofthis approach and its impact on tasks like speaker verification, keywordspotting, Emotion Recognition and my special interest conversational AI. Manas says  A new thing here was Computational Paralinguistics, that covers thenon-content parts of speech. Given my interest in the stylistic parts ofspeech, this was particularly interesting. Papers presented many ideasrelevant to building a better voicebot, like - uncertainty aware methods formultiple labels, Autism Quotient as a perception feature, and predicting CSATscores from sentiment. Prabhsimran says  Interspeech had some great sessions, from discussions on more fundamentalconcepts related to Speech Processing in the Brain, Phonetics and Phonology tonovel ideas on Training Strategies for ASR like Semantic Word Masking,Efficient Vocoder implementations for faster Neural Waveform Synthesis andAutomatic Prosody Analysis for Non-Semantic Speech Representations. It helpedme connect alot of dots and exposed me to some great ideas we should beexploring in our work. Kaustav says  I really liked attending the Speech Emotion Recognition tracks. The trackscovered a multitude of topics including self-supervised learning methods,non-semantic representations, etc. It was overall, a very balanced track witha lot of interaction among the attendees and the presenters. The speech signalrepresentation track was pretty fun too with some really interesting papers onvoice casting, universal non-semantic representations. "
    }, {
    "id": 62,
    "url": "/reading-sessions/",
    "title": "Reading Sessions",
    "body": "2020/11/30 - Studying researches and building on top of them is an important part of what ateam of ML Engineers do on a regular basis. Usually, teams do this by organizingperiodic, often weekly, paper reading sessions. Here is a snippet from aninternal work memo by Manas explaining how welook at these sessions:  Lets start with the basic motivation behind these sessions - we wantto read more papers. But beyond this individual goal, there is alsothe simpler driving force of enthusiasm - we read something we like,and we want to share it. It is the same instinct that drives us totalk about books we read, movies we watch, and podcasts we listen to. …  There are also secondary benefits, like knowledge transfer - bothspeaker and audience will understand the topic better after a goodpresentation - and discovering shared interests within a larger group,etc. But it’s not that easy to bring this in practice. Specially in a startup, whereprocesses and structures are constantly in flux. As the team size keeps growing,different kinds of diversities start interfering. Diversities in interests,reading styles, and even bandwidth. This post covers how we organize reading sessions in the ML team atSkit. ai. It might be helpful if you are tryingto do the same in your group. The very first thing that we did was to start asking people for researchpapers that they like, weekly. After voting on one, the proposer of thepaper presented it on a predetermined day. This lost steam away after awhile because of various reasons. One of them being bandwidth crunch foreveryone that time. We were just 3 people. We revived paper reading after a while. This time everyone picked andpresented a paper of their own liking. This wasn’t supposed to scale upwith team size, but we went with this for a decent while. While we werefree to choose and read whatever we wanted, lack of continuity inreadings, practical disconnects and difference in interests started toreduce the overall engagement. After lockdown, the engagement level dropped further. On video calls,you have to upgrade the quality of meetings if you want to maintain thesame level of participation. The missing modalities hurt significantly. We spent inordinate amount of time trying to get to one single view onhow to go about these sessions. We tried experimenting with variousaspects like paper selection, accountability, presentationaccessibility, etc. Rather than going in those experiments in chronological order, it makessense to think about the problems from two angles based on what we knownow. You can say that the sessions are having certain issues, oralternatively you can say that the people are having issues with thesessions. While both feed on each other and are cyclic, it helps to lookat them separately. Sessions: We can break down sessions temporarily in the following three acts. 1. Pre Session: Here it’s known that a certain session is supposed to happen. You cando the following in preparation:  Set clear expectations. What is supposed to be covered? How it’ssupposed to be covered? Who should come? What will be the outcome?etc.  Excite the potential audience. If the audience is not really awareof the topic, some amount of pre-work needs to be done to pull themin. 2. Session: During the session, you want to:  Make the presentation stimulating and engaging.  Keep the presentation accessible while not being superficial.  Develop practical connections between the audience and content. 3. Post Session: Since you want the next sessions to be successful too, you would liketo:  Make sure people are going away with a healthy amount of thoughtfood.  Nudge towards the utilitarian aspects of concepts discussed so thataudience have a few threads of experimentation to follow in their dayto day work. People: For people, we can think along the following lines1:  Resourcefulness.  Motivation.  Interests, their depths, and varieties.  Style and method of working with new knowledge.  Level of comfort with group sessions. Both for presentation anddiscussion.  Bandwidth. Specially considering industrial settings like ours.  Structural assistance and pushes. Acting on all these factors to deliver a single style of session thatworks for everyone is impossible. Not all factors might be important fora team at a given moment of time, but even a reasonably small set issufficiently diverse. The key idea is accept a pluralistic view on theissue. There is no single fundamentally correct way of doing thesesessions, and it’s better to pick a digestible subset and solve for that. Going ahead with this realization, we started doing seminars. Seminars: Reading Seminars are very similar to seminar courses in Universities. From another internal memo:  These [Seminars] exist to complement the world of paper readings. While Paper Reading sessions are about reading more papers andsharing what we like, Reading Seminars are about learning somethingspecific. These are much more structured and pointed towards a goal. The idea is to have deeper discussions, over longer periods of time,about topics that might interest you. Either directly or indirectly,this will lead to a better output (from the speaker) and experience(for the audience) in the Paper Reading sessions that follow. Seminars cover many of the issues nicely and they clearly don’t toucha few others. For example you can’t just bring in any new paper anddiscuss that in a session without setting up a seminar for that field. And that’s okay. There are other ways of handling that case. At the moment, we have the following parallel seminars running:  Multi-Style TTS End-to-End ASR Speech Representation Learning Dialog State Trackers Computational Paralinguistics Learning TheoryEach of these has a list of papers or topics to be discussed over a period of1-2 months. While not perfect, these are turning out to be decent readingroadmaps for these topics. Something we would like to open out after a couple ofmonths, similar to the old stylepaper reading sessions.       Many are derived from an internal note byManas &#8617;    "
    }, {
    "id": 63,
    "url": "/bad-audio-detection/",
    "title": "Bad Audio Detection",
    "body": "2020/07/29 - This blog will be a short one, where we’ll talk about our approach on filteringout inscrutable audios from VASR. There are situations in Call Center Automation (CCA) pipeline where userutterances are bad. Bad here is defined by things like noise, static,silences or background murmur etc. rendering the downstream SLU systemshelpless. We started with a proposal and prepare a dataset for making an MLsystem learn to reject these audios. Benefits:  No more misfires from SLU side which ultimately leads to a better userexperience.  Save compute and time by skipping bad audios.  The whole system can be used for all our audio based tasks to predict andfilter out the poor ones, hence avoiding sample noise for these tasks. Dataset: We prepared a dataset of intent tagged conversations with specially markedintents which tell us that these utterances are bad and them going further inSLU will result in errors. Also we have a sampling of non-bad utterances(tagged with regular intents) to make this a classification problem. There are total 9928 samples of audios labelled as bad and 20000 sampleslabeled as good. All the raw labels were not very useful, hence we clean and preprocess the datato finally create 2 broad categories with sub-classes.  audio-bad     audio-noisy: Noisy audio.    audio-silent: Silent audio.    audio-talking: Background talking.    hold-phone: Music from keeping on hold.     audio-goodExploratory Data Analysis: We needed to understand the class imbalance and hence we plot a histogramrepresenting number of samples for each class.   We also plot the frequency vs duration histogram plot to understand the generaldistribution of audio durations in the dataset.  Based on the mean duration of5. 26 seconds and the peaks in the histogram, we decided to threshold our audiosto 6. 5 seconds.  Anything less than that will be padded to 6. 5 seconds andanything greater than that duration will be truncated to 6. 5 seconds. Even though we have sub-classes for audio-bad spanning different areas ofwhat “bad” could be, we decided to focus only on the noisy audios. SilentAudios can be treated separately, since they do not actually require somethingas complex as an ML model to classify them. We can simply use the age-oldpowerful signal processing methods to filter those out with some goodconfidence. Model: If we are going to reject these bad audios then we need to do so with:  High Precision: We should not be rejecting good audios which areperfectly interpretable and understandable.  Low Latency: This system should have little to no latency, otherwise itwill just slow down our whole VASR flow after being deployed and integrated.  Online: The model should be capable of running in an online setting wherecontinuous chunks of audios are fed into the system. We used a standard audio classification pipeline to train our binaryclassification task. This involves generating log-mel spectrograms and then running a ConvolutionNeural Network (CNN) based feature extractor on top of this fixed sizespectrogram image.   Binary Audio Classification based on Log-Mel SpectrogramsWhile these features (spectrograms) can be generated once after processing allthe audios in the dataset, this feature generation needs to be done on the flyto make a model that can be used in deployment i. e given raw audios as input,it should be able to predict the class, that was easily incorporated through afew transforms done within the model using torch-audio. Even though this architecture is simple, it got us an accuracy of about87%. But it is not the accuracy we need to see, our choice of metric tomeasure the performance is precision as explained earlier. We are still inthe process improving these initial baseline numbers of the model. One simpleapproach for increasing the precision is to increase the threshold, trading-offsome coverage in the form of support. Misclassification Analysis: We also do a post prediction analysis on the misclassified audios, whichrevealed an interesting pattern in the dataset and in the kind of audios thatthe model was finding hard to make predictions on. Briefly these errors followed 3 major types which now helps to understand theplaces where we can make improvements.  Type 1 (Very Short Utterance) : say 0. 2 seconds in audio of 6 seconds. Due to noise in most part of such short utterance audios, our model predictsit to be noisy and not good in some occasions. This can probably be fixedwith VAD which can trim the non speech segments in such short utteranceaudios.  Type 2 (Long audios) : Audio duration is longer than 6. 5 seconds with thespeaker in latter half. Since we chose to threshold our features (log-mels)at 6. 5 seconds, the latter part of the audio is basically truncated and hencesuch errors.  Type 3 (Ambiguous / Wrongly Labeled) : There are samples in the datasetwhich are not perfectly labelled. One may say these audios are debatablem,some may find them to be bad others may think that they are ok. This type oflabel noise is something which needs to be tackled. Needless to say, there are places where we can improve these results, buthaving a solid baseline model initially is important for incrementalimprovements over time and after a few iterations we finally see these modelsin our production systems. That’s all for now. Stay tuned to our rss feed for updates and more. "
    }, {
    "id": 64,
    "url": "/speaker-diarization/",
    "title": "Speaker Diarization",
    "body": "2020/07/21 -  This blog post is based on the work done by AnirudhDagar as an intern at Skit. ai Diarization - Who spoke when?Speaker diarisation (or diarization) is the process of partitioning an input audio stream into homogeneous segments according to the speaker identity.  Speaker diarization is the process of recognizing “who spoke when. ” In an audio conversation with multiple speakers (phone calls, conference calls, dialogs etc. ), the Diarization API identifies the speaker at precisely the time they spoke during the conversation. Below is an example audio from calls recorded at a customer care center, where the agent is involved in a one-to-one dialog with the customer. This can be particularly hard sometimes as we’ll discuss later in the blog. Just to give an example, this audio below seems to have a lot of background talking and noise making it difficult even for a human to accurately understand speaker timestamps.     Below we have an example of an audio along with its transcription and speech timestamp tags.     Transcription: “Barbeque nation mei naa wo book kia tha ok table book kia tha han toh abhi na uske baad ek phone aaya tha toh wo barbeque nation se hi phone aaya tha mai receive nhi kar paaya toh yehi” Diarization Tag: AGENT: [(0, 5. 376), (8. 991, 12. 213)], CUSTOMER: [(6. 951, 7. 554)] What Diarization is NOT ?: There is a fine line between speaker diarization and other related speech processing tasks.    Diarization != Speaker Change Detection : Diarization systems spit a label, whenever a new speaker appears and if the same speaker comes again, it provides the same label. However, in speaker change detection no such labels are given, only the boundary of change is considered for prediction.     Diarization != Speaker Identification : The goal is not to learn the voice prints of any known speaker. Speakers’ are not registered before running the model.  Motivation  Fig 1. : ASR using Diarization tags to understand and segregate transcription. With the rise of speech recognition systems both in terms of scale and accuracy, the ability to process audio of multiple speakers is crucial and has become quintessential to understand speech today. As illustrated in Fig 1. above, information gained through diarization helps in enriching and improving Spoken Language Understanding (SLU) based on the Automatic Speech Recognition (ASR) transcription. It can enhance the readability of the transcription by structuring the audio stream into speaker turns and, when used together with speaker recognition systems, by providing the speaker’s true identity. This can be valuable for downstream applications such as analytics for call-center transcription and meeting transcription etc. Other than this, we at Skit. ai work on Call Center Automation (CCA) among many other speech domains, and, at the very core this is powered by our products VIVA and VASR. Information gained through diarization can be used to strengthen the VASR engine. How? Let me explain the goal of every customer care call support service, if you are not already aware of. The ultimate aim is to provide best in class service to the customers of the respective company. Quantitatively, measure of the service quality is based on the assessment of the AGENT’s (Call representative at customer care center) ability to disseminate relevant information to the CUSTOMER. During the quality check phase, an AGENT’s performance is scored on mutiple parameters, such as (but not limited to):  Whether the agent was patient enough listening to the customer or was rushing on the call Whether she/he was rude to the lead at any time or not Whether she/he used the proper language to communicate. For such occasions, identifying different speakers in a call (“AGENT” or “CUSTOMER”) and finally connecting different sentences under the same speaker is a critical task for the assessment quality. Speaker Diarization is the solution for those problems. Other applications involve:  Information retrieval from broadcast news.  Generating notes/minutes of meetings.  Turn-taking analysis of telephone conversations.  Call center Data analysis Court houses &amp; Parliaments.  Broadcast News(TV and Radio)DIHARD?: This is not 2x2=4. This is Diarization and IT IS HARD. One can say that it is one of the toughest ML problems intrinsically high on complexity, even for a human-being, in certain conditions. But Why??? Real world audios are not always sunshine and rainbows. They come with infinite complexities. To name a few:    In most of the conversations, people will interrupt each other, overtalk etc. , and, cutting the audio between sentences won’t be a trivial task due to this highly interactive nature.     Speakers are discovered dynamically. Although as you’ll see later, in our case we only have 2 speakers, a fixed number.   Sometimes the audio is noisy:     People talking in the background.    The microphone picking up speakers’ environment noises (roadside noises, industrial machinery noise, music in the background etc. ).     For telephony based audios, connection may be weak at times, leading to:     Audio being dropped/corrupted in some parts.    Static or just some buzzing noise creeping in the conversation and finding it’s way into the audio recording.    Believe me, this is not the end of many problems for diarization!  Maybe in a conference call with multiple speakers, even if the audio is clear, the difference can be very subtle between the speakers, and it is not always possible to identify/label the correct speaker for a particular timestamp/duration. Ok, so that’s it? If I have not made my point clear about the complexity of the problem, yet, then I’ll express my message through this legendary meme.   Fig 2. : Diarization is hard!More Problems?    All the problems stated above are considering that preprocessing steps like VAD/SAD worked perfectly, which you may have guessed, are obviously not 100% accurate.   What is this “preprocessing step”?   Voice Activity Detection (VAD) or Speech Activity Detection (SAD) is a widely used audio preprocessing technique, before running a typical diariaztion api based on the clustering of speaker embeddings. The objective of VAD/SAD is to get rid of all non-speech regions.  Approaching the problemKeeping in mind the complexity and hardness of the problem, multiple approaches have been devised over the years to tackle diarization. Some earlier approaches were based on Hidden Markov Models (HMMs)/ Gaussian Mixture Models (GMMs). More recently, Neural Embedding (x-vectors/d-vectors) + Clustering and End2End Neural methods have demonstrated their power. As stated in this paper by Fujita et al. , a x-vector/d-vector clustering-based system is commonly used for speaker diarization and most of our experiments are based around this approach.   Fig 3. : The image shows the cluster generated based on the speech pattern and precise time the speaker participated in the conversation. The aim of speaker clustering is to put together all the segments that belong to the same acoustic source in a recording. These don’t utilize any prior information of the speaker ID or the number of speakers in the recording. We’ll be covering a typical embedding-clustering based approach in detail in the latter sections of the blog. However, speaker diarization systems which combine the two tasks in a unified framework are gaining popularity in recent times. Fig 4 visually summarizes the End2End idea. Due to the increased amounts of data being avaialable, joint End2End modeling methods are slowly taking over older approaches across ML domains, alleviating the complex preparation processes involved earlier. One example is EEND: End2End Neural Diarization by Fujita et al. proposed recently showing some promise in regards to solving these complex steps jointly.   Fig 4. : An End to End approach diarization system. Defining Our Problem: We need to answer the question “What should be a robust diarization system?” before moving forward. We decided to try out multiple approaches and model experiments explained in the sections below. Our model should be powerful enough to capture global speaker characteristics in addition to local speech activity dynamics. A systems, that is able to accurately handle highly interactive and overlapping speech specifically in the telephony call audios conversational domain, while being resilient to variation in mobile microphones, recording environment, reverberation, ambient noise, speaker demographics. Since we have a more focused problem, for evaluating customer care center call audios, the number of speakers for our use case is fixed at two i. e “AGENT” and “CUSTOMER” for each call, we need to tune our model for the same. Method:   Fig 5. : A typical diarization pipeline.    VAD — We employ WebRTC VAD to remove noise and non speech regions during our experiments. Raw audios are split into frames with specific duration (30 ms in our case). For each input frame, WebRTC generates output 1 or 0, where 1 denotes speech and 0 denotes nonspeech. An optional setting of WebRTC is the aggressive mode, an integer between 0 and 3. 0 is the least aggressive about filtering out nonspeech while 3 is the most aggressive. These VAD/SAD models have their own respective struggles with and set of problems.     Embedder — Resemblyzer allows you to derive a high-level representation of a voice through a deep learning model (referred to as the voice encoder). Given an audio file of speech, it creates a summary vector of size 256 (embedding) that summarizes the characteristics of the voice spoken.     Clustering — We cluster the segment wise embedding using a simple K-Means to produce diarization results and determine the number of speakers with each speakers time stamps.     Resegmentation - Finally, an optional supervised classification step may be applied to actually identity every speaker cluster in a supervised way.  Evaluation Metrics: To evaluate the performance or to estimate the influence of errors on thecomplete pipeline, we use the standard metrics implemented inpyannote-metrics. We also need to account for the fact that these time-stamps are manuallyannotated by data-annotators and hence cannot be precise at the audio samplelevel. It is a common practice in speaker diarization research to remove afixed collar around each speaker turn boundary from being evaluated using ourmetric of choice. A 0. 4sec collar would exclude 200ms before and after theboundary. Diarization error rate (DER) is the de facto standard metric for evaluating and comparing speaker diarization systems. It is measured as the fraction of time that is not attributed correctly to a speaker or non-speech. The DER is composed of the following three errors: False Alarm: It is the percentage of scored time that a hypothesized speaker is labelled as a non-speech in the reference. The false alarm error occurs mainly due to the the speech/non-speech detection error (i. e. , the speech/non-speech detection considers a non-speech segment as a speech segment). Hence, false alarm error is not related to segmentation and clustering errors. Missed Detection: It is the percentage of scored time that a hypothesized non-speech segment corresponds to a reference speaker segment. The missed speech occurs mainly due to the the speech/non-speech detection error (i. e. , the speech segment is considered as a non-speech segment). Hence, missed speech is not related to segmentation and clustering errors. Confusion: It is the percentage of scored time that a speaker ID is assigned to the wrong speaker. Confusion error is mainly a diarization system error (i. e. , it is not related to speech/non-speech detection. ) It also does not take into account the overlap speeches not detected. [\text{DER} = \frac{\text{false alarm} + \text{missed detection} + \text{confusion}}{\text{total}}] Resegmentation: As stated earlier, speaker diarization consists of automatically partitioning an input audio stream into homogeneous segments (segmentation) and assigning these segments to the same speaker (speaker clustering). Read more about segmentation here. Is it possible to resegment these assignments, post clustering, to achieve an improved lower DER? This is exactly the job of a resegmentation module. Simply put, we had a condition to be improved, a difficulty to be eliminated, and a troubling question that existed.  If yes, in what scenarios this optional (Resegmentation) module helps? Before going ahead with resegmentation, we needed meaningful understanding and deliberate investigation of the predictions to answer the above question. We brainstormed on this particular section involving post-processing of predictions in the diarization pipeline. After analysis and comparisons of predicted annotations made to the true annotations, our system, though good (with DER=0. 05), was possibly facing an issue. We were hit by something called oversegmentation, which can be seen in figure 6 below.   Fig 6. : Oversegmentation. Need for resegmentation?To fix this problem and in the process making our system more robust, we tried multiple experiments tweaking the resegmentation module.  Standard Smoothing: This is a simple annotation post processing smoothing function to solve the abrupt annotated segments (oversegmented regions) by merging these items smaller than or equal to the threshold (duration less than 0. 2 seconds) with neighbouring annotation. We instantly got a 1% boost in DER (0. 04) after smoothing. Our initial hunch about oversegmentation based on the analysis was now supported by results from this simple exercise. This made us think about a few more questions:  Can we do even better? Can we use the labels and supervise the resegmentation module learning this smoothing function at the very least? Are there some other errors in the predictions which we may have overlooked, but probably a learned resegmentation model captures? Can we leverage clustering confidences to improve the resegmentation module?Aiming to answer the above questions, we came up with a Supervised Resegmentation Sequence2Sequence model.    Supervised Seq2Seq Resegmentation: The goal of the model as shown below is to learn a mapping from the initial predictions to the ground truth sequence based on supervised training.    Resegmentation Goal       PREDICTIONS             GROUND TRUTH [A, A, A, A, B, A, A, B, B, B . . . . ] -&gt; [A, A, A, A, A, A, A, B, B, B . . . . ] where A : Speaker 1    B : Speaker 2    each label represents fixed duration of 400ms in our annotations.     To achieve such a mapping we worked on a simple Seq2Seq LSTM based model. We also enriched this model with information of cluster confidences after tweaking our Embedding+Clustering pipeline to do a soft clustering, i. e. return cluster scores based on the distance of each point in the cluster from the centroid along with the clustered predictions.  Overall all the above steps regarding a supervised resegmentation model were completely experimental and based on a few ideas. We are yet to achieve convincing results based on this approach but I thought it would be nice to mention this cool experiment :). Providing more resegmentation sequences for training could definitely and we also try to tackle diarization with limited data. See here UIS-RNN: To explore more supervised methods, we also experimented with Fully Supervised Speaker Diarization or the UIS-RNN model, the current state of the art neural system for Speaker Diarization. Converting data to UIS Style format involves a set of preprocessing steps similar to what we had to for our supervised resegmentation model. More on the official UIS-RNN Repo. But a caveat with UIS-RNN is that it requires huge amounts of data to form a convincing hypothesis after training. On realizing the limited amount of tagged data we had, we worked on simulating datasets for Speaker Diarization which in itself comes with some challenges. Simulated Data Generation:   Fig 7. : Diarization Data SimulationWe started with a large number of dual channel audio calls as a requirement for generating this Speaker Diarization Dataset. These dual channel audios were then split and saved into mono channel audio files. The key idea is that each mono channel contains one speaker and if we are able to combine these mono channeled audios compunded with known timestamps of speakers, we can then possibly recreate the audios which are potentially useful for Supervised Speaker Diarization. Steps as shown in Fig 7:    Split dual channel audio calls into mono channel audios.   Running Webrtc VAD on the mono channels:     Aggressive : Speech Regions.    Mild : Invert to get Gap Regions    Compute statistics in real audios: This step is required for us to understand the dynamics of overlaps and silences in a call on avergae. We compute the following ratios:     \[\text{Silence Ratio} = \frac{\text{Duration of Silences}}{\text{Total Duration}}\]      \[\text{Overlap Ratio} = \frac{\text{Duration of Overlapping Utterances}}{\text{Total Duration}}\]       Combination of Speech from A and B with timestamps. At the same time we needed to add real Gaps/Silence fills and Overlaps (interrupts and overtalking) to mimic real world call audios which are highly interactive. To control the amount of overlap in data-generation, we used 2 parameters mainly.      scale_overlap : This allowed us to control the maximum possible duration of overlap and was set based on the stats gathered in step 3.    bias_overlap : This allowed us to control the percentage or probability if there is an overlapping segment. Eg: setting bias_overlap to 0. 75 will give 33% chance each time to add overlap.     Dump tagged speaker timestamps and simulated audios. That’s all for now, and we’ll end this blog here. Stay tuned to our rss feed for updates. Until next time, Signing Off! References:  Deep Self-Supervised Hierarchical Clustering for Speaker Diarization Joint Speech Recognition and Speaker Diarization via Sequence Transduction WebRTC VAD DIHARD II is Still Hard: Experimental Results and Discussionsfrom the DKU-LENOVO Team PyAnnote Audio Resemblyzer Awesome Diarization Robust Speaker Diarization for Meetings Generalized End-To-End Loss For Speaker Verification Fully Supervised Speaker Diarization"
    }, {
    "id": 65,
    "url": "/fast-microservices-with-grpc/",
    "title": "Building Fast and Efficient Microservices with gRPC",
    "body": "2020/02/05 - Skit. ai processes millions of speech recognition requests every day, and to handle such a load we have focused on building a highly scalable technical stack. Firstly, we realized the drawbacks of using HTTP/1. 1 keeping in mind one of our requirements to do end to end streaming recognition to reduce the overall latency of the system. So, we decided to migrate all our core services from REST APIs (HTTP/1. 1 + JSON) to gRPC (HTTP/2. 0 + Protobuf). In this article, I will take you through the benefits and challenges of adopting gRPC when compared to a REST-based communication style. What is Microservices Architecture?:  Defines an architecture that structures the application as a set of loosely coupled, collaborating services. Services communicate using either synchronous protocols such as HTTP/REST, gRPC or asynchronous protocols such as AMQP. Services can be developed and deployed independently of one another. What are the challenges of adopting Microservices Architecture?: While there are many strong benefits of moving to a microservices architecture — there are no silver bullets! This means there are tradeoffs to make! Microservices gives us a higher level of complexity, and there are several inherent challenges with microservices you need to take into account when changing from a monolithic architecture. Like,  Inter-service network communication Data serialization/de-serialization Security/Authentication Language Interoperability Streaming Data Monitoring DebugginggRPC gives us the tools and capabilities to combat these issues without having to roll your custom frameworks. What is gRPC?:  gRPC is a modern open-source high-performance RPC framework that can run in any environment. It can efficiently connect services in and across data centres with pluggable support for load balancing, tracing, health checking and authentication. It is also applicable in the last mile of distributed computing to connect devices, mobile applications and browsers to backend services. What are Protocol buffers?:  Protocol Buffers, another open-source project by Google, is a language-neutral, platform-neutral, extensible mechanism for quickly serializing structured data in a small binary packet. By default, gRPC used Protocol Buffers v3 for data serialisation. When working with Protocol Buffers, we write a . proto file to define the data structures that we will be using in our RPC calls. This also tells protobuf how to serialise this data when sending it over the wire. This results in small data packets being sent over the network, which keeps your RPC calls fast, as there are fewer data to transmit. It also makes your code execute faster, as it spends less time serializing and deserializing the data that is being transmitted. Here you can see, we are defining a Person data structure, with a name, an id and multiple phone numbers of different types. syntax =  proto3 ;message Person {string name = 1;  int32 id = 2;  string email = 3;  enum PhoneType {   MOBILE = 0;   HOME = 1;   WORK = 2;  }  message PhoneNumber {   string number = 1;   PhoneType type = 2;  }  repeated PhoneNumber phone = 4;}Along with the data structures, we can also define RPC functions in the services section of our . proto file. There are several types of RPC calls available — as we can see in GetFeature, we can do the standard synchronous request/response model, but we can also more advanced types of RPC calls, such as with RouteChat, where we can send information via bi-directional streams in a completely asynchronous way. From these . proto files, we can use the gRPC tooling to generate both clients and server-side code that handles all the technical details of the RPC invocation, data serialisation, transmission and error handling. This means we can focus on the logic of our application rather than worry about these implementation details. service RouteGuide { rpc GetFeature(Point) returns (Feature); rpc RouteChat(stream RouteNote) returns (stream RouteNote);}message Point { int32 Latitude = 1; int32 Longitude = 2;}message Feature { string name = 1; Point location = 2;}message RouteNote { Point location = 1; string message = 2;}gRPC (HTTP/2. 0 + Protobuf) vs REST (HTTP/1. 1 + JSON): HTTP/1. x HTTP/0. 9 was a one-line protocol to bootstrap the World Wide Web HTTP/1. 0 documented the popular extensions to HTTP/0. 9 in an informational standard. HTTP/1. 1 introduced an official IETF standard. HTTP/1. x clients need to use multiple connections to achieve concurrency and reduce latency; HTTP/1. x does not compress request and response headers, causing unnecessary network traffic; HTTP/1. x does not allow effective resource prioritization, resulting in poor use of the underlying TCP connection; and so on. Advantages of HTTP/2. 0 The first advantage HTTP/2 gives you over HTTP/1. x is speed.  HTTP/2 reduces latency by enabling full request and response multiplexing, minimize protocol overhead via efficient compression of HTTP header fields, support for request prioritization, allows multiple concurrent exchanges on the same connection and server push.   HTTP/2 vs HTTP/1. 1 Multiplexing — HTTP/1. 1 vs HTTP/2. 0 HTTP/2 gives us multiplexing. Therefore, multiple gRPC calls can communicate over a single TCP/IP connection without the overhead of disconnecting and reconnecting over and over again as HTTP/1. 1 will do for each request. This removes a huge overhead from traditional HTTP forms of communication.   Bi-directional Streaming HTTP/2 also has bi-directional streaming built-in. This means gRPC can send asynchronous, non-blocking data up and down, such as the RPC example we saw earlier, without having to resort to much slower techniques like HTTP-long polling. A gRPC client can send data to the server and the gRPC server can send data to the client in a completely asynchronous manner. This means that doing real-time, streaming communication over a microservices architecture is exceptionally easy to implement in our applications Multi-language support One of the big advantages of microservices architecture is using different languages for different services. gRPC works across multiple languages and platforms. Currently, these languages/frameworks are supported,  Go Ruby PHP Java / Android C++ C# Objective-C / iOS Python Node. js DartThe best part is that gRPC is not just a library, but tooling as well. Once you have your . proto file defined, you can generate client and server stubs for all of these languages, allowing your RPC services to use a single API no matter what language they are written in! This means that you can choose the right tool for the job when building your microservices — you aren’t locked into just one language or platform. gRPC will generate clients and servers stubs that are canonically written for the language you want to use, and also take care of the serialisation and deserialisation of data in a way that your language of choice will understand! There’s no need to worry about transport protocols or how data should be sent over the wire. All this is simply handled for you, and you can focus instead on the logic of your services as you write them.  This means that you can choose the right tool for the job when building your microservices — you aren’t locked into just one language or platform. gRPC will generate clients and servers stubs that are canonically written for the language you want to use, and also take care of the serialisation and deserialisation of data in a way that your language of choice will understand! There’s no need to worry about transport protocols or how data should be sent over the wire. All this is simply handled for you, and you can focus instead on the logic of your services as you write them. What are the challenges in adopting gRPC?: Load Balancing Our microservices are containerized and deployed using Kubernetes, an open-source container orchestration system. Kubernetes’s default load balancing often doesn’t work out of the box with gRPC, we use Linkerd (Service Mesh) for gRPC load balancing with sidecar container injected to existing pods. More about gRPC load balancing with Kubernetes and other gRPC load balancing options Manual Testing Since gRPC is a binary protocol, the RPC methods cannot be manually tested using GUI tools like Postman (generally used for HTTP/1. 1 text-based protocol). We have used Evans REPL for gRPC introspection and manual testing. To conclude, I would highly recommend everyone to try out gRPC and start migrating to gRPC in production. If you are interested in working with cutting-edge technologies, come work with us. Apply here "
    }, {
    "id": 66,
    "url": "/repl-conversations/",
    "title": "A REPL for Conversations",
    "body": "2020/01/30 - A REPL, in programming, is an interactive environment where a programmercan go through the cycle of writing code, getting it Read,Evaluated, output Printed and then back in a Loop. Fundamentally,a REPL merges usage and extension of a system in a single interface. This merger has been important in providing easy extensibility to theend programmers in many programming languages. Interaction is supervisionWe can envision a similar merger in human intelligence involvinglanguages. After getting to a certain level of language understanding,our interactive behavior can be manipulated itself by interactions. Interactions cover all possible dialogs. Manipulative interactions orsupervision, as we like to call it, cover whatever is involved inupdating the state of the internal model which, in turn, affects thebehavior. Most of the machine dialog platforms aren't designed to expose thisequivalence explicitly. We have a different language for providingsupervision and different for building the interaction experiences andthat's it. Consider the chatbots you see on various websites thesedays. These have an interaction language which is a natural language,and a separate supervision language which stays at the programmer'sbeck in the background. I can't improve my personal interaction withthe system or extend it's behavior by talking to it. I can do that witha human agent easily. We definitely are moving towards such systems by adding control knobspiece by piece. But still aren't looking very actively for a generalframework of user initiated supervision. Practically this is not goingto be immediately helpful for many reasons known to all of us working inthe field, but it's helpful to define various kinds of platforms formachine dialog involving users on a continuum who all program theirdesired changes using their own interactive languages. From the technical side, of course there are problems that need to besolved for this. Since natural languages are already there, as comparedto a language to be designed, this needs a lot of work inunderstanding and creating computational models of languageacquisition fromone end and works in zero and few shot learnings from the other. Sublanguages are still going to be the more efficient way for working ata certain level, just like mathematical notations are for talking aboutmathematics, but we can target a natural language REPL layer on top ofcurrent dialog platforms and aim for a sort of interactive malleabilityfor growing the system organically afterwards. Growing a conversational agentThis malleability is not only a learning but also design problem at themoment1. A problem similar to designing a programming language. Whatwe want the language user to accomplish is our focus right now. How wecan let the language itself grow is a problem we will be designingsystems for in the future. Once this interaction language is capableenough, we can have an ecosystem of extensions and libraries making aminimal system malleable enough to allow building drastically differentexperiences on top of it. And it’s important that the system accepts growth at various levels if it has togeneralize across many use cases. In his talk ‘Growing a Language’, Guy L. Steele cites lack of this growth as the difference between APL and Lisp. In APL“there was no way for a user to grow the language in a smooth way”. Thesmoothness here focuses on extensibility in terms of language primitives. Fromthe same talk:  If I want to help other persons to write all sorts of programs, shouldI design a small programming language or a large one?  I stand on this claim: I should not design a small language, and Ishould not design a large one. I need to design a language that cangrow. I need to plan ways in which it might grow—but I need, too, toleave some choices so that other persons can make those choices at alater time. Conversational systems are interesting from many more angles than justwhat their components look like at the moment, which includes Speech ToText, Language Understanding, Dialog Management, Text To Speech, etc. Improving metrics under the current framework is a decent and importantgoal, but there are many interesting challenges in the design aspects ofhow we build and let others build conversational experiences themselves.       Many researches in self learning systems are orthogonal here asyou can have low generalization and still make a natural languageREPL.  &#8617;    "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        <div class="container">
<div class="row justify-content-center">
    <div class="col-md-8">
        <div class="row align-items-center mb-5">
          <div class="col-md-9">
            <h2 class="font-weight-bold">Shashank Shailabh</h2>
            <p class="excerpt"></p>
            <div class="icon-block mt-3 d-flex justify-content-between">
            <div>
              
              
              <a target="_blank" href="https://github.com/sshailabh"><i class="fab fa-github text-muted" aria-hidden="true"></i></a> &nbsp;
              
              
              
            </div>
            </div>
          </div>
          <div class="col-md-3 text-right">
            <img alt="Shashank Shailabh" src="/assets/images/authors/avatar-person.svg" class="rounded-circle" height="100" width="100">
          </div>
        </div>
        <h4 class="font-weight-bold spanborder"><span>Posts</span></h4>
        
        <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/Code-Mixing-Seminar/">Seminar - Code Mixing in NLP and Speech</a>
	</h2>
	<p class="excerpt">
    Below are some pointers and insights from the papers that we covered in the recently concluded seminar on Code-mixing in NLP and Spee...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#machine learning">Machine Learning</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Aug 24, 2021
	</small>
</div>
</div>

        
    </div>
</div>
</div>

    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/geopattern/1.2.3/js/geopattern.min.js"></script>

    <script src="/assets/js/theme.js"></script>

    

    

    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
              <span><span class="text-dark font-weight-bold">skit</span> © <script>document.write(new Date().getFullYear())</script></span>

                <!--  Github Repo Star Btn-->
                <a class="text-dark ml-1" target="_blank" href="https://github.com/skit-ai/"><i class="fab fa-github"></i> Github</a>
                <a class="text-dark ml-1" target="_blank" href="https://twitter.com/SkitTech"><i class="fab fa-twitter"></i> Twitter</a>

            </div>
            <div>
                Made with <a target="_blank" class="text-dark font-weight-bold" href="https://www.wowthemes.net/mundana-jekyll-theme/"> Mundana Jekyll Theme </a> by <a class="text-dark" target="_blank" href="https://www.wowthemes.net">WowThemes</a>
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
