<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Privacy Policy | Vernacular.ai Tech</title>

    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Privacy Policy | Vernacular.ai Tech</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Privacy Policy" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Speech Technology from Vernacular.ai" />
<meta property="og:description" content="Speech Technology from Vernacular.ai" />
<meta property="og:site_name" content="Vernacular.ai Tech" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Privacy Policy" />
<script type="application/ld+json">
{"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/images/logo.png"}},"description":"Speech Technology from Vernacular.ai","headline":"Privacy Policy","url":"/privacy-policy.html","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">
    <link rel="stylesheet" href="/assets/css/patch.css">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
      src="https://code.jquery.com/jquery-3.3.1.min.js"
      integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
      crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WRCLS27Y94"></script>
<script>
 window.dataLayer = window.dataLayer || [];
 function gtag(){dataLayer.push(arguments);}
 gtag('js', new Date());

 gtag('config', 'G-WRCLS27Y94');
</script>



</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>Vernacular.ai</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
  <a class="nav-link" href="/ml.html">ML</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="/engineering.html">Engineering</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="/resources.html">Resources</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="/authors-list.html">Authors</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="/about.html">About</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="/feed.xml">
    <i class="fa fa-rss text-danger"></i>
  </a>
</li>

            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "/about.html",
    "title": "About",
    "body": "      Vernacular. ai is an AI-First SaaS business enhancing customer experience through intelligent voice conversations.        This is the homepage and blog of the technology teams powering our speech   conversational systems. We work on conversations from all angles, signals, symbols, and   systems.          Follow us:        Github           Gitlab           @VernacularTech           @VernacularAI           Linkedin      "
    }, {
    "id": 2,
    "url": "/authors-list.html",
    "title": "Authors",
    "body": "                                    Anirudh Dagar :       View Posts                                            &nbsp;                                                                              Kriti Anandan :       View Posts                                            &nbsp;                                                                              Mithun Arunan :       View Posts      Minimalist                               &nbsp;                      &nbsp;                      &nbsp;                      &nbsp;                                                                Swaraj Dalmia :       View Posts                                            &nbsp;                                                                              Kumarmanas Nethil :       View Posts                                            &nbsp;                                                                              Shantanu Sharma :       View Posts                                            &nbsp;                                                                              Abhinav Tushar :       View Posts                                     &nbsp;                      &nbsp;                             &nbsp;                                                                Prabhsimran Singh :       View Posts                                            &nbsp;                             &nbsp;                             "
    }, {
    "id": 3,
    "url": "/buy-me-a-coffee.html",
    "title": "Buy me a coffee",
    "body": "Hi! I am Sal, web designer &amp; developer at WowThemes. net. The free items I create are my side projects and Mundana for Jekyll is one of them. You can find all the work I release for free here. You have my permission to use the free items I develop in your personal, commercial or client projects. If you’d like to reward my work, I would be honored and I could dedicate more time maintaining the free projects. Thank you so much! Buy me a coffee "
    }, {
    "id": 4,
    "url": "/categories.html",
    "title": "Categories",
    "body": "          Categories               Machine Learning:                                  		Our new Tech blog	: 	  We are merging past webpages of our Engineering and ML team in this new, central, Vernacular Tech page. From here on, this is going t. . . 	 			In 				Engineering, 				Machine Learning, 								Feb 28, 2021						                                 		EMNLP 2020	: 	  Individual summary notes from EMNLP 2020. 	 			In 				Machine Learning, 								Dec 21, 2020						                                 		Normalizing Flows - Part 1	: 	  Normalizing flows, popularized by (Rezende, &amp; Mohamed, 2015), are techniques used in machine learning to transform simple probabi. . . 	 			In 				Machine Learning, 								Dec 19, 2020						                                 		Interspeech 2020	: 	  We recently attended the all remote Interspeech 2020. Each of us made notes on what they did overall. But instead of posting those or. . . 	 			In 				Machine Learning, 								Dec 01, 2020						                                 		Reading Sessions	: 	  Studying researches and building on top of them is an important part of what a team of ML Engineers do on a regular basis. Usually, t. . . 	 			In 				Machine Learning, 								Nov 30, 2020						                                 		Bad Audio Detection	: 	  This blog will be a short one, where we’ll talk about our approach on filtering out inscrutable audios from VASR. 	 			In 				Machine Learning, 								Jul 29, 2020						                                 		Speaker Diarization	: 	  This blog post is based on the work done by Anirudh Dagar as an intern at Vernacular. ai	 			In 				Machine Learning, 								Jul 21, 2020						                                 		A REPL for Conversations	: 	  A REPL, in programming, is an interactive environment where a programmer can go through the cycle of writing code, getting it Read, E. . . 	 			In 				Machine Learning, 								Jan 30, 2020						                              Engineering:                                  		Our new Tech blog	: 	  We are merging past webpages of our Engineering and ML team in this new, central, Vernacular Tech page. From here on, this is going t. . . 	 			In 				Engineering, 				Machine Learning, 								Feb 28, 2021						                                 		Building Fast and Efficient Microservices with gRPC	: 	  Vernacular. ai processes millions of speech recognition requests every day, and to handle such a load we have focused on building a hi. . . 	 			In 				Engineering, 								Feb 05, 2020						                                             Featured:    				                                          Interspeech 2020                          In                     Machine Learning,                                                                   "
    }, {
    "id": 5,
    "url": "/contact.html",
    "title": "Contact",
    "body": "  Please send your message to Vernacular. ai Tech. We will reply as soon as possible!   "
    }, {
    "id": 6,
    "url": "/engineering.html",
    "title": "Engineering",
    "body": "        This is the home page of the Engineering team at Vernacular. ai.            Team:    Current members of our Engineering team:               Abhishek Sachdeva (2019-)                Advait Raykar (2020-)                Aditya Satyavada (2018-)                Akarsh T S (2020-)                Akash Sridhar (2020-)                Akshay Deshraj (2016-)                Ankit Ranjan (2020-)                Anurag Shrivastava (2018-)                Charvee Punia (2021-)                Debmalya Paine (2020-)                Deepankar Agrawal (2017-)                Dimple Mathew (2021-)                Hrushikesh Kolekar (2020-)                Jobin Varghese (2020-)                Krishna Khatri (2020-)                Mithun Arunan (2018-)                Mukunth Krishnasagar (2020-)                Narayan Ubale (2020-)                Sanath Bhargav (2020-)                Saumya Bakshi (2020-)                Shivanshu (2021-)                Subhrajit Prusty (2020-)                Sunil Vijay (2020-)                Vipul Sharma (2018-)                Yash Dev Lamba (2020-)                                                      Past members:                                                                                                                                                                                                                       Shantanu Sharma (Summer Intern, 2020)                Vaibhav Mishra (Summer Intern, 2020)                Amartya Gaur (Summer Intern, 2020)                Kapil Karekar (2019-2020)                Rajdeep Kaur (2017-2020)                    Recent posts:                              Our new Tech blog            		     Feb 28, 2021	                                        Building Fast and Efficient Microservices with gRPC            		     Feb 05, 2020	                            All posts under Engineering      "
    }, {
    "id": 7,
    "url": "/",
    "title": "Home",
    "body": "                    Our new Tech blog  :      We are merging past webpages of our Engineering and ML team in this new, central, Vernacular Tech page. From here on, this is going t. . .               In                 Engineering,                 Machine Learning,                               Feb 28, 2021                                                                                                       EMNLP 2020          :                       In                         Machine Learning,                                                         Dec 21, 2020                                                                                                                 Normalizing Flows - Part 1          :                       In                         Machine Learning,                                                         Dec 19, 2020                                                                                                                Interspeech 2020          :                       In                         Machine Learning,                                                         Dec 01, 2020                                              Our new Tech blog                 We are merging past webpages of our Engineering and ML team in this new, central, Vernacular Tech page. From here on, this is going t. . .                 Read More                               All Stories:           		Our new Tech blog	: 	  We are merging past webpages of our Engineering and ML team in this new, central, Vernacular Tech page. From here on, this is going t. . . 	 			In 				Engineering, 				Machine Learning, 								Feb 28, 2021						          		EMNLP 2020	: 	  Individual summary notes from EMNLP 2020. 	 			In 				Machine Learning, 								Dec 21, 2020						          		Normalizing Flows - Part 1	: 	  Normalizing flows, popularized by (Rezende, &amp; Mohamed, 2015), are techniques used in machine learning to transform simple probabi. . . 	 			In 				Machine Learning, 								Dec 19, 2020						          		Interspeech 2020	: 	  We recently attended the all remote Interspeech 2020. Each of us made notes on what they did overall. But instead of posting those or. . . 	 			In 				Machine Learning, 								Dec 01, 2020						          		Reading Sessions	: 	  Studying researches and building on top of them is an important part of what a team of ML Engineers do on a regular basis. Usually, t. . . 	 			In 				Machine Learning, 								Nov 30, 2020						          		Bad Audio Detection	: 	  This blog will be a short one, where we’ll talk about our approach on filtering out inscrutable audios from VASR. 	 			In 				Machine Learning, 								Jul 29, 2020						          		Speaker Diarization	: 	  This blog post is based on the work done by Anirudh Dagar as an intern at Vernacular. ai	 			In 				Machine Learning, 								Jul 21, 2020						          		Building Fast and Efficient Microservices with gRPC	: 	  Vernacular. ai processes millions of speech recognition requests every day, and to handle such a load we have focused on building a hi. . . 	 			In 				Engineering, 								Feb 05, 2020						          		A REPL for Conversations	: 	  A REPL, in programming, is an interactive environment where a programmer can go through the cycle of writing code, getting it Read, E. . . 	 			In 				Machine Learning, 								Jan 30, 2020						                                 Featured:    				                                          Interspeech 2020                          In                     Machine Learning,                                                           "
    }, {
    "id": 8,
    "url": "/ml.html",
    "title": "Machine Learning",
    "body": "        This is the home page of Machine Learning team at Vernacular. ai. We   work on Speech Conversational Systems. On this website, we keep notes and   artifacts from our work.        In case of queries, reach out to ai@vernacular. ai.            Team:    Current members of our ML team:               Aayush Deshraj (2019-)                Abhinav Tushar (2018-)                Amresh Venugopal (2019-)                Ayush Shridhar (2020-)                Himansu Didwania (2020-)                Jaivarsan B (2020-)                Karthik Ganesan (2020-)                Kaustav Tamuly (2020-)                Kriti Anandan (2020-)                Kumarmanas Nethil (2019-)                Lohith Anandan (2020-)                Pakhi Bamdev (2020-)                Prabhsimran Singh (2019-)                Pushkal Katara (2021-)                Sachin Kumar (2020-)                Swaraj Dalmia (2020-)                Utsav Shukla (2021-)                                                              Past members:                                                                                                                                                       Akhilesh KR (2017-2019)                Sujay S Kumar (2018-2019)                Kartikey Pandey (Winter Intern, 2019)                Amit Manchanda (2018-2020)                Anirudh Dagar (Summer Intern, 2020)                Mehul Ramaswami (Summer Intern, 2020)                    Recent posts:                               Our new Tech blog            		     Feb 28, 2021	                                        EMNLP 2020            		     Dec 21, 2020	                                        Normalizing Flows - Part 1            		     Dec 19, 2020	                                        Interspeech 2020            		     Dec 01, 2020	                                        Reading Sessions            		     Nov 30, 2020	                            All posts under ML      "
    }, {
    "id": 9,
    "url": "/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "“Vernacular. ai Tech” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 10,
    "url": "/resources.html",
    "title": "Resources",
    "body": "ProjectsWIP DatasetsWIP "
    }, {
    "id": 11,
    "url": "/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 12,
    "url": "/authors/anirudhdagar/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 13,
    "url": "/authors/kritianandan/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 14,
    "url": "/authors/mithun/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 15,
    "url": "/authors/swarajdalmia/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 16,
    "url": "/authors/janaab11/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 17,
    "url": "/authors/shantanu28sharma/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 18,
    "url": "/authors/lepisma/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 19,
    "url": "/authors/prabhsimran/",
    "title": "",
    "body": "                 {{ page. author. info. name }}:       {{ page. author. info. bio }}                    {% if page. author. info. website %}        &nbsp;       {% endif %}       {% if page. author. info. github %}        &nbsp;       {% endif %}       {% if page. author. info. twitter %}        &nbsp;       {% endif %}       {% if page. author. info. linkedin %}        &nbsp;       {% endif %}                                         Posts:     {% for post in page. author. posts %}    {% include main-loop-card. html %}    {% endfor %}  "
    }, {
    "id": 20,
    "url": "/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 21,
    "url": "/new-blog/",
    "title": "Our new Tech blog",
    "body": "2021/02/28 - We are merging past webpages of ourEngineering andML team in this new, central, VernacularTech page. From here on, this is going to be the main source of updates onspeech technology work from Vernacular. ai. Stay tuned to our rss feed for updates. "
    }, {
    "id": 22,
    "url": "/emnlp/",
    "title": "EMNLP 2020",
    "body": "2020/12/21 - Individual summary notes from EMNLP 2020. Amresh says  I have recently been following the branch of model explainability for practicalpurposes. I was delighted to know EMNLP was experiencing a surge of similarresearch. I encountered Minumum Description Length (MDL) as a measure tounderstand a model’s ability to capture linguistic properties. The core ideabeing changing the probing task of identifying labels to transmitting data. Karthik says  The Dialogue and Interactive systems track in EMNLP was very interesting andintroduced novel techniques and approaches to some of the ML problems that I amcurrently working on for example, MAD-X: a framework for effective transferlearning to new languages using adapter base architecture , TOD-BERT:Pre-training Transformer LMs on open-source dialogue data-sets for betterfew-short learning capability this could be helpful to mitigate cold-startproblem of SLU module. Also the generative approach based papers for dialoguestate tracking were an interesting approach that might have few-shot learningcapability as compared to discriminative ones. Lohith says  “Productising” ML Models has become one of the primary answers for most of thecomplex problem we face in day to day life, and explaining how they work, hasbecome a bigger concern. I was happy to see a lot of papers and demos in thatdirection, like the LIT tool, which helps both the developer and the user tounderstand the underlying workings of an ML model and how it looks at each inputand how they help in the decision making process. I was glad to see a lot ofpapers working with various NLP techniques for better and efficient Health Carefor people, which we can all say is more important than ever, given what we haveseen this year. Manas says  The Insights from Negative Results in NLP, Workshop was a highlight for me. Itrevolved around ideas of what an interesting question is and how to connectideas that didn’t work out. These questions feel central to our research effortsin-team, and the keynotes offered a rewarding and scientific route to exploringopen problems in speech tech. Swaraj says  The papers i liked from this year’s EMNLP were : “Incremental Processing in theAge of Non-Incremental Encoders” by Madureira, B. , &amp; Schlangen, D (2020), whichwas similar to how humans incrementally process natural language and provided away to probe the workings of transformer encoders; and “Digital Voicing ofSilent Speech” by Gaddy, D. , &amp; Klein, D. (2020) which was presented very welland i felt they did a good job modelling the EMG features and got goodimprovements on silent speech. There were a lot of papers and workshops oninterpretability of models which is essential considering the direction wherethe field is heading and the gather sessions did a good job of ensuring onedoesn’t miss out on the social aspects of attending a conference. "
    }, {
    "id": 23,
    "url": "/normalizing-flows/",
    "title": "Normalizing Flows - Part 1",
    "body": "2020/12/19 - Normalizing flows, popularized by (Rezende, &amp; Mohamed,2015), are techniques used in machinelearning to transform simple probability distribution functions intocomplicated ones. One of the popular use cases is in generative modelling - anunsupervised learning method - where the goal is to model a probabilitydistribution given samples drawn from that distribution. Why bother about normalizing flows?:    They have been used in many TTS (text-to-speech) models, memorably in theParallel WaveNetmodel (2017) where aclever application of normalizing flows resulted in a 1000 times fastergeneration of audio samples in comparison to the originalWaveNet model. The Parallel WaveNet modelwas also deployed on Google assistant for real-time generation of audio.     Normally a back-propagation pass requires the activation value for eachneuron to be stored in memory. This places a restriction on training deeper,wider models on single GPU’s(since GPU’s have limited memory) and forces oneto use small batch sizes during training. In flow based networks, one doesnot need to store the activations at all, as they can be reconstructedonlineduring the back-propagation. This property was leveraged in the RevNetspaper (2017) which uses invertibleresidual blocks. Reducing the memory cost of storing activationssignificantly improve the ability to efficiently train wider and deepernetworks.     Flowtron (2020), an autoregressiveflow based TTS model does a kind of representation learning using normalizingflows to learn an invertible mapping from a data space to a latent spacewhich can be manipulated to control many aspects of speech synthesis (pitch,tone, speech rate, cadence, accent). Flowtron matches state-of-the-art TTSmodels in terms of speech quality and is able to transfer speechcharacteristics from a source speaker to a target speaker, making the targetspeaker sound more expressive.     If you’ve ever thought about reversible networks, Normalizing flows doprecisely that. Reversibility of flows also means that one can triviallyencode images into the latent space for editing. They also have coolmathematical applications, for example their use in Neural ODEsolvers (2019. ) which use continuousnormalizing flows.  Brief Introduction: Definition: A Normalizing Flow is a transformation of a simpleprobability distribution into a more complex distribution by a sequence ofinvertible and differentiable mappings.  Note: The above formalism is a simplification, for a more precisedefinition one can consult [5]. The formalism allows piecewise continuousfunctions to be used in the construction of the flow which the abovedefinition restricts. Normalizing since the transformed distribution needs to be normalized bythe change of variables formula (discussed below). Flow refers to theseries of invertible transformations which are composed with each other tocreate more complex invertible transformations. When applied as density estimators, some NFs provide a general way ofconstructing flexible probability distributions over continuous randomvariables starting from a simple probability distribution. By constraining thetransformations to be invertible, Flow-based models provide a tractable methodto calculate the exact likelihood for a wide variety of generative modelingproblems. Efficient inference and efficient synthesis: Autoregressive models, such asthe PixelCNN, are also reversible,however synthesis from such models is difficult to parallelize, and typicallyinefficient on parallel hardware. Flow-based generative models likeGlow (and RealNVP) are efficient toparallelize for both training and synthesis. Exact latent-variable inference: Within the class of exact likelihoodmodels, normalizing flows provide two key advantages: model flexibility andgeneration speed. Flows have been explored both to increase the flexibility ofthe variational posterior in the context of variational autoencoders (VAEs),and directly as a generative model.  With VAEs, one is able to infer onlyapproximately the value of the latent variables that correspond to a datapoint. GAN’s have no encoder at all to infer the latents. In flow based generativemodels, this can be done exactly without approximation. Not only does this leadto accurate inference, it also enables optimization of the exact log-likelihoodof the data, instead of a lower bound of it. Mathematical Framework: Let, \(z_0\) be a continuous random variable belonging to a simple probability distribution \(p_\theta(z_0)\) . Let it be a Gaussian with parameters \((\mu, \sigma) = (0,1)\). [z_0 \sim p_\theta (z_0) = N(z_0;0,1)] Normalizing flows transforms the simple distribution, into a desired output probability distribution with random variable \(x\), with a sequence of invertible transformations, \(f_i's\). [z_k = f_\theta (z_0) = f_k…f_2. f_1(z_0) \text{ s. t. each $f_i$ is invertible (bijective)}] The composition of all the individual flows is represented by \(f_\theta\). Since each \(f_i\) is bijective, so is \(f_\theta\). The new density \(p_\theta (z_k)\) is called a push forward of the initial density \(p_\theta(z_0)\) by the function \(f_\theta. \) An example of a transformation obtained by a normalizing flow is shown below, which transforms a base gaussian distribution into a target multi-modal distribution with the help of a bijective function.   Fig 1: The transformation of a base distribution into a target distribution using a bijective function f. The constrains of a distribution being a probability distribution is that \(\int p_\theta (z_0) =1\). However, this doesn’t hold after applying a bijective function (for intuition consider \(f_1 : z \rightarrow z^3\)). Change of Variables Formula: Consider the normalizing flow \(f_1 : Z_0 \rightarrow Z_1\).  If we want the probability distribution of the random variable \(z_1 \sim Z_1\), we need to consider the change of variables formula derived below. Consider the event \(z_0 \sim Z\), mapped to \(z_1 \sim Z_1\) s. t. \(f_1(z_0) = z_1\). Since, the mapping is bijective, the probabilities of the events are the same. Therefore, [p_\theta(z_1)\partial z_1 = p_\theta(z_0)\partial z_0] [p_\theta(z_1) = p_\theta(z_0)* \frac{\partial z_0}{\partial z_1}]       [p_\theta(z_1) = p_\theta(z_0)*   \frac{\partial z_0}{\partial z_1}   ]   (since probabilities are always &gt; 0)       [p_\theta(z_1) = p_\theta(z_0)*   \frac{\partial z_0}{\partial f_1(z_0)}   ]   (\(z_1 = f_1(z_0)\))       [p_\theta(z_1) = p_\theta(z_0)*   \frac{\partial f_1(z_0)}{\partial z_0}   ^{-1}]   In the multivariate case (\(R^D \rightarrow R^D\)) this generalises to:       [p_\theta(z_1) = p_\theta(z_0)*   \det(\frac{\partial f_1(z_0)}{\partial z_0})   ^{-1}]   Considering the sequence of compositional transformations \(f_i's\), one obtains:       [p_\theta(z_k) = p_\theta(z_0)* \prod_{i=1. . k}   \det(\frac{\partial f_i}{\partial z_{i-1}})   ^{-1}]   The term on the right i. e. determinant of the Jacobian accounts for the changeof \(\delta\) volume induced by the transformation. It serves to normalize thetransformed distribution locally, after each flow through a transformation,hence the name, Normalizing Flows. Some Questions at this stage:  The sequence of flows need to be invertible and differentiable. What sort ofconstraints does it introduce in terms of the output distributions that canbe reached? Are there families of distributions that we can’t reach startingfrom a Gaussian?Sampling: In this case, the bijective functions and the initial distribution are given. Sampling points from the output distribution requires calculating the forward pass, i. e. an efficient calculation of the functions \(f_i's\). Density Estimation using Maximum Likelihood: In this case a dataset \(\{a_1, a_2,. . . . ,a_n\}\) is provided and the objective is to learn the probability density function \(p_\theta(A)\) to which the points belong. An initial density \(p_\theta(z_0)\) is chosen. For each \(a_i\) we have:       [p_\theta(a_i) = p_\theta(z_i)*   \det(\frac{\partial f(z_i)}{\partial z_i})   ^{-1}]   where, \(a_i = f(z_i)\)       [\prod_{i=1}^n p_\theta(a_i) =\prod_{i=1}^n [p_\theta(z_i)*   \det(\frac{\partial f(z_i)}{\partial z_i})   ^{-1}]]   We need to maximize the above over all possible flows \(f\) to find the flow \(\hat{f}\) that maximizes the probability.       [\hat{f} =\text{arg max}f \text{ }\prod{i=1}^n [p_\theta(z_i)*   \det(\frac{\partial f(z_i)}{\partial z_i})   ^{-1}]]   Using log likelihood maximization we arrive at:       [\hat{f} =\text{arg max}f \text{ }\sum{i=1}^n [log(p_\theta(z_i)) - log(   \det(\frac{\partial f(z_i)}{\partial z_i})   )]]   The equation shown above is used during training for density estimation. However, since we only know \(a_i's\) the only way to find \(z_i's\) which are used, is to find the inverse mapping i. e. \(z_i = f^{-1}(a_i)\) .  So for density estimation and training, the calculation of both the inverse and determinant of the Jacobian are required. However, calculating the inverse and the determinant of the Jacobian of a sequence of high dimensional transformations can be very time consuming (for dimensionality \(d\) matrix, both are of complexity \(O(d^3)\)). There are various tricks that are used to reduce the complexity of these two operations, one of the popular ones being the use of triangular maps. Triangular maps:: Let \(T\) be a normalizing flow, \(T: z \rightarrow x\) where \(x = (x_1, x_2 . . . . x_d)\) and \(z = (z_1, z_2 . . . . z_d)\). More generally, one can decompose \(T\) into \(T_1, T_2. . . T_d\) such that \(x_i = T_i(z_1,z_2. . . . . z_d)\). Now if we want to introduce an additional constraint on \(T\) i. e. for \(T\) to be a triangular map, each \(T_j\) should be a function of \((z_1,z_2. . . . . z_j)\) i. e. the first \(j\) elements and not all the \(d\) elements. For triangular maps/matrices, both the inverse and the determinant of the jacobian is easy to compute. The jacobian for a triangular map is shown below. The determinant is simply the product of the diagonals and has a complexity of \(O(d)\) instead of \(O(d^3)\). The complexity for the calculation of the inverse is \(O(d^2)\) instead of \(O(d^3)\).   Fig 2: The jacobian for a triangular map. This is taken from here. Note: For an increasing triangular map, \(\frac {\partial T_i}{\partial z_i} &gt; 0\). This will be useful in Part - 2 where the different types/families of Normalizing flows will be considered. References:  Normalizing Flows: An Introduction and Review of Current Methods Flow-based Deep Generative Models Lecture on NFs by Priyank Jaini"
    }, {
    "id": 24,
    "url": "/interspeech/",
    "title": "Interspeech 2020",
    "body": "2020/12/01 - We recently attended the all remote Interspeech2020. Each of us made notes on what they didoverall. But instead of posting those or going with awhat-we-think-about-the-conference style post, we thought to just ask the teamwhat interested them the most in a few sentences. Here are the individualresponses: Abhinav says  I really liked the ZeroSpeech challenge. As usual,they had few really interesting unsupervised problems and solutions. I findtheir tracks really ambitious as evident by this statement on their website“… infants learn to speak their native language, spontaneously, from rawsensory input, without supervision from text or linguists. It should bepossible to do the same in machines”.  Next year, 2021, the target is Spoken LanguageModeling. Looking forward to that too. Amresh says  The Meta Learning Tutorial on day 1 was a detailed session on the topic. Thepromise of performing well on a set of task(s) with less amount of data had myattention. The authors take care of introduction, utility and comparison ofthis approach and its impact on tasks like speaker verification, keywordspotting, Emotion Recognition and my special interest conversational AI. Manas says  A new thing here was Computational Paralinguistics, that covers thenon-content parts of speech. Given my interest in the stylistic parts ofspeech, this was particularly interesting. Papers presented many ideasrelevant to building a better voicebot, like - uncertainty aware methods formultiple labels, Autism Quotient as a perception feature, and predicting CSATscores from sentiment. Prabhsimran says  Interspeech had some great sessions, from discussions on more fundamentalconcepts related to Speech Processing in the Brain, Phonetics and Phonology tonovel ideas on Training Strategies for ASR like Semantic Word Masking,Efficient Vocoder implementations for faster Neural Waveform Synthesis andAutomatic Prosody Analysis for Non-Semantic Speech Representations. It helpedme connect alot of dots and exposed me to some great ideas we should beexploring in our work. Kaustav says  I really liked attending the Speech Emotion Recognition tracks. The trackscovered a multitude of topics including self-supervised learning methods,non-semantic representations, etc. It was overall, a very balanced track witha lot of interaction among the attendees and the presenters. The speech signalrepresentation track was pretty fun too with some really interesting papers onvoice casting, universal non-semantic representations. "
    }, {
    "id": 25,
    "url": "/reading-sessions/",
    "title": "Reading Sessions",
    "body": "2020/11/30 - Studying researches and building on top of them is an important part of what ateam of ML Engineers do on a regular basis. Usually, teams do this by organizingperiodic, often weekly, paper reading sessions. Here is a snippet from aninternal work memo by Manas explaining how welook at these sessions:  Lets start with the basic motivation behind these sessions - we wantto read more papers. But beyond this individual goal, there is alsothe simpler driving force of enthusiasm - we read something we like,and we want to share it. It is the same instinct that drives us totalk about books we read, movies we watch, and podcasts we listen to. …  There are also secondary benefits, like knowledge transfer - bothspeaker and audience will understand the topic better after a goodpresentation - and discovering shared interests within a larger group,etc. But it’s not that easy to bring this in practice. Specially in a startup, whereprocesses and structures are constantly in flux. As the team size keeps growing,different kinds of diversities start interfering. Diversities in interests,reading styles, and even bandwidth. This post covers how we organize reading sessions in the ML team atVernacular. ai. It might be helpful if you are tryingto do the same in your group. The very first thing that we did was to start asking people for researchpapers that they like, weekly. After voting on one, the proposer of thepaper presented it on a predetermined day. This lost steam away after awhile because of various reasons. One of them being bandwidth crunch foreveryone that time. We were just 3 people. We revived paper reading after a while. This time everyone picked andpresented a paper of their own liking. This wasn’t supposed to scale upwith team size, but we went with this for a decent while. While we werefree to choose and read whatever we wanted, lack of continuity inreadings, practical disconnects and difference in interests started toreduce the overall engagement. After lockdown, the engagement level dropped further. On video calls,you have to upgrade the quality of meetings if you want to maintain thesame level of participation. The missing modalities hurt significantly. We spent inordinate amount of time trying to get to one single view onhow to go about these sessions. We tried experimenting with variousaspects like paper selection, accountability, presentationaccessibility, etc. Rather than going in those experiments in chronological order, it makessense to think about the problems from two angles based on what we knownow. You can say that the sessions are having certain issues, oralternatively you can say that the people are having issues with thesessions. While both feed on each other and are cyclic, it helps to lookat them separately. Sessions: We can break down sessions temporarily in the following three acts. 1. Pre Session: Here it’s known that a certain session is supposed to happen. You cando the following in preparation:  Set clear expectations. What is supposed to be covered? How it’ssupposed to be covered? Who should come? What will be the outcome?etc.  Excite the potential audience. If the audience is not really awareof the topic, some amount of pre-work needs to be done to pull themin. 2. Session: During the session, you want to:  Make the presentation stimulating and engaging.  Keep the presentation accessible while not being superficial.  Develop practical connections between the audience and content. 3. Post Session: Since you want the next sessions to be successful too, you would liketo:  Make sure people are going away with a healthy amount of thoughtfood.  Nudge towards the utilitarian aspects of concepts discussed so thataudience have a few threads of experimentation to follow in their dayto day work. People: For people, we can think along the following lines1:  Resourcefulness.  Motivation.  Interests, their depths, and varieties.  Style and method of working with new knowledge.  Level of comfort with group sessions. Both for presentation anddiscussion.  Bandwidth. Specially considering industrial settings like ours.  Structural assistance and pushes. Acting on all these factors to deliver a single style of session thatworks for everyone is impossible. Not all factors might be important fora team at a given moment of time, but even a reasonably small set issufficiently diverse. The key idea is accept a pluralistic view on theissue. There is no single fundamentally correct way of doing thesesessions, and it’s better to pick a digestible subset and solve for that. Going ahead with this realization, we started doing seminars. Seminars: Reading Seminars are very similar to seminar courses in Universities. From another internal memo:  These [Seminars] exist to complement the world of paper readings. While Paper Reading sessions are about reading more papers andsharing what we like, Reading Seminars are about learning somethingspecific. These are much more structured and pointed towards a goal. The idea is to have deeper discussions, over longer periods of time,about topics that might interest you. Either directly or indirectly,this will lead to a better output (from the speaker) and experience(for the audience) in the Paper Reading sessions that follow. Seminars cover many of the issues nicely and they clearly don’t toucha few others. For example you can’t just bring in any new paper anddiscuss that in a session without setting up a seminar for that field. And that’s okay. There are other ways of handling that case. At the moment, we have the following parallel seminars running:  Multi-Style TTS End-to-End ASR Speech Representation Learning Dialog State Trackers Computational Paralinguistics Learning TheoryEach of these has a list of papers or topics to be discussed over a period of1-2 months. While not perfect, these are turning out to be decent readingroadmaps for these topics. Something we would like to open out after a couple ofmonths, similar to the old stylepaper reading sessions.       Many are derived from an internal note byManas &#8617;    "
    }, {
    "id": 26,
    "url": "/bad-audio-detection/",
    "title": "Bad Audio Detection",
    "body": "2020/07/29 - This blog will be a short one, where we’ll talk about our approach on filteringout inscrutable audios from VASR. There are situations in Call Center Automation (CCA) pipeline where userutterances are bad. Bad here is defined by things like noise, static,silences or background murmur etc. rendering the downstream SLU systemshelpless. We started with a proposal and prepare a dataset for making an MLsystem learn to reject these audios. Benefits:  No more misfires from SLU side which ultimately leads to a better userexperience.  Save compute and time by skipping bad audios.  The whole system can be used for all our audio based tasks to predict andfilter out the poor ones, hence avoiding sample noise for these tasks. Dataset: We prepared a dataset of intent tagged conversations with specially markedintents which tell us that these utterances are bad and them going further inSLU will result in errors. Also we have a sampling of non-bad utterances(tagged with regular intents) to make this a classification problem. There are total 9928 samples of audios labelled as bad and 20000 sampleslabeled as good. All the raw labels were not very useful, hence we clean and preprocess the datato finally create 2 broad categories with sub-classes.  audio-bad     audio-noisy: Noisy audio.    audio-silent: Silent audio.    audio-talking: Background talking.    hold-phone: Music from keeping on hold.     audio-goodExploratory Data Analysis: We needed to understand the class imbalance and hence we plot a histogramrepresenting number of samples for each class.   We also plot the frequency vs duration histogram plot to understand the generaldistribution of audio durations in the dataset.  Based on the mean duration of5. 26 seconds and the peaks in the histogram, we decided to threshold our audiosto 6. 5 seconds.  Anything less than that will be padded to 6. 5 seconds andanything greater than that duration will be truncated to 6. 5 seconds. Even though we have sub-classes for audio-bad spanning different areas ofwhat “bad” could be, we decided to focus only on the noisy audios. SilentAudios can be treated separately, since they do not actually require somethingas complex as an ML model to classify them. We can simply use the age-oldpowerful signal processing methods to filter those out with some goodconfidence. Model: If we are going to reject these bad audios then we need to do so with:  High Precision: We should not be rejecting good audios which areperfectly interpretable and understandable.  Low Latency: This system should have little to no latency, otherwise itwill just slow down our whole VASR flow after being deployed and integrated.  Online: The model should be capable of running in an online setting wherecontinuous chunks of audios are fed into the system. We used a standard audio classification pipeline to train our binaryclassification task. This involves generating log-mel spectrograms and then running a ConvolutionNeural Network (CNN) based feature extractor on top of this fixed sizespectrogram image.   Binary Audio Classification based on Log-Mel SpectrogramsWhile these features (spectrograms) can be generated once after processing allthe audios in the dataset, this feature generation needs to be done on the flyto make a model that can be used in deployment i. e given raw audios as input,it should be able to predict the class, that was easily incorporated through afew transforms done within the model using torch-audio. Even though this architecture is simple, it got us an accuracy of about87%. But it is not the accuracy we need to see, our choice of metric tomeasure the performance is precision as explained earlier. We are still inthe process improving these initial baseline numbers of the model. One simpleapproach for increasing the precision is to increase the threshold, trading-offsome coverage in the form of support. Misclassification Analysis: We also do a post prediction analysis on the misclassified audios, whichrevealed an interesting pattern in the dataset and in the kind of audios thatthe model was finding hard to make predictions on. Briefly these errors followed 3 major types which now helps to understand theplaces where we can make improvements.  Type 1 (Very Short Utterance) : say 0. 2 seconds in audio of 6 seconds. Due to noise in most part of such short utterance audios, our model predictsit to be noisy and not good in some occasions. This can probably be fixedwith VAD which can trim the non speech segments in such short utteranceaudios.  Type 2 (Long audios) : Audio duration is longer than 6. 5 seconds with thespeaker in latter half. Since we chose to threshold our features (log-mels)at 6. 5 seconds, the latter part of the audio is basically truncated and hencesuch errors.  Type 3 (Ambiguous / Wrongly Labeled) : There are samples in the datasetwhich are not perfectly labelled. One may say these audios are debatablem,some may find them to be bad others may think that they are ok. This type oflabel noise is something which needs to be tackled. Needless to say, there are places where we can improve these results, buthaving a solid baseline model initially is important for incrementalimprovements over time and after a few iterations we finally see these modelsin our production systems. That’s all for now. Stay tuned to our rss feed for updates and more. "
    }, {
    "id": 27,
    "url": "/speaker-diarization/",
    "title": "Speaker Diarization",
    "body": "2020/07/21 -  This blog post is based on the work done by AnirudhDagar as an intern at Vernacular. ai Diarization - Who spoke when?Speaker diarisation (or diarization) is the process of partitioning an input audio stream into homogeneous segments according to the speaker identity.  Speaker diarization is the process of recognizing “who spoke when. ” In an audio conversation with multiple speakers (phone calls, conference calls, dialogs etc. ), the Diarization API identifies the speaker at precisely the time they spoke during the conversation. Below is an example audio from calls recorded at a customer care center, where the agent is involved in a one-to-one dialog with the customer. This can be particularly hard sometimes as we’ll discuss later in the blog. Just to give an example, this audio below seems to have a lot of background talking and noise making it difficult even for a human to accurately understand speaker timestamps.     Below we have an example of an audio along with its transcription and speech timestamp tags.     Transcription: “Barbeque nation mei naa wo book kia tha ok table book kia tha han toh abhi na uske baad ek phone aaya tha toh wo barbeque nation se hi phone aaya tha mai receive nhi kar paaya toh yehi” Diarization Tag: AGENT: [(0, 5. 376), (8. 991, 12. 213)], CUSTOMER: [(6. 951, 7. 554)] What Diarization is NOT ?: There is a fine line between speaker diarization and other related speech processing tasks.    Diarization != Speaker Change Detection : Diarization systems spit a label, whenever a new speaker appears and if the same speaker comes again, it provides the same label. However, in speaker change detection no such labels are given, only the boundary of change is considered for prediction.     Diarization != Speaker Identification : The goal is not to learn the voice prints of any known speaker. Speakers’ are not registered before running the model.  Motivation  Fig 1. : ASR using Diarization tags to understand and segregate transcription. With the rise of speech recognition systems both in terms of scale and accuracy, the ability to process audio of multiple speakers is crucial and has become quintessential to understand speech today. As illustrated in Fig 1. above, information gained through diarization helps in enriching and improving Spoken Language Understanding (SLU) based on the Automatic Speech Recognition (ASR) transcription. It can enhance the readability of the transcription by structuring the audio stream into speaker turns and, when used together with speaker recognition systems, by providing the speaker’s true identity. This can be valuable for downstream applications such as analytics for call-center transcription and meeting transcription etc. Other than this, we at Vernacular. ai work on Call Center Automation (CCA) among many other speech domains, and, at the very core this is powered by our products VIVA and VASR. Information gained through diarization can be used to strengthen the VASR engine. How? Let me explain the goal of every customer care call support service, if you are not already aware of. The ultimate aim is to provide best in class service to the customers of the respective company. Quantitatively, measure of the service quality is based on the assessment of the AGENT’s (Call representative at customer care center) ability to disseminate relevant information to the CUSTOMER. During the quality check phase, an AGENT’s performance is scored on mutiple parameters, such as (but not limited to):  Whether the agent was patient enough listening to the customer or was rushing on the call Whether she/he was rude to the lead at any time or not Whether she/he used the proper language to communicate. For such occasions, identifying different speakers in a call (“AGENT” or “CUSTOMER”) and finally connecting different sentences under the same speaker is a critical task for the assessment quality. Speaker Diarization is the solution for those problems. Other applications involve:  Information retrieval from broadcast news.  Generating notes/minutes of meetings.  Turn-taking analysis of telephone conversations.  Call center Data analysis Court houses &amp; Parliaments.  Broadcast News(TV and Radio)DIHARD?: This is not 2x2=4. This is Diarization and IT IS HARD. One can say that it is one of the toughest ML problems intrinsically high on complexity, even for a human-being, in certain conditions. But Why??? Real world audios are not always sunshine and rainbows. They come with infinite complexities. To name a few:    In most of the conversations, people will interrupt each other, overtalk etc. , and, cutting the audio between sentences won’t be a trivial task due to this highly interactive nature.     Speakers are discovered dynamically. Although as you’ll see later, in our case we only have 2 speakers, a fixed number.   Sometimes the audio is noisy:     People talking in the background.    The microphone picking up speakers’ environment noises (roadside noises, industrial machinery noise, music in the background etc. ).     For telephony based audios, connection may be weak at times, leading to:     Audio being dropped/corrupted in some parts.    Static or just some buzzing noise creeping in the conversation and finding it’s way into the audio recording.    Believe me, this is not the end of many problems for diarization!  Maybe in a conference call with multiple speakers, even if the audio is clear, the difference can be very subtle between the speakers, and it is not always possible to identify/label the correct speaker for a particular timestamp/duration. Ok, so that’s it? If I have not made my point clear about the complexity of the problem, yet, then I’ll express my message through this legendary meme.   Fig 2. : Diarization is hard!More Problems?    All the problems stated above are considering that preprocessing steps like VAD/SAD worked perfectly, which you may have guessed, are obviously not 100% accurate.   What is this “preprocessing step”?   Voice Activity Detection (VAD) or Speech Activity Detection (SAD) is a widely used audio preprocessing technique, before running a typical diariaztion api based on the clustering of speaker embeddings. The objective of VAD/SAD is to get rid of all non-speech regions.  Approaching the problemKeeping in mind the complexity and hardness of the problem, multiple approaches have been devised over the years to tackle diarization. Some earlier approaches were based on Hidden Markov Models (HMMs)/ Gaussian Mixture Models (GMMs). More recently, Neural Embedding (x-vectors/d-vectors) + Clustering and End2End Neural methods have demonstrated their power. As stated in this paper by Fujita et al. , a x-vector/d-vector clustering-based system is commonly used for speaker diarization and most of our experiments are based around this approach.   Fig 3. : The image shows the cluster generated based on the speech pattern and precise time the speaker participated in the conversation. The aim of speaker clustering is to put together all the segments that belong to the same acoustic source in a recording. These don’t utilize any prior information of the speaker ID or the number of speakers in the recording. We’ll be covering a typical embedding-clustering based approach in detail in the latter sections of the blog. However, speaker diarization systems which combine the two tasks in a unified framework are gaining popularity in recent times. Fig 4 visually summarizes the End2End idea. Due to the increased amounts of data being avaialable, joint End2End modeling methods are slowly taking over older approaches across ML domains, alleviating the complex preparation processes involved earlier. One example is EEND: End2End Neural Diarization by Fujita et al. proposed recently showing some promise in regards to solving these complex steps jointly.   Fig 4. : An End to End approach diarization system. Defining Our Problem: We need to answer the question “What should be a robust diarization system?” before moving forward. We decided to try out multiple approaches and model experiments explained in the sections below. Our model should be powerful enough to capture global speaker characteristics in addition to local speech activity dynamics. A systems, that is able to accurately handle highly interactive and overlapping speech specifically in the telephony call audios conversational domain, while being resilient to variation in mobile microphones, recording environment, reverberation, ambient noise, speaker demographics. Since we have a more focused problem, for evaluating customer care center call audios, the number of speakers for our use case is fixed at two i. e “AGENT” and “CUSTOMER” for each call, we need to tune our model for the same. Method:   Fig 5. : A typical diarization pipeline.    VAD — We employ WebRTC VAD to remove noise and non speech regions during our experiments. Raw audios are split into frames with specific duration (30 ms in our case). For each input frame, WebRTC generates output 1 or 0, where 1 denotes speech and 0 denotes nonspeech. An optional setting of WebRTC is the aggressive mode, an integer between 0 and 3. 0 is the least aggressive about filtering out nonspeech while 3 is the most aggressive. These VAD/SAD models have their own respective struggles with and set of problems.     Embedder — Resemblyzer allows you to derive a high-level representation of a voice through a deep learning model (referred to as the voice encoder). Given an audio file of speech, it creates a summary vector of size 256 (embedding) that summarizes the characteristics of the voice spoken.     Clustering — We cluster the segment wise embedding using a simple K-Means to produce diarization results and determine the number of speakers with each speakers time stamps.     Resegmentation - Finally, an optional supervised classification step may be applied to actually identity every speaker cluster in a supervised way.  Evaluation Metrics: To evaluate the performance or to estimate the influence of errors on thecomplete pipeline, we use the standard metrics implemented inpyannote-metrics. We also need to account for the fact that these time-stamps are manuallyannotated by data-annotators and hence cannot be precise at the audio samplelevel. It is a common practice in speaker diarization research to remove afixed collar around each speaker turn boundary from being evaluated using ourmetric of choice. A 0. 4sec collar would exclude 200ms before and after theboundary. Diarization error rate (DER) is the de facto standard metric for evaluating and comparing speaker diarization systems. It is measured as the fraction of time that is not attributed correctly to a speaker or non-speech. The DER is composed of the following three errors: False Alarm: It is the percentage of scored time that a hypothesized speaker is labelled as a non-speech in the reference. The false alarm error occurs mainly due to the the speech/non-speech detection error (i. e. , the speech/non-speech detection considers a non-speech segment as a speech segment). Hence, false alarm error is not related to segmentation and clustering errors. Missed Detection: It is the percentage of scored time that a hypothesized non-speech segment corresponds to a reference speaker segment. The missed speech occurs mainly due to the the speech/non-speech detection error (i. e. , the speech segment is considered as a non-speech segment). Hence, missed speech is not related to segmentation and clustering errors. Confusion: It is the percentage of scored time that a speaker ID is assigned to the wrong speaker. Confusion error is mainly a diarization system error (i. e. , it is not related to speech/non-speech detection. ) It also does not take into account the overlap speeches not detected. [\text{DER} = \frac{\text{false alarm} + \text{missed detection} + \text{confusion}}{\text{total}}] Resegmentation: As stated earlier, speaker diarization consists of automatically partitioning an input audio stream into homogeneous segments (segmentation) and assigning these segments to the same speaker (speaker clustering). Read more about segmentation here. Is it possible to resegment these assignments, post clustering, to achieve an improved lower DER? This is exactly the job of a resegmentation module. Simply put, we had a condition to be improved, a difficulty to be eliminated, and a troubling question that existed.  If yes, in what scenarios this optional (Resegmentation) module helps? Before going ahead with resegmentation, we needed meaningful understanding and deliberate investigation of the predictions to answer the above question. We brainstormed on this particular section involving post-processing of predictions in the diarization pipeline. After analysis and comparisons of predicted annotations made to the true annotations, our system, though good (with DER=0. 05), was possibly facing an issue. We were hit by something called oversegmentation, which can be seen in figure 6 below.   Fig 6. : Oversegmentation. Need for resegmentation?To fix this problem and in the process making our system more robust, we tried multiple experiments tweaking the resegmentation module.  Standard Smoothing: This is a simple annotation post processing smoothing function to solve the abrupt annotated segments (oversegmented regions) by merging these items smaller than or equal to the threshold (duration less than 0. 2 seconds) with neighbouring annotation. We instantly got a 1% boost in DER (0. 04) after smoothing. Our initial hunch about oversegmentation based on the analysis was now supported by results from this simple exercise. This made us think about a few more questions:  Can we do even better? Can we use the labels and supervise the resegmentation module learning this smoothing function at the very least? Are there some other errors in the predictions which we may have overlooked, but probably a learned resegmentation model captures? Can we leverage clustering confidences to improve the resegmentation module?Aiming to answer the above questions, we came up with a Supervised Resegmentation Sequence2Sequence model.    Supervised Seq2Seq Resegmentation: The goal of the model as shown below is to learn a mapping from the initial predictions to the ground truth sequence based on supervised training.    Resegmentation Goal       PREDICTIONS             GROUND TRUTH [A, A, A, A, B, A, A, B, B, B . . . . ] -&gt; [A, A, A, A, A, A, A, B, B, B . . . . ] where A : Speaker 1    B : Speaker 2    each label represents fixed duration of 400ms in our annotations.     To achieve such a mapping we worked on a simple Seq2Seq LSTM based model. We also enriched this model with information of cluster confidences after tweaking our Embedding+Clustering pipeline to do a soft clustering, i. e. return cluster scores based on the distance of each point in the cluster from the centroid along with the clustered predictions.  Overall all the above steps regarding a supervised resegmentation model were completely experimental and based on a few ideas. We are yet to achieve convincing results based on this approach but I thought it would be nice to mention this cool experiment :). Providing more resegmentation sequences for training could definitely and we also try to tackle diarization with limited data. See here UIS-RNN: To explore more supervised methods, we also experimented with Fully Supervised Speaker Diarization or the UIS-RNN model, the current state of the art neural system for Speaker Diarization. Converting data to UIS Style format involves a set of preprocessing steps similar to what we had to for our supervised resegmentation model. More on the official UIS-RNN Repo. But a caveat with UIS-RNN is that it requires huge amounts of data to form a convincing hypothesis after training. On realizing the limited amount of tagged data we had, we worked on simulating datasets for Speaker Diarization which in itself comes with some challenges. Simulated Data Generation:   Fig 7. : Diarization Data SimulationWe started with a large number of dual channel audio calls as a requirement for generating this Speaker Diarization Dataset. These dual channel audios were then split and saved into mono channel audio files. The key idea is that each mono channel contains one speaker and if we are able to combine these mono channeled audios compunded with known timestamps of speakers, we can then possibly recreate the audios which are potentially useful for Supervised Speaker Diarization. Steps as shown in Fig 7:    Split dual channel audio calls into mono channel audios.   Running Webrtc VAD on the mono channels:     Aggressive : Speech Regions.    Mild : Invert to get Gap Regions    Compute statistics in real audios: This step is required for us to understand the dynamics of overlaps and silences in a call on avergae. We compute the following ratios:     \[\text{Silence Ratio} = \frac{\text{Duration of Silences}}{\text{Total Duration}}\]      \[\text{Overlap Ratio} = \frac{\text{Duration of Overlapping Utterances}}{\text{Total Duration}}\]       Combination of Speech from A and B with timestamps. At the same time we needed to add real Gaps/Silence fills and Overlaps (interrupts and overtalking) to mimic real world call audios which are highly interactive. To control the amount of overlap in data-generation, we used 2 parameters mainly.      scale_overlap : This allowed us to control the maximum possible duration of overlap and was set based on the stats gathered in step 3.    bias_overlap : This allowed us to control the percentage or probability if there is an overlapping segment. Eg: setting bias_overlap to 0. 75 will give 33% chance each time to add overlap.     Dump tagged speaker timestamps and simulated audios. That’s all for now, and we’ll end this blog here. Stay tuned to our rss feed for updates. Until next time, Signing Off! References:  Deep Self-Supervised Hierarchical Clustering for Speaker Diarization Joint Speech Recognition and Speaker Diarization via Sequence Transduction WebRTC VAD DIHARD II is Still Hard: Experimental Results and Discussionsfrom the DKU-LENOVO Team PyAnnote Audio Resemblyzer Awesome Diarization Robust Speaker Diarization for Meetings Generalized End-To-End Loss For Speaker Verification Fully Supervised Speaker Diarization"
    }, {
    "id": 28,
    "url": "/fast-microservices-with-grpc/",
    "title": "Building Fast and Efficient Microservices with gRPC",
    "body": "2020/02/05 - Vernacular. ai processes millions of speech recognition requests every day, and to handle such a load we have focused on building a highly scalable technical stack. Firstly, we realized the drawbacks of using HTTP/1. 1 keeping in mind one of our requirements to do end to end streaming recognition to reduce the overall latency of the system. So, we decided to migrate all our core services from REST APIs (HTTP/1. 1 + JSON) to gRPC (HTTP/2. 0 + Protobuf). In this article, I will take you through the benefits and challenges of adopting gRPC when compared to a REST-based communication style. What is Microservices Architecture?:  Defines an architecture that structures the application as a set of loosely coupled, collaborating services. Services communicate using either synchronous protocols such as HTTP/REST, gRPC or asynchronous protocols such as AMQP. Services can be developed and deployed independently of one another. What are the challenges of adopting Microservices Architecture?: While there are many strong benefits of moving to a microservices architecture — there are no silver bullets! This means there are tradeoffs to make! Microservices gives us a higher level of complexity, and there are several inherent challenges with microservices you need to take into account when changing from a monolithic architecture. Like,  Inter-service network communication Data serialization/de-serialization Security/Authentication Language Interoperability Streaming Data Monitoring DebugginggRPC gives us the tools and capabilities to combat these issues without having to roll your custom frameworks. What is gRPC?:  gRPC is a modern open-source high-performance RPC framework that can run in any environment. It can efficiently connect services in and across data centres with pluggable support for load balancing, tracing, health checking and authentication. It is also applicable in the last mile of distributed computing to connect devices, mobile applications and browsers to backend services. What are Protocol buffers?:  Protocol Buffers, another open-source project by Google, is a language-neutral, platform-neutral, extensible mechanism for quickly serializing structured data in a small binary packet. By default, gRPC used Protocol Buffers v3 for data serialisation. When working with Protocol Buffers, we write a . proto file to define the data structures that we will be using in our RPC calls. This also tells protobuf how to serialise this data when sending it over the wire. This results in small data packets being sent over the network, which keeps your RPC calls fast, as there are fewer data to transmit. It also makes your code execute faster, as it spends less time serializing and deserializing the data that is being transmitted. Here you can see, we are defining a Person data structure, with a name, an id and multiple phone numbers of different types. syntax =  proto3 ;message Person {string name = 1;  int32 id = 2;  string email = 3;  enum PhoneType {   MOBILE = 0;   HOME = 1;   WORK = 2;  }  message PhoneNumber {   string number = 1;   PhoneType type = 2;  }  repeated PhoneNumber phone = 4;}Along with the data structures, we can also define RPC functions in the services section of our . proto file. There are several types of RPC calls available — as we can see in GetFeature, we can do the standard synchronous request/response model, but we can also more advanced types of RPC calls, such as with RouteChat, where we can send information via bi-directional streams in a completely asynchronous way. From these . proto files, we can use the gRPC tooling to generate both clients and server-side code that handles all the technical details of the RPC invocation, data serialisation, transmission and error handling. This means we can focus on the logic of our application rather than worry about these implementation details. service RouteGuide { rpc GetFeature(Point) returns (Feature); rpc RouteChat(stream RouteNote) returns (stream RouteNote);}message Point { int32 Latitude = 1; int32 Longitude = 2;}message Feature { string name = 1; Point location = 2;}message RouteNote { Point location = 1; string message = 2;}gRPC (HTTP/2. 0 + Protobuf) vs REST (HTTP/1. 1 + JSON): HTTP/1. x HTTP/0. 9 was a one-line protocol to bootstrap the World Wide Web HTTP/1. 0 documented the popular extensions to HTTP/0. 9 in an informational standard. HTTP/1. 1 introduced an official IETF standard. HTTP/1. x clients need to use multiple connections to achieve concurrency and reduce latency; HTTP/1. x does not compress request and response headers, causing unnecessary network traffic; HTTP/1. x does not allow effective resource prioritization, resulting in poor use of the underlying TCP connection; and so on. Advantages of HTTP/2. 0 The first advantage HTTP/2 gives you over HTTP/1. x is speed.  HTTP/2 reduces latency by enabling full request and response multiplexing, minimize protocol overhead via efficient compression of HTTP header fields, support for request prioritization, allows multiple concurrent exchanges on the same connection and server push.   HTTP/2 vs HTTP/1. 1 Multiplexing — HTTP/1. 1 vs HTTP/2. 0 HTTP/2 gives us multiplexing. Therefore, multiple gRPC calls can communicate over a single TCP/IP connection without the overhead of disconnecting and reconnecting over and over again as HTTP/1. 1 will do for each request. This removes a huge overhead from traditional HTTP forms of communication.   Bi-directional Streaming HTTP/2 also has bi-directional streaming built-in. This means gRPC can send asynchronous, non-blocking data up and down, such as the RPC example we saw earlier, without having to resort to much slower techniques like HTTP-long polling. A gRPC client can send data to the server and the gRPC server can send data to the client in a completely asynchronous manner. This means that doing real-time, streaming communication over a microservices architecture is exceptionally easy to implement in our applications Multi-language support One of the big advantages of microservices architecture is using different languages for different services. gRPC works across multiple languages and platforms. Currently, these languages/frameworks are supported,  Go Ruby PHP Java / Android C++ C# Objective-C / iOS Python Node. js DartThe best part is that gRPC is not just a library, but tooling as well. Once you have your . proto file defined, you can generate client and server stubs for all of these languages, allowing your RPC services to use a single API no matter what language they are written in! This means that you can choose the right tool for the job when building your microservices — you aren’t locked into just one language or platform. gRPC will generate clients and servers stubs that are canonically written for the language you want to use, and also take care of the serialisation and deserialisation of data in a way that your language of choice will understand! There’s no need to worry about transport protocols or how data should be sent over the wire. All this is simply handled for you, and you can focus instead on the logic of your services as you write them.  This means that you can choose the right tool for the job when building your microservices — you aren’t locked into just one language or platform. gRPC will generate clients and servers stubs that are canonically written for the language you want to use, and also take care of the serialisation and deserialisation of data in a way that your language of choice will understand! There’s no need to worry about transport protocols or how data should be sent over the wire. All this is simply handled for you, and you can focus instead on the logic of your services as you write them. What are the challenges in adopting gRPC?: Load Balancing Our microservices are containerized and deployed using Kubernetes, an open-source container orchestration system. Kubernetes’s default load balancing often doesn’t work out of the box with gRPC, we use Linkerd (Service Mesh) for gRPC load balancing with sidecar container injected to existing pods. More about gRPC load balancing with Kubernetes and other gRPC load balancing options Manual Testing Since gRPC is a binary protocol, the RPC methods cannot be manually tested using GUI tools like Postman (generally used for HTTP/1. 1 text-based protocol). We have used Evans REPL for gRPC introspection and manual testing. To conclude, I would highly recommend everyone to try out gRPC and start migrating to gRPC in production. If you are interested in working with cutting-edge technologies, come work with us. Apply here "
    }, {
    "id": 29,
    "url": "/repl-conversations/",
    "title": "A REPL for Conversations",
    "body": "2020/01/30 - A REPL, in programming, is an interactive environment where a programmercan go through the cycle of writing code, getting it Read,Evaluated, output Printed and then back in a Loop. Fundamentally,a REPL merges usage and extension of a system in a single interface. This merger has been important in providing easy extensibility to theend programmers in many programming languages. Interaction is supervisionWe can envision a similar merger in human intelligence involvinglanguages. After getting to a certain level of language understanding,our interactive behavior can be manipulated itself by interactions. Interactions cover all possible dialogs. Manipulative interactions orsupervision, as we like to call it, cover whatever is involved inupdating the state of the internal model which, in turn, affects thebehavior. Most of the machine dialog platforms aren't designed to expose thisequivalence explicitly. We have a different language for providingsupervision and different for building the interaction experiences andthat's it. Consider the chatbots you see on various websites thesedays. These have an interaction language which is a natural language,and a separate supervision language which stays at the programmer'sbeck in the background. I can't improve my personal interaction withthe system or extend it's behavior by talking to it. I can do that witha human agent easily. We definitely are moving towards such systems by adding control knobspiece by piece. But still aren't looking very actively for a generalframework of user initiated supervision. Practically this is not goingto be immediately helpful for many reasons known to all of us working inthe field, but it's helpful to define various kinds of platforms formachine dialog involving users on a continuum who all program theirdesired changes using their own interactive languages. From the technical side, of course there are problems that need to besolved for this. Since natural languages are already there, as comparedto a language to be designed, this needs a lot of work inunderstanding and creating computational models of languageacquisition fromone end and works in zero and few shot learnings from the other. Sublanguages are still going to be the more efficient way for working ata certain level, just like mathematical notations are for talking aboutmathematics, but we can target a natural language REPL layer on top ofcurrent dialog platforms and aim for a sort of interactive malleabilityfor growing the system organically afterwards. Growing a conversational agentThis malleability is not only a learning but also design problem at themoment1. A problem similar to designing a programming language. Whatwe want the language user to accomplish is our focus right now. How wecan let the language itself grow is a problem we will be designingsystems for in the future. Once this interaction language is capableenough, we can have an ecosystem of extensions and libraries making aminimal system malleable enough to allow building drastically differentexperiences on top of it. And it’s important that the system accepts growth at various levels if it has togeneralize across many use cases. In his talk ‘Growing a Language’, Guy L. Steele cites lack of this growth as the difference between APL and Lisp. In APL“there was no way for a user to grow the language in a smooth way”. Thesmoothness here focuses on extensibility in terms of language primitives. Fromthe same talk:  If I want to help other persons to write all sorts of programs, shouldI design a small programming language or a large one?  I stand on this claim: I should not design a small language, and Ishould not design a large one. I need to design a language that cangrow. I need to plan ways in which it might grow—but I need, too, toleave some choices so that other persons can make those choices at alater time. Conversational systems are interesting from many more angles than justwhat their components look like at the moment, which includes Speech ToText, Language Understanding, Dialog Management, Text To Speech, etc. Improving metrics under the current framework is a decent and importantgoal, but there are many interesting challenges in the design aspects ofhow we build and let others build conversational experiences themselves.       Many researches in self learning systems are orthogonal here asyou can have low generalization and still make a natural languageREPL.  &#8617;    "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        <div class="container">
<h3 class="font-weight-bold spanborder"><span>Privacy Policy</span></h3>
<div class="page-content">
<p>“Vernacular.ai Tech” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used.</p>

<h4 id="collection-of-routine-information">Collection of Routine Information</h4>

<p>This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes.</p>

<h4 id="cookies">Cookies</h4>

<p>Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content.</p>

<h4 id="advertisement-and-other-third-parties">Advertisement and Other Third Parties</h4>

<p>Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to <a href="http://www.google.com/privacy_ads.html">opt out of Google’s cookie usage</a>.</p>

<h4 id="links-to-third-party-websites">Links to Third Party Websites</h4>

<p>We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own.</p>

<h4 id="security">Security</h4>

<p>The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security.</p>

<h4 id="changes-to-this-privacy-policy">Changes To This Privacy Policy</h4>

<p>This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page.</p>

<p>We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website.</p>

<h4 id="contact-information">Contact Information</h4>

<p>For any questions or concerns regarding the privacy policy, please <a href="/contact.html">contact us here</a>.</p>

</div>
</div>

    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>

    

    

    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
              <span><span class="text-dark font-weight-bold">Vernacular.ai</span> © <script>document.write(new Date().getFullYear())</script></span>

                <!--  Github Repo Star Btn-->
                <a class="text-dark ml-1" target="_blank" href="https://github.com/Vernacular-ai/"><i class="fab fa-github"></i> Github</a>
                <a class="text-dark ml-1" target="_blank" href="https://twitter.com/VernacularTech"><i class="fab fa-twitter"></i> Twitter</a>

            </div>
            <div>
                Made with <a target="_blank" class="text-dark font-weight-bold" href="https://www.wowthemes.net/mundana-jekyll-theme/"> Mundana Jekyll Theme </a> by <a class="text-dark" target="_blank" href="https://www.wowthemes.net">WowThemes</a>
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
